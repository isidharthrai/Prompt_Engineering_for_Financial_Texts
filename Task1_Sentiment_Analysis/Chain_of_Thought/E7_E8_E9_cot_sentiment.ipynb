{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e486c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated setup complete for new LLM sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated imports for new LLMs and libraries\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    " )\n",
    "\n",
    "# API setup for LLMs (Groq, dotenv)\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Updated setup complete for new LLM sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7a8c8",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68548254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2264 sentences\n",
      "\n",
      "Sentiment distribution:\n",
      "true_sentiment\n",
      "neutral     1391\n",
      "positive     570\n",
      "negative     303\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Sample sentences:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>The contract value amounts to EUR 2.4 million .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>Kemira shares closed at (  x20ac ) 16.66 ( $ 2...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>The company slipped to an operating loss of EU...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>According to Atria 's President and CEO Matti ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>In 2009 , Fiskars ' cash flow from operating a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence true_sentiment\n",
       "1755    The contract value amounts to EUR 2.4 million .        neutral\n",
       "1281  Kemira shares closed at (  x20ac ) 16.66 ( $ 2...        neutral\n",
       "350   The company slipped to an operating loss of EU...       negative\n",
       "420   According to Atria 's President and CEO Matti ...       positive\n",
       "56    In 2009 , Fiskars ' cash flow from operating a...       positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the 100% agreement dataset (highest quality)\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ad40e",
   "metadata": {},
   "source": [
    "## 2. Chain-of-Thought Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Stepwise reasoning with explicit JSON output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd3d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Chain-of-Thought prompt for sentiment classification.\n",
    "    Guides the model through stepwise reasoning.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze the following financial statement step-by-step.\n",
    "\n",
    "Classify the sentiment of the following financial statement as either \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Think through this systematically:\n",
    "Step 1: Identify the key financial metrics, events, or indicators mentioned in the statement.\n",
    "Step 2: List any positive signals (growth, profit increases, expansions, etc.).\n",
    "Step 3: List any negative signals (losses, declines, challenges, etc.).\n",
    "Step 4: Evaluate the net impact on stock price from an investor's perspective.\n",
    "Step 5: Based on your analysis, classify the sentiment.\n",
    "\n",
    "Classification guidelines:\n",
    "- Positive: Good news for stock price (revenue increase, profit growth, etc.)\n",
    "- Negative: Bad news for stock price (losses, declining sales, etc.)\n",
    "- Neutral: No clear impact on stock price or mixed signals\n",
    "\n",
    "IMPORTANT: Provide your final answer in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief summary of your step-by-step reasoning\"\n",
    "}}\n",
    "\n",
    "Financial Statement: \"{sentence}\"\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6719e6",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a4a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model inference functions defined\n"
     ]
    }
   ],
   "source": [
    "def call_llama(prompt, model_name, temperature=0.0):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    last_error = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: {last_error}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Parse error: {}\".format(str(e)[:100]))\n",
    "        print(\"Raw response was:\\n{}\".format(response_text))\n",
    "        # Fallback: try to extract sentiment with regex\n",
    "        response_lower = response_text.lower() if response_text else \"\"\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "print(\"✓ Model inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3e124",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9648a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running E7: Updated GPT OSS 20B (Chain-of-Thought)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E7 Progress:   1%|          | 1/100 [00:01<02:06,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199846, Requested 367. Please try again in 1m32.015999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Attempt 2/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199844, Requested 367. Please try again in 1m31.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Attempt 3/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199839, Requested 367. Please try again in 1m28.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E7 Progress:   2%|▏         | 2/100 [00:05<04:33,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199837, Requested 359. Please try again in 1m24.672s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Attempt 2/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199835, Requested 359. Please try again in 1m23.808s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Attempt 3/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199830, Requested 359. Please try again in 1m21.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E7 Progress:   3%|▎         | 3/100 [00:08<05:09,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199829, Requested 351. Please try again in 1m17.759999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Attempt 2/3 failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199826, Requested 351. Please try again in 1m16.464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E7 Progress:   3%|▎         | 3/100 [00:11<05:55,  3.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcall_llama\u001b[39m\u001b[34m(prompt, model_name, temperature)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     chat_completion = \u001b[43mgroq_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/lib/python3.13/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/lib/python3.13/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1239\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/lib/python3.13/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1043\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kf8cn3b9fm4vzg7zcc3qwwhk` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199826, Requested 351. Please try again in 1m16.464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(test_df.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(test_df), desc=\u001b[33m\"\u001b[39m\u001b[33mE7 Progress\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      8\u001b[39m     prompt = create_cot_prompt(row[\u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mcall_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai/gpt-oss-20b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m     11\u001b[39m         parsed = parse_response(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcall_llama\u001b[39m\u001b[34m(prompt, model_name, temperature)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Updated experiment runs for new LLMs\n",
    "test_df = df.head(100).copy()  # Use a sample of the dataset\n",
    "\n",
    "# E7: Updated GPT OSS 20B (Chain-of-Thought)\n",
    "print(\"Running E7: Updated GPT OSS 20B (Chain-of-Thought)...\")\n",
    "e7_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E7 Progress\"):\n",
    "    prompt = create_cot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"openai/gpt-oss-20b\")\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e7_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "            })\n",
    "        else:\n",
    "            e7_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"Parse error\",\n",
    "            })\n",
    "    time.sleep(0.5)\n",
    "e7_df = pd.DataFrame(e7_results)\n",
    "print(f\"\\n✓ E7 completed: {len(e7_df)} predictions\")\n",
    "display(e7_df.head())\n",
    "\n",
    "# E8: Updated GPT OSS 1Z20B (Chain-of-Thought)\n",
    "print(\"Running E8: Updated GPT OSS 120B (Chain-of-Thought)...\")\n",
    "e8_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E8 Progress\"):\n",
    "    prompt = create_cot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"openai/gpt-oss-120b\")\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e8_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "            })\n",
    "        else:\n",
    "            e8_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"Parse error\",\n",
    "            })\n",
    "    time.sleep(0.5)\n",
    "e8_df = pd.DataFrame(e8_results)\n",
    "print(f\"\\n✓ E8 completed: {len(e8_df)} predictions\")\n",
    "display(e8_df.head())\n",
    "\n",
    "# E9: Updated Llama-3.3-70B (Chain-of-Thought)\n",
    "print(\"Running E9: Updated Llama-3.3-70B (Chain-of-Thought)...\")\n",
    "e9_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E9 Progress\"):\n",
    "    prompt = create_cot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"llama-3.3-70b-versatile\")\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e9_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "            })\n",
    "        else:\n",
    "            e9_results.append({\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"Parse error\",\n",
    "            })\n",
    "    time.sleep(0.5)\n",
    "e9_df = pd.DataFrame(e9_results)\n",
    "print(f\"\\n✓ E9 completed: {len(e9_df)} predictions\")\n",
    "display(e9_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fd992",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated metrics calculation for new LLMs\n",
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    valid_df = df[df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])].copy()\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e7_metrics, e7_cm, e7_valid = calculate_metrics(e7_df, \"E7: Updated GPT OSS 20B (CoT)\")\n",
    "e8_metrics, e8_cm, e8_valid = calculate_metrics(e8_df, \"E8: Updated GPT OSS 120B (CoT)\")\n",
    "e9_metrics, e9_cm, e9_valid = calculate_metrics(e9_df, \"E9: Updated Llama-3.3-70B (CoT)\")\n",
    "\n",
    "metrics_df = pd.DataFrame([e7_metrics, e8_metrics, e9_metrics])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df[[\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "for df_result, exp_name in [\n",
    "    (e7_valid, \"E7: GPT OSS 20B (CoT)\"),\n",
    "    (e8_valid, \"E8: GPT OSS 120B (CoT)\"),\n",
    "    (e9_valid, \"E9: Llama-3.3-70B (CoT)\"),\n",
    "]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CLASSIFICATION REPORT: {exp_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            df_result[\"true_sentiment\"],\n",
    "            df_result[\"predicted_sentiment\"],\n",
    "            labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "            target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da85e7",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dab937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [(e7_valid, \"GPT OSS 20B (CoT)\"), (e8_valid, \"GPT OSS 120B (CoT)\"), (e9_valid, \"Llama-3.3-70B (CoT)\")]\n",
    "):\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    correct_conf = df_result[df_result[\"correct\"]][\"confidence\"]\n",
    "    incorrect_conf = df_result[~df_result[\"correct\"]][\"confidence\"]\n",
    "\n",
    "    axes[idx].hist(\n",
    "        [correct_conf, incorrect_conf],\n",
    "        bins=20,\n",
    "        label=[\"Correct\", \"Incorrect\"],\n",
    "        alpha=0.7,\n",
    "        color=[\"green\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nMean Conf: Correct={correct_conf.mean():.3f}, Incorrect={incorrect_conf.mean():.3f}\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Distribution: Correct vs Incorrect Predictions\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e7_metrics, \"GPT OSS 20B (CoT)\"),\n",
    "        (e8_metrics, \"GPT OSS 120B (CoT)\"),\n",
    "        (e9_metrics, \"Llama-3.3-70B (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Chain-of-Thought)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e7_metrics, \"GPT OSS 20B (CoT)\"),\n",
    "        (e8_metrics, \"GPT OSS 120B (CoT)\"),\n",
    "        (e9_metrics, \"Llama-3.3-70B (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Chain-of-Thought)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e7_cm, \"E7: GPT OSS 20B (CoT)\"),\n",
    "        (e8_cm, \"E8: GPT OSS 120B (CoT)\"),\n",
    "        (e9_cm, \"E9: Llama-3.3-70B (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Chain-of-Thought Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82f9ed",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce62254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e7_df.to_csv(f\"e7_GPT_OSS_20B_cot_{timestamp}.csv\", index=False)\n",
    "e8_df.to_csv(f\"e8_GPT_OSS_120B_cot_{timestamp}.csv\", index=False)\n",
    "e9_df.to_csv(f\"e9_Llama-3.3-70B_cot_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df.to_csv(f\"cot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Chain-of-Thought results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - e7_GPT_OSS_20B_cot_{timestamp}.csv\")\n",
    "print(f\"  - e8_GPT_OSS_120B_cot_{timestamp}.csv\")\n",
    "print(f\"  - e9_Llama-3.3-70B_cot_{timestamp}.csv\")\n",
    "print(f\"  - cot_metrics_summary_{timestamp}.csv\")\n",
    "print(f\"  - cot_performance_comparison.png\")\n",
    "print(f\"  - cot_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfc13b",
   "metadata": {},
   "source": [
    "## 8. Key Findings\n",
    "\n",
    "### Summary of Chain-of-Thought Results:\n",
    "\n",
    "1. **Model Performance Ranking**\n",
    "   - Compare accuracy and F1 scores across the three models\n",
    "   - Identify which model performs best in chain-of-thought setting\n",
    "\n",
    "2. **Class-Specific Performance**\n",
    "   - Analyze which sentiment class is hardest to classify\n",
    "   - Check if neutral class causes most confusion\n",
    "\n",
    "3. **Error Analysis**\n",
    "   - Review misclassified examples\n",
    "   - Identify common error patterns\n",
    "\n",
    "4. **Confidence Calibration**\n",
    "   - Examine relationship between confidence scores and accuracy\n",
    "   - Identify overconfident or underconfident predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
