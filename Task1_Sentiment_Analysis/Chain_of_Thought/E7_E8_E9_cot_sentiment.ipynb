{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports for new LLMs and libraries\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "# API setup for LLMs (Groq, dotenv)\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load FinBERT model for local inference\n",
    "print(\"Loading FinBERT model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(f\"✓ FinBERT loaded on {'GPU' if device == 0 else 'CPU'}\")\n",
    "print(f\"✓ Groq API configured: {bool(GROQ_API_KEY)}\")\n",
    "print(\"✓ Updated setup complete for new LLM sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7a8c8",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68548254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset (highest quality)\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ad40e",
   "metadata": {},
   "source": [
    "## 2. Chain-of-Thought Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Stepwise reasoning with explicit JSON output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Chain-of-Thought prompt for sentiment classification.\n",
    "    Guides the model through stepwise reasoning with emphasis on negative detection.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Use step-by-step reasoning to classify this financial statement.\n",
    "\n",
    "Financial Statement: \"{sentence}\"\n",
    "\n",
    "Think through this systematically:\n",
    "\n",
    "1. IDENTIFY Key Metrics: What financial numbers, trends, or events are mentioned?\n",
    "\n",
    "2. POSITIVE Signals: Any growth, profit increases, revenue gains, expansions, cost reductions?\n",
    "\n",
    "3. NEGATIVE Signals: Any losses, declines, revenue drops, widening losses, layoffs, failed ventures?\n",
    "   ⚠️ Pay special attention to: \"loss\", \"decline\", \"decrease\", \"fell\", \"dropped\", \"worse\", \"weak\"\n",
    "\n",
    "4. NET IMPACT: From an investor's perspective, does this help or hurt the stock price?\n",
    "\n",
    "5. FINAL CLASSIFICATION:\n",
    "   - Positive: Financial improvements, growth, profitability increases\n",
    "   - Negative: Financial deterioration, losses, declining metrics\n",
    "   - Neutral: No clear financial impact or balanced signals\n",
    "\n",
    "Provide ONLY this JSON format (no markdown, no extra text):\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Your step-by-step reasoning summary\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6719e6",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama(prompt, model_name, temperature=0.0):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    last_error = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: {last_error}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Parse error: {}\".format(str(e)[:100]))\n",
    "        print(\"Raw response was:\\n{}\".format(response_text))\n",
    "        # Fallback: try to extract sentiment with regex\n",
    "        response_lower = response_text.lower() if response_text else \"\"\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"Call FinBERT for sentiment classification (local inference)\"\"\"\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence[:512])  # FinBERT max length\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "        return {\n",
    "            \"sentiment\": label_map.get(result[0][\"label\"].lower(), \"neutral\"),\n",
    "            \"confidence\": result[0][\"score\"],\n",
    "            \"rationale\": f\"FinBERT classification: {result[0]['label']}\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"FinBERT error: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Model inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3e124",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset for comprehensive evaluation\n",
    "test_df = df.copy()\n",
    "\n",
    "# E7: Updated Mixtral-8x7B (Chain-of-Thought)\n",
    "print(\"Running E7: Updated Mixtral-8x7B (Chain-of-Thought)...\")\n",
    "e7_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E7 Progress\"):\n",
    "    prompt = create_cot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"mixtral-8x7b-32768\")\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e7_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e7_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    time.sleep(0.5)\n",
    "e7_df = pd.DataFrame(e7_results)\n",
    "print(f\"\\n✓ E7 completed: {len(e7_df)} predictions\")\n",
    "display(e7_df.head())\n",
    "\n",
    "# E8: Updated GPT OSS 1Z20B (Chain-of-Thought)\n",
    "print(\"Running E8: Updated Llama-3.1-70B (Chain-of-Thought)...\")\n",
    "e8_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E8 Progress\"):\n",
    "    prompt = create_cot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"llama-3.1-70b-versatile\")\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e8_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e8_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    time.sleep(0.5)\n",
    "e8_df = pd.DataFrame(e8_results)\n",
    "print(f\"\\n✓ E8 completed: {len(e8_df)} predictions\")\n",
    "display(e8_df.head())\n",
    "\n",
    "# E9: FinBERT (Chain-of-Thought - Note: Cannot use reasoning)\n",
    "print(\"Running E9: FinBERT (Chain-of-Thought)...\")\n",
    "print(\n",
    "    \"⚠️ Note: FinBERT uses pre-trained weights, cannot perform chain-of-thought reasoning\"\n",
    ")\n",
    "e9_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E9 Progress\"):\n",
    "    # FinBERT uses direct inference (CoT reasoning is not applicable)\n",
    "    result = call_finbert(row[\"sentence\"])\n",
    "    if result:\n",
    "        e9_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": result.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0),\n",
    "                \"rationale\": result.get(\"rationale\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        e9_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"FinBERT inference error\",\n",
    "            }\n",
    "        )\n",
    "e9_df = pd.DataFrame(e9_results)\n",
    "print(f\"\\n✓ E9 completed: {len(e9_df)} predictions\")\n",
    "display(e9_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fd992",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated metrics calculation for new LLMs\n",
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e7_metrics, e7_cm, e7_valid = calculate_metrics(e7_df, \"E7: Updated Mixtral-8x7B (CoT)\")\n",
    "e8_metrics, e8_cm, e8_valid = calculate_metrics(\n",
    "    e8_df, \"E8: Updated Llama-3.1-70B (CoT)\"\n",
    ")\n",
    "e9_metrics, e9_cm, e9_valid = calculate_metrics(e9_df, \"E9: Updated FinBERT (CoT)\")\n",
    "\n",
    "metrics_df = pd.DataFrame([e7_metrics, e8_metrics, e9_metrics])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82f9ed",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce62254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e7_metrics, \"Mixtral-8x7B (CoT)\"),\n",
    "        (e8_metrics, \"Llama-3.1-70B (CoT)\"),\n",
    "        (e9_metrics, \"FinBERT (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Chain-of-Thought)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e7_metrics, \"Mixtral-8x7B (CoT)\"),\n",
    "        (e8_metrics, \"Llama-3.1-70B (CoT)\"),\n",
    "        (e9_metrics, \"FinBERT (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Chain-of-Thought)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e7_cm, \"E7: Mixtral-8x7B (CoT)\"),\n",
    "        (e8_cm, \"E8: Llama-3.1-70B (CoT)\"),\n",
    "        (e9_cm, \"E9: FinBERT (CoT)\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Chain-of-Thought Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315f33f",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ac82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e7_df.to_csv(f\"e7_Mixtral_8x7B_cot_{timestamp}.csv\", index=False)\n",
    "e8_df.to_csv(f\"e8_Llama_3_1_70B_cot_{timestamp}.csv\", index=False)\n",
    "e9_df.to_csv(f\"e9_FinBERT_cot_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df.to_csv(f\"cot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Chain-of-Thought results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - e7_Mixtral_8x7B_cot_{timestamp}.csv\")\n",
    "print(f\"  - e8_Llama_3_1_70B_cot_{timestamp}.csv\")\n",
    "print(f\"  - e9_FinBERT_cot_{timestamp}.csv\")\n",
    "print(f\"  - cot_metrics_summary_{timestamp}.csv\")\n",
    "print(f\"  - cot_performance_comparison.png\")\n",
    "print(f\"  - cot_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65008e09",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44038f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Most Common Misclassifications\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS: CHAIN-OF-THOUGHT MISCLASSIFICATION PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e7_valid, \"E7: Mixtral-8x7B\"),\n",
    "    (e8_valid, \"E8: Llama-3.1-70B\"),\n",
    "    (e9_valid, \"E9: FinBERT\"),\n",
    "]:\n",
    "    print(f\"\\n{exp_name}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Identify errors\n",
    "    errors = df_result[df_result[\"true_sentiment\"] != df_result[\"predicted_sentiment\"]]\n",
    "\n",
    "    # Count error types\n",
    "    error_types = (\n",
    "        errors.groupby([\"true_sentiment\", \"predicted_sentiment\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    error_types = error_types.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTotal Errors: {len(errors)} / {len(df_result)} ({len(errors) / len(df_result) * 100:.2f}%)\"\n",
    "    )\n",
    "    print(\"\\nMost Common Error Types:\")\n",
    "    display(error_types.head(5))\n",
    "\n",
    "    # Show examples of worst errors (high confidence, wrong prediction)\n",
    "    if len(errors) > 0:\n",
    "        worst_errors = errors.nlargest(3, \"confidence\")\n",
    "        print(f\"\\nTop 3 High-Confidence Errors:\")\n",
    "        for idx, row in worst_errors.iterrows():\n",
    "            print(\n",
    "                f\"\\n  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Conf: {row['confidence']:.2f}\"\n",
    "            )\n",
    "            print(f\"  Sentence: {row['sentence'][:120]}...\")\n",
    "            print(f\"  Rationale: {row['rationale']}\")\n",
    "\n",
    "# Class-wise Performance Comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS-WISE PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_comparison = []\n",
    "for metrics, model_name in [\n",
    "    (e7_metrics, \"Mixtral-8x7B\"),\n",
    "    (e8_metrics, \"Llama-3.1-70B\"),\n",
    "    (e9_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    for sentiment in [\"Positive\", \"Negative\", \"Neutral\"]:\n",
    "        class_comparison.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Class\": sentiment,\n",
    "                \"Precision\": metrics[f\"{sentiment}_Precision\"],\n",
    "                \"Recall\": metrics[f\"{sentiment}_Recall\"],\n",
    "                \"F1-Score\": metrics[f\"{sentiment}_F1\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "class_df = pd.DataFrame(class_comparison)\n",
    "\n",
    "# Pivot for better visualization\n",
    "for metric in [\"Precision\", \"Recall\", \"F1-Score\"]:\n",
    "    print(f\"\\n{metric} by Class:\")\n",
    "    pivot = class_df.pivot(index=\"Class\", columns=\"Model\", values=metric)\n",
    "    display(pivot.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE METRICS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8c004",
   "metadata": {},
   "source": [
    "## 9. Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [(e7_valid, \"Mixtral-8x7B\"), (e8_valid, \"Llama-3.1-70B\"), (e9_valid, \"FinBERT\")]\n",
    "):\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    correct_conf = df_result[df_result[\"correct\"]][\"confidence\"]\n",
    "    incorrect_conf = df_result[~df_result[\"correct\"]][\"confidence\"]\n",
    "\n",
    "    axes[idx].hist(\n",
    "        [correct_conf, incorrect_conf],\n",
    "        bins=20,\n",
    "        label=[\"Correct\", \"Incorrect\"],\n",
    "        alpha=0.7,\n",
    "        color=[\"green\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nMean Conf: Correct={correct_conf.mean():.3f}, Incorrect={incorrect_conf.mean():.3f}\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Distribution: Correct vs Incorrect Predictions (Chain-of-Thought)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cot_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIDENCE CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "for df_result, exp_name in [\n",
    "    (e7_valid, \"E7: Mixtral-8x7B\"),\n",
    "    (e8_valid, \"E8: Llama-3.1-70B\"),\n",
    "    (e9_valid, \"E9: FinBERT\"),\n",
    "]:\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    avg_conf_correct = df_result[df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    avg_conf_incorrect = df_result[~df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Average Confidence (Correct): {avg_conf_correct:.4f}\")\n",
    "    print(f\"  Average Confidence (Incorrect): {avg_conf_incorrect:.4f}\")\n",
    "    print(f\"  Calibration Gap: {calibration_gap:.4f}\")\n",
    "    print(f\"  Total Correct: {df_result['correct'].sum()} / {len(df_result)}\")\n",
    "\n",
    "    # Confidence by sentiment class\n",
    "    print(f\"\\n  Confidence by Predicted Class:\")\n",
    "    for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        class_df = df_result[df_result[\"predicted_sentiment\"] == sentiment]\n",
    "        if len(class_df) > 0:\n",
    "            print(\n",
    "                f\"    {sentiment.capitalize()}: {class_df['confidence'].mean():.4f} (n={len(class_df)})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbf33b",
   "metadata": {},
   "source": [
    "## 10. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Reports\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e7_valid, \"E7: Mixtral-8x7B\"),\n",
    "    (e8_valid, \"E8: Llama-3.1-70B\"),\n",
    "    (e9_valid, \"E9: FinBERT\"),\n",
    "]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{exp_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            df_result[\"true_sentiment\"],\n",
    "            df_result[\"predicted_sentiment\"],\n",
    "            labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "            target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Class-wise Metrics Summary Table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS-WISE METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for metrics, model in [\n",
    "    (e7_metrics, \"Mixtral-8x7B\"),\n",
    "    (e8_metrics, \"Llama-3.1-70B\"),\n",
    "    (e9_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Model\": model,\n",
    "            \"Pos_P\": metrics[\"Positive_Precision\"],\n",
    "            \"Pos_R\": metrics[\"Positive_Recall\"],\n",
    "            \"Pos_F1\": metrics[\"Positive_F1\"],\n",
    "            \"Neg_P\": metrics[\"Negative_Precision\"],\n",
    "            \"Neg_R\": metrics[\"Negative_Recall\"],\n",
    "            \"Neg_F1\": metrics[\"Negative_F1\"],\n",
    "            \"Neu_P\": metrics[\"Neutral_Precision\"],\n",
    "            \"Neu_R\": metrics[\"Neutral_Recall\"],\n",
    "            \"Neu_F1\": metrics[\"Neutral_F1\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nPer-Class Metrics (P=Precision, R=Recall, F1=F1-Score):\")\n",
    "display(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171f490",
   "metadata": {},
   "source": [
    "## 11. CoT vs Few-Shot vs Zero-Shot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8367c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Few-Shot and Zero-Shot results (if available)\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT vs FEW-SHOT vs ZERO-SHOT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to load Few-Shot and Zero-Shot results\n",
    "few_shot_files = glob.glob(\"../Few_Shot/few_shot_metrics_summary_*.csv\")\n",
    "zero_shot_files = glob.glob(\"../Zero_Shot/zero_shot_metrics_summary_*.csv\")\n",
    "\n",
    "comparison_data = [metrics_df.copy()]\n",
    "comparison_data[0][\"Approach\"] = \"Chain-of-Thought\"\n",
    "\n",
    "if few_shot_files:\n",
    "    latest_few_shot = max(few_shot_files, key=os.path.getctime)\n",
    "    print(f\"\\n✓ Loading Few-Shot results from: {os.path.basename(latest_few_shot)}\")\n",
    "    try:\n",
    "        few_shot_df = pd.read_csv(latest_few_shot)\n",
    "        few_shot_df[\"Approach\"] = \"Few-Shot\"\n",
    "        comparison_data.append(few_shot_df)\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not load few-shot results: {str(e)}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No Few-Shot results found\")\n",
    "\n",
    "if zero_shot_files:\n",
    "    latest_zero_shot = max(zero_shot_files, key=os.path.getctime)\n",
    "    print(f\"✓ Loading Zero-Shot results from: {os.path.basename(latest_zero_shot)}\")\n",
    "    try:\n",
    "        zero_shot_df = pd.read_csv(latest_zero_shot)\n",
    "        zero_shot_df[\"Approach\"] = \"Zero-Shot\"\n",
    "        comparison_data.append(zero_shot_df)\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not load zero-shot results: {str(e)}\")\n",
    "else:\n",
    "    print(\"⚠️ No Zero-Shot results found\")\n",
    "\n",
    "if len(comparison_data) > 1:\n",
    "    combined = pd.concat(comparison_data, ignore_index=True)\n",
    "\n",
    "    # Select key metrics for comparison\n",
    "    comparison_cols = [\n",
    "        \"Experiment\",\n",
    "        \"Approach\",\n",
    "        \"Accuracy\",\n",
    "        \"Macro-F1\",\n",
    "        \"MCC\",\n",
    "        \"Negative_F1\",\n",
    "        \"Positive_F1\",\n",
    "        \"Neutral_F1\",\n",
    "    ]\n",
    "\n",
    "    available_cols = [col for col in comparison_cols if col in combined.columns]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY METRICS COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    display(combined[available_cols].round(4))\n",
    "\n",
    "    # Calculate improvements\n",
    "    if len(comparison_data) == 3:  # All three approaches available\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PERFORMANCE PROGRESSION\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for i in range(min(3, len(metrics_df))):\n",
    "            model_name = [\"Mixtral-8x7B\", \"Llama-3.1-70B\", \"FinBERT\"][i]\n",
    "            print(f\"\\n{model_name}:\")\n",
    "\n",
    "            if i < len(zero_shot_df):\n",
    "                zs_macro = zero_shot_df.iloc[i][\"Macro-F1\"]\n",
    "                print(f\"  Zero-Shot Macro-F1: {zs_macro:.4f}\")\n",
    "\n",
    "            if i < len(few_shot_df):\n",
    "                fs_macro = few_shot_df.iloc[i][\"Macro-F1\"]\n",
    "                zs_to_fs = (\n",
    "                    ((fs_macro - zs_macro) / zs_macro * 100) if zs_macro > 0 else 0\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Few-Shot Macro-F1:  {fs_macro:.4f} ({zs_to_fs:+.2f}% vs Zero-Shot)\"\n",
    "                )\n",
    "\n",
    "            cot_macro = metrics_df.iloc[i][\"Macro-F1\"]\n",
    "            fs_to_cot = ((cot_macro - fs_macro) / fs_macro * 100) if fs_macro > 0 else 0\n",
    "            total_improvement = (\n",
    "                ((cot_macro - zs_macro) / zs_macro * 100) if zs_macro > 0 else 0\n",
    "            )\n",
    "            print(\n",
    "                f\"  CoT Macro-F1:       {cot_macro:.4f} ({fs_to_cot:+.2f}% vs Few-Shot, {total_improvement:+.2f}% vs Zero-Shot)\"\n",
    "            )\n",
    "\n",
    "            # Negative F1 comparison\n",
    "            print(f\"\\n  Negative F1 Progression:\")\n",
    "            if i < len(zero_shot_df):\n",
    "                zs_neg = zero_shot_df.iloc[i][\"Negative_F1\"]\n",
    "                print(f\"    Zero-Shot: {zs_neg:.4f}\")\n",
    "            if i < len(few_shot_df):\n",
    "                fs_neg = few_shot_df.iloc[i][\"Negative_F1\"]\n",
    "                print(f\"    Few-Shot:  {fs_neg:.4f}\")\n",
    "            cot_neg = metrics_df.iloc[i][\"Negative_F1\"]\n",
    "            print(f\"    CoT:       {cot_neg:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Run Few-Shot and Zero-Shot experiments for complete comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfc13b",
   "metadata": {},
   "source": [
    "## 12. Expected Conclusions from Chain-of-Thought Experiment\n",
    "\n",
    "### 1. **CoT vs Few-Shot vs Zero-Shot Performance**\n",
    "   - **Key Question**: Does explicit step-by-step reasoning improve accuracy?\n",
    "   - **Expected**: CoT should outperform Few-Shot by 5-15% on complex cases\n",
    "   - **Reality Check**: Previous 100-sample run showed E9 (Llama) had 100% parsing failures\n",
    "   - **Metric to Watch**: Negative F1 improvement (was 0.0 in previous run for E7 & E8)\n",
    "\n",
    "### 2. **Reasoning Quality Assessment**\n",
    "   - **Step-by-Step Analysis**: Do models actually follow the 5-step reasoning process?\n",
    "   - **Rationale Depth**: Are CoT rationales more detailed than Few-Shot/Zero-Shot?\n",
    "   - **Error Transparency**: Can we identify where reasoning goes wrong?\n",
    "   - **Parsing Success Rate**: Critical for Llama (previous: 0% success on CoT format)\n",
    "\n",
    "### 3. **Model-Specific CoT Effectiveness**\n",
    "   - **GPT OSS Models**: Expected to handle structured prompts better\n",
    "   - **FinBERT**: Previous run showed catastrophic JSON formatting failure\n",
    "   - **Format Compliance**: New simplified format should reduce parsing errors\n",
    "   - **Reasoning Capability**: Which model provides best step-by-step analysis?\n",
    "\n",
    "### 4. **Negative Class Detection (Critical)**\n",
    "   - **Previous Results**: E7 & E8 had 0% Negative F1 despite CoT reasoning\n",
    "   - **Enhanced Prompt**: New version explicitly highlights negative indicators with ⚠️\n",
    "   - **Target**: Negative F1 > 0.60 (297 negative samples in full dataset)\n",
    "   - **Hypothesis**: Step 3 (NEGATIVE Signals) should force models to look for losses/declines\n",
    "\n",
    "### 5. **JSON Format Compliance**\n",
    "   - **Previous Issue**: FinBERT had 100/100 parsing errors\n",
    "   - **Root Cause**: Complex CoT format with multiple steps confused the model\n",
    "   - **Fix**: Simplified format, clearer JSON instruction (\"no markdown, no extra text\")\n",
    "   - **Success Metric**: Parsing error rate < 10%\n",
    "\n",
    "### 6. **Confidence Calibration in CoT**\n",
    "   - **Expected**: CoT should produce more calibrated confidence scores\n",
    "   - **Reasoning**: Step-by-step analysis allows uncertainty assessment\n",
    "   - **Previous**: Llama all predictions had 0.5 confidence (parsing fallback)\n",
    "   - **Comparison**: CoT vs Few-Shot confidence distribution\n",
    "\n",
    "### 7. **Computational Cost Analysis**\n",
    "   - **Longer Prompts**: CoT uses ~2-3x more tokens than Few-Shot\n",
    "   - **API Costs**: Higher per-prediction cost due to prompt length\n",
    "   - **Latency**: Full dataset (2,217 samples × 3 models × 0.5s) = ~2 hours\n",
    "   - **Trade-off**: Is performance gain worth the added cost?\n",
    "\n",
    "### 8. **Error Pattern Analysis**\n",
    "   - **Common Mistakes**: Do all models fail on same sentences?\n",
    "   - **Reasoning Failures**: Where does step-by-step logic break down?\n",
    "   - **High-Confidence Errors**: Are wrong predictions still confident?\n",
    "   - **Class Confusion**: Negative → Neutral or Negative → Positive?\n",
    "\n",
    "### 9. **CoT Prompt Engineering Effectiveness**\n",
    "   - **Structured Steps**: Does numbered reasoning improve consistency?\n",
    "   - **Warning Symbol ⚠️**: Does emphasis on negatives help detection?\n",
    "   - **Explicit Keywords**: Do listed negative terms (\"loss\", \"decline\") trigger recognition?\n",
    "   - **Simplified Format**: Does cleaner JSON reduce Llama parsing errors?\n",
    "\n",
    "### 10. **Production Deployment Viability**\n",
    "   - **Accuracy Threshold**: Need Macro-F1 > 0.75 for deployment\n",
    "   - **Parsing Reliability**: Must have < 5% format errors\n",
    "   - **Negative Detection**: Critical for risk assessment (can't miss bad news)\n",
    "   - **Cost-Benefit**: CoT worth it ONLY if significantly better than Few-Shot\n",
    "\n",
    "### 11. **Critical Success Factors**\n",
    "\n",
    "**For Experiment to be Valuable:**\n",
    "✅ Llama parsing success rate > 90% (vs 0% previously)\n",
    "✅ Negative F1 > 0.50 for at least one model (vs 0.0 previously)\n",
    "✅ Macro-F1 improvement over Few-Shot by > 10%\n",
    "✅ Reasoning quality demonstrably better than Few-Shot\n",
    "\n",
    "**If These Fail:**\n",
    "❌ CoT not suitable for these models\n",
    "❌ Stick with simpler Few-Shot approach\n",
    "❌ Consider fine-tuning instead of prompt engineering\n",
    "\n",
    "### 12. **Actionable Recommendations Based on Results**\n",
    "\n",
    "- **If E9 still fails**: Llama incompatible with complex CoT formats\n",
    "- **If Negative F1 < 0.3**: Dataset imbalance too severe, need different approach\n",
    "- **If CoT ≈ Few-Shot**: Simpler prompts preferred (lower cost, same performance)\n",
    "- **If parsing errors > 20%**: Format still too complex, simplify further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
