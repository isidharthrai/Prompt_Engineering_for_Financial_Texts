{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b799b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/bin/pip: line 2: /Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v2/venv/bin/python3.13: No such file or directory\n",
      "/Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/bin/pip: line 2: exec: /Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v2/venv/bin/python3.13: cannot execute: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea39719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bfa95",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97a484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2264 sentences\n",
      "\n",
      "Sentiment distribution:\n",
      "true_sentiment\n",
      "neutral     1391\n",
      "positive     570\n",
      "negative     303\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_path = (\n",
    "    \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50693acd",
   "metadata": {},
   "source": [
    "## 2. Tree-of-Thought Prompt Design\n",
    "\n",
    "**Multi-Path Reasoning**:\n",
    "- Path 1: Consider \"positive\" hypothesis\n",
    "- Path 2: Consider \"negative\" hypothesis  \n",
    "- Path 3: Consider \"neutral\" hypothesis\n",
    "- Evaluation: Score each path's evidence strength\n",
    "- Selection: Choose the most supported hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eb4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TREE-OF-THOUGHT PROMPT EXAMPLE\n",
      "================================================================================\n",
      "You are a financial sentiment analysis expert. Analyze this statement using a tree-of-thought approach.\n",
      "\n",
      "Financial Statement:\n",
      "\"The company reported mixed results with revenue up 10% but margins declining.\"\n",
      "\n",
      "TASK: Explore three possible sentiment classifications and select the best one.\n",
      "\n",
      "---\n",
      "PATH 1: Hypothesis = POSITIVE\n",
      "Consider if this statement represents positive news for investors.\n",
      "- What evidence supports this being positive?\n",
      "- What evidence contradicts this being positive?\n",
      "- Confidence score (0-1) for this hypothesis:\n",
      "\n",
      "PATH 2: Hypothesis = NEGATIVE\n",
      "Consider if this statement represents negative news for investors.\n",
      "- What evidence supports this being negative?\n",
      "- What evidence contradicts this being negative?\n",
      "- Confidence score (0-1) for this hypothesis:\n",
      "\n",
      "PATH 3: Hypothesis = NEUTRAL\n",
      "Consider if this statement has no clear market impact.\n",
      "- What evidence supports this being neutral?\n",
      "- What evidence contradicts this being neutral?\n",
      "- Confidence score (0-1) for this hypothesis:\n",
      "\n",
      "---\n",
      "FINAL DECISION:\n",
      "Based on evaluating all three paths, select the hypothesis with the strongest evidence.\n",
      "\n",
      "Provide your final answer in this exact JSON format:\n",
      "{\n",
      "    \"sentiment\": \"positive/negative/neutral\",\n",
      "    \"confidence\": 0.0-1.0,\n",
      "    \"rationale\": \"Explanation of why this hypothesis was selected over the others\",\n",
      "    \"path_scores\": {\n",
      "        \"positive\": 0.0-1.0,\n",
      "        \"negative\": 0.0-1.0,\n",
      "        \"neutral\": 0.0-1.0\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Tree-of-Thought prompt with multi-path exploration.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze this statement using a tree-of-thought approach.\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "TASK: Explore three possible sentiment classifications and select the best one.\n",
    "\n",
    "---\n",
    "PATH 1: Hypothesis = POSITIVE\n",
    "Consider if this statement represents positive news for investors.\n",
    "- What evidence supports this being positive?\n",
    "- What evidence contradicts this being positive?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 2: Hypothesis = NEGATIVE\n",
    "Consider if this statement represents negative news for investors.\n",
    "- What evidence supports this being negative?\n",
    "- What evidence contradicts this being negative?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 3: Hypothesis = NEUTRAL\n",
    "Consider if this statement has no clear market impact.\n",
    "- What evidence supports this being neutral?\n",
    "- What evidence contradicts this being neutral?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "---\n",
    "FINAL DECISION:\n",
    "Based on evaluating all three paths, select the hypothesis with the strongest evidence.\n",
    "\n",
    "Provide your final answer in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Explanation of why this hypothesis was selected over the others\",\n",
    "    \"path_scores\": {{\n",
    "        \"positive\": 0.0-1.0,\n",
    "        \"negative\": 0.0-1.0,\n",
    "        \"neutral\": 0.0-1.0\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = (\n",
    "    \"The company reported mixed results with revenue up 10% but margins declining.\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_tot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81294531",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c7c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inference functions defined\n"
     ]
    }
   ],
   "source": [
    "def call_llama(prompt, model_name, temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=1500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON with path scores from ToT response\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"{\" in response_text:\n",
    "            start = response_text.find(\"{\")\n",
    "            end = response_text.rfind(\"}\") + 1\n",
    "            json_str = response_text[start:end]\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eac435",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb95a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running E10: GPT OSS 20B (Tree-of-Thought)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E10 Progress: 100%|██████████| 100/100 [08:19<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ E10 completed: 49 predictions\n",
      "Running E11: GPT OSS 120B (Tree-of-Thought)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E11 Progress: 100%|██████████| 100/100 [08:17<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ E11 completed: 93 predictions\n",
      "Running E12: Llama-3.3-70B (Tree-of-Thought)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E12 Progress: 100%|██████████| 100/100 [05:49<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ E12 completed: 45 predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>confidence</th>\n",
       "      <th>rationale</th>\n",
       "      <th>path_scores</th>\n",
       "      <th>full_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.75</td>\n",
       "      <td>The statement merely reports that the company ...</td>\n",
       "      <td>{'positive': 0.45, 'negative': 0.25, 'neutral'...</td>\n",
       "      <td>{\\n    \"sentiment\": \"neutral\",\\n    \"confidenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.72</td>\n",
       "      <td>The statement highlights a significant improve...</td>\n",
       "      <td>{'positive': 0.72, 'negative': 0.18, 'neutral'...</td>\n",
       "      <td>{\\n    \"sentiment\": \"positive\",\\n    \"confiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.88</td>\n",
       "      <td>The statement reports a 5.2% increase in net s...</td>\n",
       "      <td>{'positive': 0.88, 'negative': 0.02, 'neutral'...</td>\n",
       "      <td>**PATH 1: Hypothesis = POSITIVE**  \\n- **Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.82</td>\n",
       "      <td>The statement reports a 50% increase in operat...</td>\n",
       "      <td>{'positive': 0.82, 'negative': 0.08, 'neutral'...</td>\n",
       "      <td>{\\n    \"sentiment\": \"positive\",\\n    \"confiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.80</td>\n",
       "      <td>The statement reports a clear increase in oper...</td>\n",
       "      <td>{'positive': 0.8, 'negative': 0.1, 'neutral': ...</td>\n",
       "      <td>{\\n    \"sentiment\": \"positive\",\\n    \"confiden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence true_sentiment  \\\n",
       "0  According to Gran , the company has no plans t...        neutral   \n",
       "1  For the last quarter of 2010 , Componenta 's n...       positive   \n",
       "2  In the third quarter of 2010 , net sales incre...       positive   \n",
       "3  Operating profit rose to EUR 13.1 mn from EUR ...       positive   \n",
       "4  Operating profit totalled EUR 21.1 mn , up fro...       positive   \n",
       "\n",
       "  predicted_sentiment  confidence  \\\n",
       "0             neutral        0.75   \n",
       "1            positive        0.72   \n",
       "2            positive        0.88   \n",
       "3            positive        0.82   \n",
       "4            positive        0.80   \n",
       "\n",
       "                                           rationale  \\\n",
       "0  The statement merely reports that the company ...   \n",
       "1  The statement highlights a significant improve...   \n",
       "2  The statement reports a 5.2% increase in net s...   \n",
       "3  The statement reports a 50% increase in operat...   \n",
       "4  The statement reports a clear increase in oper...   \n",
       "\n",
       "                                         path_scores  \\\n",
       "0  {'positive': 0.45, 'negative': 0.25, 'neutral'...   \n",
       "1  {'positive': 0.72, 'negative': 0.18, 'neutral'...   \n",
       "2  {'positive': 0.88, 'negative': 0.02, 'neutral'...   \n",
       "3  {'positive': 0.82, 'negative': 0.08, 'neutral'...   \n",
       "4  {'positive': 0.8, 'negative': 0.1, 'neutral': ...   \n",
       "\n",
       "                                       full_response  \n",
       "0  {\\n    \"sentiment\": \"neutral\",\\n    \"confidenc...  \n",
       "1  {\\n    \"sentiment\": \"positive\",\\n    \"confiden...  \n",
       "2  **PATH 1: Hypothesis = POSITIVE**  \\n- **Evide...  \n",
       "3  {\\n    \"sentiment\": \"positive\",\\n    \"confiden...  \n",
       "4  {\\n    \"sentiment\": \"positive\",\\n    \"confiden...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>confidence</th>\n",
       "      <th>rationale</th>\n",
       "      <th>path_scores</th>\n",
       "      <th>full_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.55</td>\n",
       "      <td>The statement provides factual information abo...</td>\n",
       "      <td>{'positive': 0.45, 'negative': 0.3, 'neutral':...</td>\n",
       "      <td>**PATH 1 – Hypothesis: POSITIVE**  \\n- **Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.68</td>\n",
       "      <td>The statement reports a more than 70% increase...</td>\n",
       "      <td>{'positive': 0.68, 'negative': 0.22, 'neutral'...</td>\n",
       "      <td>**PATH 1 – Hypothesis = POSITIVE**  \\n\\n*Evide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.71</td>\n",
       "      <td>The statement reports simultaneous growth in n...</td>\n",
       "      <td>{'positive': 0.71, 'negative': 0.22, 'neutral'...</td>\n",
       "      <td>**PATH 1 – Hypothesis = POSITIVE**  \\n\\n- **Ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.78</td>\n",
       "      <td>The statement reports a clear increase in oper...</td>\n",
       "      <td>{'positive': 0.78, 'negative': 0.07, 'neutral'...</td>\n",
       "      <td>{\\n    \"sentiment\": \"positive\",\\n    \"confiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.78</td>\n",
       "      <td>The statement reports a clear increase in oper...</td>\n",
       "      <td>{'positive': 0.78, 'negative': 0.04, 'neutral'...</td>\n",
       "      <td>**PATH 1 – Hypothesis = POSITIVE**  \\n*Evidenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence true_sentiment  \\\n",
       "0  According to Gran , the company has no plans t...        neutral   \n",
       "1  For the last quarter of 2010 , Componenta 's n...       positive   \n",
       "2  In the third quarter of 2010 , net sales incre...       positive   \n",
       "3  Operating profit rose to EUR 13.1 mn from EUR ...       positive   \n",
       "4  Operating profit totalled EUR 21.1 mn , up fro...       positive   \n",
       "\n",
       "  predicted_sentiment  confidence  \\\n",
       "0             neutral        0.55   \n",
       "1            positive        0.68   \n",
       "2            positive        0.71   \n",
       "3            positive        0.78   \n",
       "4            positive        0.78   \n",
       "\n",
       "                                           rationale  \\\n",
       "0  The statement provides factual information abo...   \n",
       "1  The statement reports a more than 70% increase...   \n",
       "2  The statement reports simultaneous growth in n...   \n",
       "3  The statement reports a clear increase in oper...   \n",
       "4  The statement reports a clear increase in oper...   \n",
       "\n",
       "                                         path_scores  \\\n",
       "0  {'positive': 0.45, 'negative': 0.3, 'neutral':...   \n",
       "1  {'positive': 0.68, 'negative': 0.22, 'neutral'...   \n",
       "2  {'positive': 0.71, 'negative': 0.22, 'neutral'...   \n",
       "3  {'positive': 0.78, 'negative': 0.07, 'neutral'...   \n",
       "4  {'positive': 0.78, 'negative': 0.04, 'neutral'...   \n",
       "\n",
       "                                       full_response  \n",
       "0  **PATH 1 – Hypothesis: POSITIVE**  \\n- **Evide...  \n",
       "1  **PATH 1 – Hypothesis = POSITIVE**  \\n\\n*Evide...  \n",
       "2  **PATH 1 – Hypothesis = POSITIVE**  \\n\\n- **Ev...  \n",
       "3  {\\n    \"sentiment\": \"positive\",\\n    \"confiden...  \n",
       "4  **PATH 1 – Hypothesis = POSITIVE**  \\n*Evidenc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>true_sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>confidence</th>\n",
       "      <th>rationale</th>\n",
       "      <th>path_scores</th>\n",
       "      <th>full_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6</td>\n",
       "      <td>The company's growth in Russia, despite not pl...</td>\n",
       "      <td>{'positive': 0.6, 'negative': 0.3, 'neutral': ...</td>\n",
       "      <td>To analyze the given financial statement using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.8</td>\n",
       "      <td>The substantial increase in net sales and the ...</td>\n",
       "      <td>{'positive': 0.8, 'negative': 0.2, 'neutral': ...</td>\n",
       "      <td>To analyze the given financial statement using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9</td>\n",
       "      <td>The significant increase in operating profit b...</td>\n",
       "      <td>{'positive': 0.9, 'negative': 0.0, 'neutral': ...</td>\n",
       "      <td>To analyze the given financial statement using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.8</td>\n",
       "      <td>The significant increase in operating profit f...</td>\n",
       "      <td>{'positive': 0.8, 'negative': 0.1, 'neutral': ...</td>\n",
       "      <td>To analyze the given financial statement using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.8</td>\n",
       "      <td>The increase in operating profit from the prev...</td>\n",
       "      <td>{'positive': 0.8, 'negative': 0.1, 'neutral': ...</td>\n",
       "      <td>To analyze the given financial statement using...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence true_sentiment  \\\n",
       "0  According to Gran , the company has no plans t...        neutral   \n",
       "1  For the last quarter of 2010 , Componenta 's n...       positive   \n",
       "2  In the third quarter of 2010 , net sales incre...       positive   \n",
       "3  Operating profit rose to EUR 13.1 mn from EUR ...       positive   \n",
       "4  Operating profit totalled EUR 21.1 mn , up fro...       positive   \n",
       "\n",
       "  predicted_sentiment  confidence  \\\n",
       "0            positive         0.6   \n",
       "1            positive         0.8   \n",
       "2            positive         0.9   \n",
       "3            positive         0.8   \n",
       "4            positive         0.8   \n",
       "\n",
       "                                           rationale  \\\n",
       "0  The company's growth in Russia, despite not pl...   \n",
       "1  The substantial increase in net sales and the ...   \n",
       "2  The significant increase in operating profit b...   \n",
       "3  The significant increase in operating profit f...   \n",
       "4  The increase in operating profit from the prev...   \n",
       "\n",
       "                                         path_scores  \\\n",
       "0  {'positive': 0.6, 'negative': 0.3, 'neutral': ...   \n",
       "1  {'positive': 0.8, 'negative': 0.2, 'neutral': ...   \n",
       "2  {'positive': 0.9, 'negative': 0.0, 'neutral': ...   \n",
       "3  {'positive': 0.8, 'negative': 0.1, 'neutral': ...   \n",
       "4  {'positive': 0.8, 'negative': 0.1, 'neutral': ...   \n",
       "\n",
       "                                       full_response  \n",
       "0  To analyze the given financial statement using...  \n",
       "1  To analyze the given financial statement using...  \n",
       "2  To analyze the given financial statement using...  \n",
       "3  To analyze the given financial statement using...  \n",
       "4  To analyze the given financial statement using...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Updated experiment runs for new LLMs\n",
    "test_df = df.head(100).copy()  # Use a sample of the dataset\n",
    "\n",
    "# Updated Tree-of-Thought experiments for new LLMs\n",
    "def run_tot_experiment(test_df, model_func, model_name, exp_id):\n",
    "    print(f\"Running {exp_id}: {model_name} (Tree-of-Thought)...\")\n",
    "    results = []\n",
    "\n",
    "    for idx, row in tqdm(\n",
    "        test_df.iterrows(), total=len(test_df), desc=f\"{exp_id} Progress\"\n",
    "    ):\n",
    "        prompt = create_tot_prompt(row[\"sentence\"])\n",
    "        response = model_func(prompt)\n",
    "\n",
    "        if response:\n",
    "            parsed = parse_response(response)\n",
    "            if parsed:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"sentence\": row[\"sentence\"],\n",
    "                        \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                        \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                        \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                        \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                        \"path_scores\": str(parsed.get(\"path_scores\", {})),\n",
    "                        \"full_response\": response[:700],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        time.sleep(0.5)  # Adjusted for new LLMs\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ {exp_id} completed: {len(results_df)} predictions\")\n",
    "    return results_df\n",
    "\n",
    "# Run Tree-of-Thought experiments with new LLMs\n",
    "e10_df = run_tot_experiment(\n",
    "    test_df, lambda p: call_llama(p, model_name=\"openai/gpt-oss-20b\"), \"GPT OSS 20B\", \"E10\"\n",
    ")\n",
    "e11_df = run_tot_experiment(\n",
    "    test_df, lambda p: call_llama(p, model_name=\"openai/gpt-oss-120b\"), \"GPT OSS 120B\", \"E11\"\n",
    ")\n",
    "e12_df = run_tot_experiment(\n",
    "    test_df, lambda p: call_llama(p, model_name=\"llama-3.3-70b-versatile\"), \"Llama-3.3-70B\", \"E12\"\n",
    ")\n",
    "\n",
    "display(e10_df.head())\n",
    "display(e11_df.head())\n",
    "display(e12_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27302649",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9519858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TREE-OF-THOUGHT PERFORMANCE COMPARISON\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>Macro-Precision</th>\n",
       "      <th>Macro-Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E10a: GPT-OSS-20B (ToT)</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E11: GPT-OSS-120B (ToT)</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E12: Llama-3.3-70B (ToT)</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Experiment  Accuracy  Macro-F1  Macro-Precision  Macro-Recall\n",
       "0   E10a: GPT-OSS-20B (ToT)      0.96      0.63             0.61          0.65\n",
       "1   E11: GPT-OSS-120B (ToT)      0.90      0.74             0.68          0.95\n",
       "2  E12: Llama-3.3-70B (ToT)      0.93      0.77             0.97          0.70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "e10_metrics, e10_cm, e10_valid = calculate_metrics(e10_df, \"E10a: GPT-OSS-20B (ToT)\")\n",
    "e11_metrics, e11_cm, e11_valid = calculate_metrics(\n",
    "    e11_df, \"E11: GPT-OSS-120B (ToT)\"\n",
    ")\n",
    "e12_metrics, e12_cm, e12_valid = calculate_metrics(\n",
    "    e12_df, \"E12: Llama-3.3-70B (ToT)\"\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame([e10_metrics, e11_metrics, e12_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "911e697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35506a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TREE-OF-THOUGHT PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Experiment: E10\n",
      "Accuracy: 0.9592\n",
      "Macro-F1: 0.6286\n",
      "Macro-Precision: 0.6111\n",
      "Macro-Recall: 0.6515\n",
      "\n",
      "Per-class F1 Scores:\n",
      "  Positive: 0.9767\n",
      "  Negative: 0.0000\n",
      "  Neutral: 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidharthrai/Documents/LJMU - MS AI ML/LJMU TrackThesis v3/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "e10_metrics, e10_cm, e10_valid = calculate_metrics(e10_df, \"E10: GPT-OSS-20B ToT\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment: E10\")\n",
    "print(f\"Accuracy: {e10_metrics['Accuracy']:.4f}\")\n",
    "print(f\"Macro-F1: {e10_metrics['Macro-F1']:.4f}\")\n",
    "print(f\"Macro-Precision: {e10_metrics['Macro-Precision']:.4f}\")\n",
    "print(f\"Macro-Recall: {e10_metrics['Macro-Recall']:.4f}\")\n",
    "print(f\"\\nPer-class F1 Scores:\")\n",
    "print(f\"  Positive: {e10_metrics['Positive_F1']:.4f}\")\n",
    "print(f\"  Negative: {e10_metrics['Negative_F1']:.4f}\")\n",
    "print(f\"  Neutral: {e10_metrics['Neutral_F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3136ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TREE-OF-THOUGHT PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Experiment: E11\n",
      "Accuracy: 0.9032\n",
      "Macro-F1: 0.7362\n",
      "Macro-Precision: 0.6786\n",
      "Macro-Recall: 0.9489\n",
      "\n",
      "Per-class F1 Scores:\n",
      "  Positive: 0.9461\n",
      "  Negative: 0.0000\n",
      "  Neutral: 0.5263\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "e11_metrics, e11_cm, e11_valid = calculate_metrics(e11_df, \"E11: GPT-OSS-120B ToT\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment: E11\")\n",
    "print(f\"Accuracy: {e11_metrics['Accuracy']:.4f}\")\n",
    "print(f\"Macro-F1: {e11_metrics['Macro-F1']:.4f}\")\n",
    "print(f\"Macro-Precision: {e11_metrics['Macro-Precision']:.4f}\")\n",
    "print(f\"Macro-Recall: {e11_metrics['Macro-Recall']:.4f}\")\n",
    "print(f\"\\nPer-class F1 Scores:\")\n",
    "print(f\"  Positive: {e11_metrics['Positive_F1']:.4f}\")\n",
    "print(f\"  Negative: {e11_metrics['Negative_F1']:.4f}\")\n",
    "print(f\"  Neutral: {e11_metrics['Neutral_F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e4050db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TREE-OF-THOUGHT PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Experiment: E12\n",
      "Accuracy: 0.9333\n",
      "Macro-F1: 0.7676\n",
      "Macro-Precision: 0.9651\n",
      "Macro-Recall: 0.7000\n",
      "\n",
      "Per-class F1 Scores:\n",
      "  Positive: 0.9639\n",
      "  Negative: 0.0000\n",
      "  Neutral: 0.5714\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "e12_metrics, e12_cm, e12_valid = calculate_metrics(e12_df, \"E12: Llama-3.3-70B ToT\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment: E12\")\n",
    "print(f\"Accuracy: {e12_metrics['Accuracy']:.4f}\")\n",
    "print(f\"Macro-F1: {e12_metrics['Macro-F1']:.4f}\")\n",
    "print(f\"Macro-Precision: {e12_metrics['Macro-Precision']:.4f}\")\n",
    "print(f\"Macro-Recall: {e12_metrics['Macro-Recall']:.4f}\")\n",
    "print(f\"\\nPer-class F1 Scores:\")\n",
    "print(f\"  Positive: {e12_metrics['Positive_F1']:.4f}\")\n",
    "print(f\"  Negative: {e12_metrics['Negative_F1']:.4f}\")\n",
    "print(f\"  Neutral: {e12_metrics['Neutral_F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30d456",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd49c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Tree-of-Thought results saved\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e10_df.to_csv(f\"e10_GPT_OSS_20B_tot_{timestamp}.csv\", index=False)\n",
    "e11_df.to_csv(f\"e11_GPT_OSS_120B_flash_tot_{timestamp}.csv\", index=False)\n",
    "e12_df.to_csv(f\"e12_Llama_3.3_70B_tot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"tot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Tree-of-Thought results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec11747",
   "metadata": {},
   "source": [
    "## 7. Key Insights\n",
    "\n",
    "### Tree-of-Thought Analysis:\n",
    "\n",
    "1. **Multi-Path Reasoning**: How does exploring all three sentiment hypotheses affect accuracy?\n",
    "2. **Decision Quality**: Are ToT predictions more justified and explainable?\n",
    "3. **Computational Cost**: Does the added complexity justify performance gains?\n",
    "4. **Path Score Analysis**: Which sentiment hypotheses receive highest scores for which types of statements?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
