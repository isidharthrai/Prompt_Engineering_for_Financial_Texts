{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b799b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Initialize FinBERT for E12\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete (FinBERT loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bfa95",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50693acd",
   "metadata": {},
   "source": [
    "## 2. Tree-of-Thought Prompt Design\n",
    "\n",
    "**Multi-Path Reasoning**:\n",
    "- Path 1: Consider \"positive\" hypothesis\n",
    "- Path 2: Consider \"negative\" hypothesis  \n",
    "- Path 3: Consider \"neutral\" hypothesis\n",
    "- Evaluation: Score each path's evidence strength\n",
    "- Selection: Choose the most supported hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Tree-of-Thought prompt with multi-path exploration.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze this statement using a tree-of-thought approach.\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "TASK: Explore three possible sentiment classifications and select the best one.\n",
    "\n",
    "---\n",
    "PATH 1: Hypothesis = POSITIVE\n",
    "Consider if this statement represents positive news for investors.\n",
    "- What evidence supports this being positive?\n",
    "- What evidence contradicts this being positive?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 2: Hypothesis = NEGATIVE ⚠️\n",
    "Consider if this statement represents FINANCIAL DETERIORATION for investors.\n",
    "Look specifically for: losses, declines, revenue drops, margin compression, layoffs, failed ventures, widening losses, falling sales, cost increases, debt problems.\n",
    "- What evidence supports this being negative?\n",
    "- What evidence contradicts this being negative?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 3: Hypothesis = NEUTRAL\n",
    "Consider if this statement has no clear market impact.\n",
    "- What evidence supports this being neutral?\n",
    "- What evidence contradicts this being neutral?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "---\n",
    "FINAL DECISION:\n",
    "Based on evaluating all three paths, select the hypothesis with the strongest evidence.\n",
    "Provide ONLY this JSON format (no markdown, no extra text):\n",
    "Provide your final answer in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Explanation of why this hypothesis was selected over the others\",\n",
    "    \"path_scores\": {{\n",
    "        \"positive\": 0.0-1.0,\n",
    "        \"negative\": 0.0-1.0,\n",
    "        \"neutral\": 0.0-1.0\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = (\n",
    "    \"The company reported mixed results with revenue up 10% but margins declining.\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_tot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81294531",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama(prompt, model_name, temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=1500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON with path scores from ToT response\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"{\" in response_text:\n",
    "            start = response_text.find(\"{\")\n",
    "            end = response_text.rfind(\"}\") + 1\n",
    "            json_str = response_text[start:end]\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"\n",
    "    Call FinBERT for sentiment classification.\n",
    "    Note: FinBERT does not support Tree-of-Thought reasoning.\n",
    "    Returns direct classification result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence)[0]\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "        sentiment = label_map.get(result[\"label\"].lower(), \"neutral\")\n",
    "\n",
    "        return {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"confidence\": result[\"score\"],\n",
    "            \"rationale\": \"FinBERT direct inference (ToT reasoning not applicable)\",\n",
    "            \"path_scores\": {},\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"FinBERT error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined (including FinBERT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eac435",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb95a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset for comprehensive evaluation\n",
    "test_df = df.copy()  # Use complete dataset (2,217 samples)\n",
    "\n",
    "\n",
    "# Updated Tree-of-Thought experiments for new LLMs\n",
    "def run_tot_experiment(test_df, model_func, model_name, exp_id):\n",
    "    print(f\"Running {exp_id}: {model_name} (Tree-of-Thought)...\")\n",
    "    results = []\n",
    "\n",
    "    for idx, row in tqdm(\n",
    "        test_df.iterrows(), total=len(test_df), desc=f\"{exp_id} Progress\"\n",
    "    ):\n",
    "        prompt = create_tot_prompt(row[\"sentence\"])\n",
    "        response = model_func(prompt)\n",
    "\n",
    "        if response:\n",
    "            parsed = parse_response(response)\n",
    "            if parsed:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"sentence\": row[\"sentence\"],\n",
    "                        \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                        \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                        \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                        \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                        \"path_scores\": str(parsed.get(\"path_scores\", {})),\n",
    "                        \"full_response\": response[:700],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        time.sleep(0.5)  # Adjusted for new LLMs\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ {exp_id} completed: {len(results_df)} predictions\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run Tree-of-Thought experiments with new LLMs\n",
    "e10_df = run_tot_experiment(\n",
    "    test_df,\n",
    "    lambda p: call_llama(p, model_name=\"mixtral-8x7b-32768\"),\n",
    "    \"Mixtral-8x7B\",\n",
    "    \"E10\",\n",
    ")\n",
    "e11_df = run_tot_experiment(\n",
    "    test_df,\n",
    "    lambda p: call_llama(p, model_name=\"llama-3.1-70b-versatile\"),\n",
    "    \"Llama-3.1-70B\",\n",
    "    \"E11\",\n",
    ")\n",
    "\n",
    "# E12: FinBERT (direct inference, no ToT reasoning)\n",
    "print(\"Running E12: FinBERT (Tree-of-Thought - Direct Classification)...\")\n",
    "e12_results = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E12 Progress\"):\n",
    "    result = call_finbert(row[\"sentence\"])\n",
    "    if result:\n",
    "        e12_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": result[\"sentiment\"],\n",
    "                \"confidence\": result[\"confidence\"],\n",
    "                \"rationale\": result[\"rationale\"],\n",
    "                \"path_scores\": str(result[\"path_scores\"]),\n",
    "                \"full_response\": f\"FinBERT: {result['sentiment']} ({result['confidence']:.2f})\",\n",
    "            }\n",
    "        )\n",
    "    time.sleep(0.1)\n",
    "\n",
    "e12_df = pd.DataFrame(e12_results)\n",
    "print(f\"\\n✓ E12 completed: {len(e12_df)} predictions\")\n",
    "\n",
    "display(e10_df.head())\n",
    "display(e11_df.head())\n",
    "display(e12_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27302649",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e10_metrics, e10_cm, e10_valid = calculate_metrics(e10_df, \"E10: Mixtral-8x7B (ToT)\")\n",
    "e11_metrics, e11_cm, e11_valid = calculate_metrics(e11_df, \"E11: Llama-3.1-70B (ToT)\")\n",
    "e12_metrics, e12_cm, e12_valid = calculate_metrics(e12_df, \"E12: FinBERT (ToT)\")\n",
    "\n",
    "# Create metrics comparison table\n",
    "metrics_df = pd.DataFrame([e10_metrics, e11_metrics, e12_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\", \"MCC\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252dc56",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14735cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\", \"MCC\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e10_metrics, \"Mixtral-8x7B\"),\n",
    "        (e11_metrics, \"Llama-3.1-70B\"),\n",
    "        (e12_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Tree-of-Thought)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot, rotation=15, ha=\"right\")\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e10_metrics, \"Mixtral-8x7B\"),\n",
    "        (e11_metrics, \"Llama-3.1-70B\"),\n",
    "        (e12_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Tree-of-Thought)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tot_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e10_cm, \"E10: Mixtral-8x7B\"),\n",
    "        (e11_cm, \"E11: Llama-3.1-70B\"),\n",
    "        (e12_cm, \"E12: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Tree-of-Thought Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7e715",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e10_df.to_csv(f\"e10_Mixtral_8x7B_tot_{timestamp}.csv\", index=False)\n",
    "e11_df.to_csv(f\"e11_Llama_3.1_70B_tot_{timestamp}.csv\", index=False)\n",
    "e12_df.to_csv(f\"e12_FinBERT_tot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"tot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - e10_Mixtral_8x7B_tot_{timestamp}.csv\")\n",
    "print(f\"  - e11_Llama_3.1_70B_tot_{timestamp}.csv\")\n",
    "print(f\"  - e12_FinBERT_tot_{timestamp}.csv\")\n",
    "print(f\"  - tot_metrics_summary_{timestamp}.csv\")\n",
    "print(f\"  - tot_performance_comparison.png\")\n",
    "print(f\"  - tot_confusion_matrices.png\")\n",
    "print(f\"  - tot_confidence_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60681ef5",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Most Common Misclassifications\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS: TOP MISCLASSIFIED PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e10_valid, \"E10: Mixtral-8x7B\"),\n",
    "    (e11_valid, \"E11: Llama-3.1-70B\"),\n",
    "    (e12_valid, \"E12: FinBERT\"),\n",
    "]:\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    errors = df_result[df_result[\"true_sentiment\"] != df_result[\"predicted_sentiment\"]]\n",
    "\n",
    "    # Count confusion pairs\n",
    "    confusion_pairs = errors.groupby([\"true_sentiment\", \"predicted_sentiment\"]).size()\n",
    "    print(f\"Total Errors: {len(errors)}\")\n",
    "    print(\"\\nMost Common Misclassifications:\")\n",
    "    for (true_label, pred_label), count in (\n",
    "        confusion_pairs.sort_values(ascending=False).head(5).items()\n",
    "    ):\n",
    "        print(f\"  {true_label} → {pred_label}: {count} errors\")\n",
    "\n",
    "    # Show sample errors\n",
    "    print(f\"\\nSample Misclassified Sentences:\")\n",
    "    for idx, row in errors.head(3).iterrows():\n",
    "        print(f\"\\n  Sentence: {row['sentence'][:100]}...\")\n",
    "        print(\n",
    "            f\"  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Confidence: {row['confidence']:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543770e",
   "metadata": {},
   "source": [
    "## 9. Confidence Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [(e10_valid, \"Mixtral-8x7B\"), (e11_valid, \"Llama-3.1-70B\"), (e12_valid, \"FinBERT\")]\n",
    "):\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    correct_conf = df_result[df_result[\"correct\"]][\"confidence\"]\n",
    "    incorrect_conf = df_result[~df_result[\"correct\"]][\"confidence\"]\n",
    "\n",
    "    axes[idx].hist(\n",
    "        [correct_conf, incorrect_conf],\n",
    "        bins=20,\n",
    "        label=[\"Correct\", \"Incorrect\"],\n",
    "        alpha=0.7,\n",
    "        color=[\"green\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nMean Conf: Correct={correct_conf.mean():.3f}, Incorrect={incorrect_conf.mean():.3f}\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Distribution: Correct vs Incorrect Predictions (ToT)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tot_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1d093",
   "metadata": {},
   "source": [
    "## 10. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4381af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed metrics for each experiment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e10_valid, \"E10: Mixtral-8x7B (ToT)\"),\n",
    "    (e11_valid, \"E11: Llama-3.1-70B (ToT)\"),\n",
    "    (e12_valid, \"E12: FinBERT (ToT)\"),\n",
    "]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{exp_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            df_result[\"true_sentiment\"],\n",
    "            df_result[\"predicted_sentiment\"],\n",
    "            labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "            target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Per-class metrics summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "class_metrics_summary = metrics_df[\n",
    "    [\n",
    "        \"Experiment\",\n",
    "        \"Positive_Precision\",\n",
    "        \"Positive_Recall\",\n",
    "        \"Positive_F1\",\n",
    "        \"Negative_Precision\",\n",
    "        \"Negative_Recall\",\n",
    "        \"Negative_F1\",\n",
    "        \"Neutral_Precision\",\n",
    "        \"Neutral_Recall\",\n",
    "        \"Neutral_F1\",\n",
    "    ]\n",
    "]\n",
    "display(class_metrics_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827156da",
   "metadata": {},
   "source": [
    "## 11. Tree-of-Thought vs CoT vs Few-Shot vs Zero-Shot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e206fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from all prompting approaches for comprehensive comparison\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find the most recent results files from each approach\n",
    "zero_shot_metrics = glob.glob(\"../Zero_Shot/zero_shot_metrics_summary_*.csv\")\n",
    "few_shot_metrics = glob.glob(\"../Few_Shot/few_shot_metrics_summary_*.csv\")\n",
    "cot_metrics = glob.glob(\"../Chain_of_Thought/cot_metrics_summary_*.csv\")\n",
    "\n",
    "all_approaches = []\n",
    "\n",
    "# Load Zero-Shot results\n",
    "if zero_shot_metrics:\n",
    "    zs_df = pd.read_csv(sorted(zero_shot_metrics)[-1])  # Most recent\n",
    "    zs_df[\"Approach\"] = \"Zero-Shot\"\n",
    "    all_approaches.append(zs_df)\n",
    "\n",
    "# Load Few-Shot results\n",
    "if few_shot_metrics:\n",
    "    fs_df = pd.read_csv(sorted(few_shot_metrics)[-1])\n",
    "    fs_df[\"Approach\"] = \"Few-Shot\"\n",
    "    all_approaches.append(fs_df)\n",
    "\n",
    "# Load CoT results\n",
    "if cot_metrics:\n",
    "    cot_df = pd.read_csv(sorted(cot_metrics)[-1])\n",
    "    cot_df[\"Approach\"] = \"Chain-of-Thought\"\n",
    "    all_approaches.append(cot_df)\n",
    "\n",
    "# Add current ToT results\n",
    "tot_current = metrics_df.copy()\n",
    "tot_current[\"Approach\"] = \"Tree-of-Thought\"\n",
    "all_approaches.append(tot_current)\n",
    "\n",
    "# Combine all results\n",
    "if all_approaches:\n",
    "    comparison_df = pd.concat(all_approaches, ignore_index=True)\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    cols = [\n",
    "        \"Approach\",\n",
    "        \"Experiment\",\n",
    "        \"Accuracy\",\n",
    "        \"Macro-F1\",\n",
    "        \"MCC\",\n",
    "        \"Macro-Precision\",\n",
    "        \"Macro-Recall\",\n",
    "    ]\n",
    "    available_cols = [c for c in cols if c in comparison_df.columns]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPREHENSIVE PROMPTING APPROACH COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    display(comparison_df[available_cols].round(4))\n",
    "\n",
    "    # Best performance by approach\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"BEST PERFORMANCE BY APPROACH\")\n",
    "    print(\"=\" * 100)\n",
    "    best_by_approach = comparison_df.groupby(\"Approach\")[\n",
    "        [\"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "    ].max()\n",
    "    display(best_by_approach.round(4))\n",
    "\n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 100)\n",
    "    best_accuracy_row = comparison_df.loc[comparison_df[\"Accuracy\"].idxmax()]\n",
    "    best_f1_row = comparison_df.loc[comparison_df[\"Macro-F1\"].idxmax()]\n",
    "    best_mcc_row = comparison_df.loc[comparison_df[\"MCC\"].idxmax()]\n",
    "\n",
    "    print(\n",
    "        f\"✓ Best Accuracy: {best_accuracy_row['Experiment']} ({best_accuracy_row['Approach']}) = {best_accuracy_row['Accuracy']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"✓ Best Macro-F1: {best_f1_row['Experiment']} ({best_f1_row['Approach']}) = {best_f1_row['Macro-F1']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"✓ Best MCC: {best_mcc_row['Experiment']} ({best_mcc_row['Approach']}) = {best_mcc_row['MCC']:.4f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"⚠️ No comparison files found. Run other experiments first to enable full comparison.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7676296",
   "metadata": {},
   "source": [
    "## 12. Expected Conclusions from Tree-of-Thought Experiment\n",
    "\n",
    "### Multi-Path Reasoning Analysis\n",
    "\n",
    "**1. Hypothesis Exploration Impact**\n",
    "- **Strength**: ToT forces models to systematically evaluate all three sentiment possibilities (positive, negative, neutral) before making a decision\n",
    "- **Expected Behavior**: Should reduce \"first impression bias\" where models commit to an answer too quickly\n",
    "- **Path Score Analysis**: When all three paths have similar scores, this indicates an ambiguous/difficult sentence that may benefit from human review\n",
    "- **Justification Quality**: ToT provides explicit rationale comparing all options, making predictions more explainable than zero-shot\n",
    "\n",
    "**2. Computational Cost vs Performance Trade-Off**\n",
    "- **Token Usage**: ~3x higher than Zero-Shot due to exploring three hypothesis paths\n",
    "- **Cost Implication**: If ToT improves Macro-F1 by <5%, it may not justify 3x cost increase\n",
    "- **Latency**: Longer prompts = slower inference, problematic for real-time applications\n",
    "- **Recommendation**: Use ToT only if accuracy gains outweigh cost, or for high-stakes predictions where explainability matters\n",
    "\n",
    "**3. Negative Sentiment Detection (Critical)**\n",
    "- **Challenge**: Negative class is typically hardest to detect (lowest recall in all approaches)\n",
    "- **ToT Advantage**: Explicit \"negative hypothesis\" path with listed negative indicators (losses, declines, revenue drops, etc.) should improve negative recall\n",
    "- **Expected Improvement**: If ToT negative recall > CoT/Few-Shot/Zero-Shot, this validates the multi-path approach\n",
    "- **Business Impact**: Better negative detection = earlier risk identification in financial monitoring\n",
    "\n",
    "**4. Model-Specific Behaviors**\n",
    "\n",
    "   **Mixtral-8x7B (E10)**:\n",
    "   - **Architecture**: Mixture-of-Experts (8 specialists)\n",
    "   - **Expected Strength**: Different experts may naturally align with different hypothesis paths\n",
    "   - **Potential Weakness**: 20B params may struggle with complex multi-step reasoning\n",
    "   - **Prediction**: Moderate performance, good cost-efficiency\n",
    "\n",
    "   **Llama-3.1-70B (E11)**:\n",
    "   - **Architecture**: Dense 70B parameter model\n",
    "   - **Expected Strength**: Larger capacity = better complex reasoning and hypothesis evaluation\n",
    "   - **Expected Weakness**: May still lack financial domain knowledge\n",
    "   - **Prediction**: Best general performance among LLMs\n",
    "\n",
    "   **FinBERT (E12)**:\n",
    "   - **Architecture**: BERT-base fine-tuned on financial text\n",
    "   - **Expected Strength**: Deep financial domain knowledge, fast inference\n",
    "   - **Expected Weakness**: Cannot perform actual ToT reasoning (no multi-path exploration)\n",
    "   - **Note**: E12 uses direct classification, not ToT reasoning (included for comparison only)\n",
    "   - **Prediction**: Competitive performance despite no ToT reasoning, due to domain specialization\n",
    "\n",
    "**5. Confidence Calibration**\n",
    "- **Well-Calibrated**: Correct predictions should have higher avg confidence than incorrect ones\n",
    "- **ToT Advantage**: Path score agreement can indicate confidence (all paths agree → high confidence)\n",
    "- **Calibration Gap**: Measure difference between avg confidence for correct vs incorrect predictions\n",
    "- **Expected Finding**: ToT should have better calibration than Zero-Shot due to multi-path validation\n",
    "\n",
    "**6. Error Pattern Analysis**\n",
    "- **Common Errors**: neutral→negative, positive→neutral (from all previous experiments)\n",
    "- **ToT Mitigation**: Explicit neutral hypothesis path should reduce false positives (mislabeling neutral as positive/negative)\n",
    "- **High-Confidence Errors**: When ToT is confident but wrong, examine path scores to understand why all paths converged on wrong answer\n",
    "- **Learning Opportunity**: Error analysis reveals whether ToT reasoning is fundamentally flawed or just needs better path prompts\n",
    "\n",
    "**7. Decision Quality Metrics**\n",
    "- **MCC (Matthews Correlation Coefficient)**: Better metric than accuracy for imbalanced classes\n",
    "- **Per-Class F1**: Track Positive_F1, Negative_F1, Neutral_F1 separately to identify class-specific strengths/weaknesses\n",
    "- **Macro-F1 vs Weighted-F1**: Macro treats all classes equally (better for our balanced dataset), Weighted accounts for class imbalance\n",
    "- **Expected Ranking**: MCC should rank models similarly to Macro-F1, but with more penalty for class imbalance\n",
    "\n",
    "**8. Comparison with Other Approaches**\n",
    "\n",
    "   **vs Zero-Shot**:\n",
    "   - **Expected**: ToT should outperform due to structured reasoning\n",
    "   - **Trade-Off**: 3x cost, slower inference\n",
    "   - **Decision**: Use ToT if accuracy gain > 5%\n",
    "\n",
    "   **vs Few-Shot**:\n",
    "   - **Expected**: Close competition (Few-Shot has 6 examples, ToT has 3 paths)\n",
    "   - **Advantage ToT**: No need to curate examples\n",
    "   - **Advantage Few-Shot**: Provides concrete patterns to mimic\n",
    "\n",
    "   **vs Chain-of-Thought**:\n",
    "   - **Expected**: ToT should match or exceed CoT\n",
    "   - **CoT**: Linear 5-step reasoning\n",
    "   - **ToT**: Parallel 3-path hypothesis exploration\n",
    "   - **Key Difference**: ToT explores multiple possibilities, CoT follows single reasoning chain\n",
    "\n",
    "**9. Production Deployment Considerations**\n",
    "\n",
    "   **Best for Accuracy** (regardless of cost):\n",
    "   - Highest Macro-F1 model\n",
    "   - Priority: Minimize misclassifications\n",
    "\n",
    "   **Best for Cost-Efficiency**:\n",
    "   - Acceptable F1 (>0.75) at lowest token cost\n",
    "   - Priority: Balance accuracy and operational cost\n",
    "   - Likely winner: Zero-Shot or FinBERT\n",
    "\n",
    "   **Best for Reliability** (lowest variance, highest negative recall):\n",
    "   - Consistent performance across all classes\n",
    "   - Priority: Avoid missing negative signals\n",
    "   - Metric: Highest Negative_Recall + lowest stddev across runs\n",
    "\n",
    "   **Best for Explainability**:\n",
    "   - ToT provides path scores and explicit reasoning\n",
    "   - Priority: Audit trail for regulatory compliance\n",
    "   - Use case: High-stakes decisions requiring justification\n",
    "\n",
    "**10. Hypothesis Validation**\n",
    "- **Hypothesis 1**: ToT improves negative recall over simpler approaches ✓/✗\n",
    "- **Hypothesis 2**: ToT provides better confidence calibration ✓/✗\n",
    "- **Hypothesis 3**: ToT justifies 3x cost increase with >5% accuracy improvement ✓/✗\n",
    "- **Hypothesis 4**: Larger models (Llama-70B) benefit more from ToT reasoning than smaller models ✓/✗\n",
    "- **Hypothesis 5**: FinBERT's domain knowledge compensates for lack of ToT reasoning ✓/✗\n",
    "\n",
    "**11. Limitations and Future Work**\n",
    "- **Path Design**: Current 3-path design is manual; could optimize with automatic path generation\n",
    "- **Path Scoring**: Models may not accurately self-score hypothesis strength\n",
    "- **Prompt Sensitivity**: ToT performance heavily depends on how negative indicators are phrased\n",
    "- **Computational Cost**: 3x token usage limits scalability for high-volume applications\n",
    "- **Future**: Experiment with 2-path (positive/negative only) or 4-path (adding \"mixed\" sentiment) designs\n",
    "\n",
    "**12. Final Recommendation**\n",
    "- **Academic/Research**: Use ToT for explainability and to understand model reasoning processes\n",
    "- **Production/Enterprise**: Use ToT only if:\n",
    "  1. Accuracy gain > 5% over simpler approaches\n",
    "  2. Explainability is required for compliance\n",
    "  3. Computational cost is acceptable for use case\n",
    "- **Most Practical**: FinBERT (E12) or best Few-Shot model likely offers best accuracy/cost trade-off for real-world financial sentiment monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
