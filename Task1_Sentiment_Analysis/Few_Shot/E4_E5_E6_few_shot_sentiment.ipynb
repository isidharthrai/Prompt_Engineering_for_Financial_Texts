{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (No installation required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for sentiment analysis with open-source LLMs\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "# API setup for LLMs (Groq, dotenv)\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load FinBERT model for local inference\n",
    "print(\"Loading FinBERT model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete for open-source LLM sentiment analysis\")\n",
    "print(f\"✓ Groq API configured: {bool(GROQ_API_KEY)}\")\n",
    "print(f\"✓ FinBERT loaded on {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bad17b",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Load the FinancialPhraseBank dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa34453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset (highest quality)\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb34c9",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Examples\n",
    "\n",
    "Curated examples (2 positive, 2 negative, 1 neutral) representing typical financial sentiment patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated few-shot examples - BALANCED with emphasis on negative detection\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"sentence\": \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Operating profit increased significantly, indicating improved financial performance.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Net sales increased by 18.5% to EUR 167.8 million compared to the previous year.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Strong revenue growth of 18.5% signals business expansion and market success.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company reported a net loss of EUR 2.5 million compared to a profit of EUR 1.2 million in the previous quarter.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Shift from profit to loss represents deteriorating financial health.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Sales decreased by 15% year-over-year due to weakening demand in key markets.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Significant sales decline indicates business challenges and market difficulties.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Operating loss widened to EUR 5.8 million from EUR 3.2 million in the same period last year.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Widening losses show worsening profitability and deteriorating business conditions.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company announced the appointment of a new chief financial officer effective next month.\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"rationale\": \"Executive appointment is routine corporate news without clear financial impact.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Few-Shot Examples:\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "    print(f\"Rationale: {ex['rationale']}\")\n",
    "    print(f\"Sentence: {ex['sentence']}\")\n",
    "    print(f\"\\nExample {i} [{ex['sentiment'].upper()}]:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa5433",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Prompt Design\n",
    "\n",
    "Prompt template for open-source LLMs using few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a few-shot prompt with 6 labeled examples (balanced representation).\n",
    "    \"\"\"\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "        examples_text += f\"\"\"\\nExample {i}:\n",
    "Sentence: \"{ex[\"sentence\"]}\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"sentiment\": \"{ex[\"sentiment\"]}\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"rationale\": \"{ex[\"rationale\"]}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze financial statements with precision.\n",
    "\n",
    "Classify the sentiment as \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Guidelines:\n",
    "- Positive: Financial improvements, growth, profits, revenue increases, cost reductions, successful expansions\n",
    "- Negative: Financial declines, losses, revenue drops, cost increases, widening losses, failed ventures, layoffs\n",
    "- Neutral: Factual statements with no clear financial impact, routine announcements, balanced mixed signals\n",
    "\n",
    "⚠️ IMPORTANT: Pay special attention to negative indicators (losses, declines, decreases, deterioration).\n",
    "\n",
    "Here are {len(FEW_SHOT_EXAMPLES)} examples to learn from:\n",
    "{examples_text}\n",
    "\n",
    "Now classify this new statement:\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"The company's quarterly revenue exceeded analyst expectations by 12%.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"FEW-SHOT PROMPT EXAMPLE\")\n",
    "\n",
    "print(\"=\" * 80)print(create_few_shot_prompt(test_sentence)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233c39f",
   "metadata": {},
   "source": [
    "## 4. Model Inference Functions\n",
    "\n",
    "Functions for calling open-source LLMs (Groq API) and parsing their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Llama or other OSS models via Groq API\n",
    "def call_llama(prompt, model_name=\"finbert\", temperature=0.0):\n",
    "    \"\"\"Call Llama or other OSS models via Groq API, with model selection\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"Call FinBERT for sentiment classification (local inference)\"\"\"\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence[:512])  # FinBERT max length\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "        return {\n",
    "            \"sentiment\": label_map.get(result[0][\"label\"].lower(), \"neutral\"),\n",
    "            \"confidence\": result[0][\"score\"],\n",
    "            \"rationale\": f\"FinBERT classification: {result[0]['label']}\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"FinBERT error: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dd8cb",
   "metadata": {},
   "source": [
    "## 5. Run Experiments\n",
    "\n",
    "Run few-shot sentiment analysis using open-source LLMs:\n",
    "- **E4:** Mixtral-8x7B (Few-Shot)\n",
    "- **E5:** Llama-3.1-70B (Few-Shot)\n",
    "- **E6:** FinBERT (Few-Shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c552bf",
   "metadata": {},
   "source": [
    "### E4: Mixtral-8x7B (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset for comprehensive evaluation\n",
    "test_df = df.copy()\n",
    "\n",
    "# E4: Mixtral-8x7B (Few-Shot)\n",
    "print(\"Running E4: Mixtral-8x7B (Few-Shot)...\")\n",
    "e4_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E4 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e4_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e4_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "e4_df = pd.DataFrame(e4_results)\n",
    "print(f\"\\n✓ E4 completed: {len(e4_df)} predictions\")\n",
    "display(e4_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa2de7",
   "metadata": {},
   "source": [
    "### E5: Llama-3.1-70B (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5: Llama-3.1-70B (Few-Shot)\n",
    "print(\"Running E5: Llama-3.1-70B (Few-Shot)...\")\n",
    "e5_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E5 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e5_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e5_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e5_df = pd.DataFrame(e5_results)\n",
    "print(f\"\\n✓ E5 completed: {len(e5_df)} predictions\")\n",
    "display(e5_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088fd90",
   "metadata": {},
   "source": [
    "### E6: FinBERT (Few-Shot)\n",
    "\n",
    "⚠️ **Note:** FinBERT uses its pre-trained weights and cannot leverage few-shot examples (no in-context learning capability). This experiment uses the same FinBERT model as E3 (Zero-Shot) for consistency and cost-benefit comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E6: FinBERT (Few-Shot - Note: Cannot use few-shot examples)\n",
    "print(\"Running E6: FinBERT (Few-Shot)...\")\n",
    "print(\"⚠️ Note: FinBERT uses pre-trained weights, cannot leverage few-shot examples\")\n",
    "e6_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E6 Progress\"):\n",
    "    # FinBERT uses direct inference (few-shot examples are not applicable)\n",
    "    result = call_finbert(row[\"sentence\"])\n",
    "\n",
    "    if result:\n",
    "        e6_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": result.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0),\n",
    "                \"rationale\": result.get(\"rationale\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        e6_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"FinBERT inference error\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "e6_df = pd.DataFrame(e6_results)\n",
    "print(f\"\\n✓ E6 completed: {len(e6_df)} predictions\")\n",
    "display(e6_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea0062",
   "metadata": {},
   "source": [
    "## 6. Calculate Metrics\n",
    "\n",
    "Compute accuracy, F1, precision, recall, and confusion matrices for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e4_metrics, e4_cm, e4_valid = calculate_metrics(e4_df, \"E4: Mixtral-8x7B (Few-Shot)\")\n",
    "e5_metrics, e5_cm, e5_valid = calculate_metrics(e5_df, \"E5: Llama-3.1-70B (Few-Shot)\")\n",
    "e6_metrics, e6_cm, e6_valid = calculate_metrics(e6_df, \"E6: FinBERT (Few-Shot)\")\n",
    "\n",
    "# Create metrics comparison table\n",
    "metrics_df = pd.DataFrame([e4_metrics, e5_metrics, e6_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEW-SHOT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e02f3",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Visualize performance metrics and confusion matrices for all open-source LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e4_metrics, \"Mixtral-8x7B\"),\n",
    "        (e5_metrics, \"Llama-3.1-70B\"),\n",
    "        (e6_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Few-Shot)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e4_metrics, \"Mixtral-8x7B\"),\n",
    "        (e5_metrics, \"Llama-3.1-70B\"),\n",
    "        (e6_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Few-Shot)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74071aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e4_cm, \"E4: Mixtral-8x7B (Few-Shot)\"),\n",
    "        (e5_cm, \"E5: Llama-3.1-70B (Few-Shot)\"),\n",
    "        (e6_cm, \"E6: FinBERT (Few-Shot)\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Greens\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Few-Shot Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a39b34",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save experiment results and metrics to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e4_df.to_csv(f\"e4_mixtral_8x7b_few_shot_{timestamp}.csv\", index=False)\n",
    "e5_df.to_csv(f\"e5_llama_3_1_70b_few_shot_{timestamp}.csv\", index=False)\n",
    "e6_df.to_csv(f\"e6_finbert_few_shot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"few_shot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dd679",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9046b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Most Common Misclassifications\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS: FEW-SHOT MISCLASSIFICATION PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e4_valid, \"E4: Mixtral-8x7B\"),\n",
    "    (e5_valid, \"E5: Llama-3.1-70B\"),\n",
    "    (e6_valid, \"E6: FinBERT\"),\n",
    "]:\n",
    "    print(f\"\\n{exp_name}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Identify errors\n",
    "    errors = df_result[df_result[\"true_sentiment\"] != df_result[\"predicted_sentiment\"]]\n",
    "\n",
    "    # Count error types\n",
    "    error_types = (\n",
    "        errors.groupby([\"true_sentiment\", \"predicted_sentiment\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    error_types = error_types.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTotal Errors: {len(errors)} / {len(df_result)} ({len(errors) / len(df_result) * 100:.2f}%)\"\n",
    "    )\n",
    "    print(\"\\nMost Common Error Types:\")\n",
    "    display(error_types.head(5))\n",
    "\n",
    "    # Show examples of worst errors (high confidence, wrong prediction)\n",
    "    if len(errors) > 0:\n",
    "        worst_errors = errors.nlargest(3, \"confidence\")\n",
    "        print(f\"\\nTop 3 High-Confidence Errors:\")\n",
    "        for idx, row in worst_errors.iterrows():\n",
    "            print(\n",
    "                f\"\\n  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Conf: {row['confidence']:.2f}\"\n",
    "            )\n",
    "            print(f\"  Sentence: {row['sentence'][:120]}...\")\n",
    "            print(f\"  Rationale: {row['rationale']}\")\n",
    "\n",
    "# Class-wise Performance Comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS-WISE PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_comparison = []\n",
    "for metrics, model_name in [\n",
    "    (e4_metrics, \"Mixtral-8x7B\"),\n",
    "    (e5_metrics, \"Llama-3.1-70B\"),\n",
    "    (e6_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    for sentiment in [\"Positive\", \"Negative\", \"Neutral\"]:\n",
    "        class_comparison.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Class\": sentiment,\n",
    "                \"Precision\": metrics[f\"{sentiment}_Precision\"],\n",
    "                \"Recall\": metrics[f\"{sentiment}_Recall\"],\n",
    "                \"F1-Score\": metrics[f\"{sentiment}_F1\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "class_df = pd.DataFrame(class_comparison)\n",
    "\n",
    "# Pivot for better visualization\n",
    "for metric in [\"Precision\", \"Recall\", \"F1-Score\"]:\n",
    "    print(f\"\\n{metric} by Class:\")\n",
    "    pivot = class_df.pivot(index=\"Class\", columns=\"Model\", values=metric)\n",
    "    display(pivot.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE METRICS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2d654",
   "metadata": {},
   "source": [
    "## 10. Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [(e4_valid, \"Mixtral-8x7B\"), (e5_valid, \"Llama-3.1-70B\"), (e6_valid, \"FinBERT\")]\n",
    "):\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    correct_conf = df_result[df_result[\"correct\"]][\"confidence\"]\n",
    "    incorrect_conf = df_result[~df_result[\"correct\"]][\"confidence\"]\n",
    "\n",
    "    axes[idx].hist(\n",
    "        [correct_conf, incorrect_conf],\n",
    "        bins=20,\n",
    "        label=[\"Correct\", \"Incorrect\"],\n",
    "        alpha=0.7,\n",
    "        color=[\"green\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nMean Conf: Correct={correct_conf.mean():.3f}, Incorrect={incorrect_conf.mean():.3f}\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Distribution: Correct vs Incorrect Predictions (Few-Shot)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIDENCE CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "for df_result, exp_name in [\n",
    "    (e4_valid, \"E4: Mixtral-8x7B\"),\n",
    "    (e5_valid, \"E5: Llama-3.1-70B\"),\n",
    "    (e6_valid, \"E6: FinBERT\"),\n",
    "]:\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    avg_conf_correct = df_result[df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    avg_conf_incorrect = df_result[~df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Average Confidence (Correct): {avg_conf_correct:.4f}\")\n",
    "    print(f\"  Average Confidence (Incorrect): {avg_conf_incorrect:.4f}\")\n",
    "    print(f\"  Calibration Gap: {calibration_gap:.4f}\")\n",
    "    print(f\"  Total Correct: {df_result['correct'].sum()} / {len(df_result)}\")\n",
    "\n",
    "    # Confidence by sentiment class\n",
    "    print(f\"\\n  Confidence by Predicted Class:\")\n",
    "    for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        class_df = df_result[df_result[\"predicted_sentiment\"] == sentiment]\n",
    "        if len(class_df) > 0:\n",
    "            print(\n",
    "                f\"    {sentiment.capitalize()}: {class_df['confidence'].mean():.4f} (n={len(class_df)})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668de8b",
   "metadata": {},
   "source": [
    "## 11. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Reports\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e4_valid, \"E4: Mixtral-8x7B\"),\n",
    "    (e5_valid, \"E5: Llama-3.1-70B\"),\n",
    "    (e6_valid, \"E6: FinBERT\"),\n",
    "]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{exp_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            df_result[\"true_sentiment\"],\n",
    "            df_result[\"predicted_sentiment\"],\n",
    "            labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "            target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Class-wise Metrics Summary Table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS-WISE METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for metrics, model in [\n",
    "    (e4_metrics, \"Mixtral-8x7B\"),\n",
    "    (e5_metrics, \"Llama-3.1-70B\"),\n",
    "    (e6_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Model\": model,\n",
    "            \"Pos_P\": metrics[\"Positive_Precision\"],\n",
    "            \"Pos_R\": metrics[\"Positive_Recall\"],\n",
    "            \"Pos_F1\": metrics[\"Positive_F1\"],\n",
    "            \"Neg_P\": metrics[\"Negative_Precision\"],\n",
    "            \"Neg_R\": metrics[\"Negative_Recall\"],\n",
    "            \"Neg_F1\": metrics[\"Negative_F1\"],\n",
    "            \"Neu_P\": metrics[\"Neutral_Precision\"],\n",
    "            \"Neu_R\": metrics[\"Neutral_Recall\"],\n",
    "            \"Neu_F1\": metrics[\"Neutral_F1\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nPer-Class Metrics (P=Precision, R=Recall, F1=F1-Score):\")\n",
    "display(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b2571",
   "metadata": {},
   "source": [
    "## 12. Few-Shot vs Zero-Shot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Zero-Shot results (if available)\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEW-SHOT vs ZERO-SHOT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to load the latest zero-shot results\n",
    "zero_shot_files = glob.glob(\"../Zero_Shot/zero_shot_metrics_summary_*.csv\")\n",
    "\n",
    "if zero_shot_files:\n",
    "    # Get the most recent file\n",
    "    latest_zero_shot = max(zero_shot_files, key=os.path.getctime)\n",
    "    print(f\"\\nLoading Zero-Shot results from: {os.path.basename(latest_zero_shot)}\")\n",
    "\n",
    "    try:\n",
    "        zero_shot_df = pd.read_csv(latest_zero_shot)\n",
    "\n",
    "        # Combine Few-Shot and Zero-Shot for comparison\n",
    "        few_shot_df = metrics_df.copy()\n",
    "        few_shot_df[\"Approach\"] = \"Few-Shot\"\n",
    "        zero_shot_df[\"Approach\"] = \"Zero-Shot\"\n",
    "\n",
    "        # Select key metrics for comparison\n",
    "        comparison_cols = [\n",
    "            \"Experiment\",\n",
    "            \"Approach\",\n",
    "            \"Accuracy\",\n",
    "            \"Macro-F1\",\n",
    "            \"Negative_F1\",\n",
    "            \"Positive_F1\",\n",
    "            \"Neutral_F1\",\n",
    "        ]\n",
    "\n",
    "        combined = pd.concat(\n",
    "            [\n",
    "                few_shot_df[comparison_cols]\n",
    "                if all(col in few_shot_df.columns for col in comparison_cols)\n",
    "                else few_shot_df,\n",
    "                zero_shot_df[comparison_cols]\n",
    "                if all(col in zero_shot_df.columns for col in comparison_cols)\n",
    "                else zero_shot_df,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"\\nKey Metrics Comparison:\")\n",
    "        display(combined[comparison_cols].round(4))\n",
    "\n",
    "        # Calculate improvements\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"IMPROVEMENT: Few-Shot vs Zero-Shot\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for i in range(min(3, len(few_shot_df))):\n",
    "            model_name = [\"Mixtral-8x7B\", \"Llama-3.1-70B\", \"FinBERT\"][i]\n",
    "            if i < len(zero_shot_df):\n",
    "                fs_macro = few_shot_df.iloc[i][\"Macro-F1\"]\n",
    "                zs_macro = zero_shot_df.iloc[i][\"Macro-F1\"]\n",
    "                improvement = (\n",
    "                    ((fs_macro - zs_macro) / zs_macro * 100) if zs_macro > 0 else 0\n",
    "                )\n",
    "\n",
    "                fs_neg = few_shot_df.iloc[i][\"Negative_F1\"]\n",
    "                zs_neg = zero_shot_df.iloc[i][\"Negative_F1\"]\n",
    "                neg_improvement = (\n",
    "                    ((fs_neg - zs_neg) / zs_neg * 100)\n",
    "                    if zs_neg > 0\n",
    "                    else float(\"inf\")\n",
    "                    if fs_neg > 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(\n",
    "                    f\"  Macro-F1: {zs_macro:.4f} → {fs_macro:.4f} ({improvement:+.2f}%)\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Negative F1: {zs_neg:.4f} → {fs_neg:.4f} ({neg_improvement:+.2f}% improvement)\"\n",
    "                    if neg_improvement != float(\"inf\")\n",
    "                    else f\"  Negative F1: {zs_neg:.4f} → {fs_neg:.4f} (∞% - from zero!)\"\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Could not load zero-shot results: {str(e)}\")\n",
    "        print(\"Run Zero-Shot experiments first for comparison.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No Zero-Shot results found in ../Zero_Shot/\")\n",
    "    print(\"Run Zero-Shot experiments first to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301381a1",
   "metadata": {},
   "source": [
    "## 13. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1301f3",
   "metadata": {},
   "source": [
    "### Expected Conclusions from Few-Shot Experiment\n",
    "\n",
    "### 1. **Few-Shot vs Zero-Shot Performance Gains**\n",
    "   - **Hypothesis**: Few-shot learning should outperform zero-shot, especially on negative sentiment (minority class)\n",
    "   - **Expected Improvement**: 5-15% boost in Macro-F1 compared to zero-shot baseline\n",
    "   - **Negative Class Boost**: Few-shot examples specifically target negative detection (3/6 examples are negative)\n",
    "\n",
    "### 2. **Impact of Curated Examples**\n",
    "   - **Balanced Representation**: 6 examples (3 negative, 2 positive, 1 neutral) address class imbalance\n",
    "   - **Explicit Negative Indicators**: Examples demonstrate losses, declines, and layoffs\n",
    "   - **Learning Effect**: Models should better recognize financial distress signals\n",
    "\n",
    "### 3. **Model Comparison Insights**\n",
    "   - **Size vs Examples**: Does Llama-3.1-70B benefit more from examples than Mixtral-8x7B?\n",
    "   - **FinBERT Advantage**: Pretrained financial domain knowledge + few-shot may yield best results\n",
    "   - **Consistency**: Check if all models improve uniformly or if some benefit more\n",
    "\n",
    "### 4. **Class-Specific Performance**\n",
    "   - **Negative F1 Improvement**: Primary success metric - should increase significantly from zero-shot\n",
    "   - **Positive/Neutral Stability**: Should maintain high performance while improving negative detection\n",
    "   - **Confusion Reduction**: Fewer neutral→negative and negative→neutral misclassifications\n",
    "\n",
    "### 5. **Confidence Calibration**\n",
    "   - **Higher Confidence on Negatives**: Examples should reduce uncertainty on negative predictions\n",
    "   - **Calibration Gap**: Expect narrower gap between correct/incorrect prediction confidence\n",
    "   - **Class-Specific Confidence**: Negative predictions should approach positive/neutral confidence levels\n",
    "\n",
    "### 6. **Error Analysis Patterns**\n",
    "   - **Reduced High-Confidence Errors**: Examples should prevent overconfident misclassifications\n",
    "   - **Boundary Cases**: Models may still struggle with subtle negatives (e.g., \"widening losses\")\n",
    "   - **Context Understanding**: Few-shot should improve handling of comparative statements\n",
    "\n",
    "### 7. **Prompt Engineering Validation**\n",
    "   - **Example Quality**: 6 examples sufficient or need more for negative class?\n",
    "   - **Format Consistency**: JSON compliance should improve with explicit examples\n",
    "   - **Rationale Quality**: Models should provide more specific, example-aligned reasoning\n",
    "\n",
    "### 8. **Production Readiness Assessment**\n",
    "   - **Deployment Threshold**: Macro-F1 > 0.80 and Negative F1 > 0.50 for production use\n",
    "   - **Cost-Benefit**: Few-shot adds ~500 tokens/request - is performance gain worth API cost increase?\n",
    "   - **Comparison with CoT**: Establish baseline for Chain-of-Thought and Tree-of-Thought experiments\n",
    "   - **Model Selection**: Identify best model for advancing to CoT (E7-E9) and ToT (E10-E12)\n",
    "\n",
    "### 9. **Key Success Metrics**\n",
    "   - **Primary**: Negative F1 > 0.40 (vs ~0.20-0.30 in zero-shot)\n",
    "   - **Secondary**: Macro-F1 > 0.75\n",
    "   - **Tertiary**: MCC (Matthews Correlation Coefficient) improvement shows better overall discrimination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
