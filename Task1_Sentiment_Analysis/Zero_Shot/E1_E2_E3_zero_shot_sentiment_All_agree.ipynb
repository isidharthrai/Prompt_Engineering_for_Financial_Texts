{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn groq python-dotenv tqdm transformers torch -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "# API setup\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure APIs\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load FinBERT model\n",
    "print(\"Loading FinBERT model...\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Groq API configured: {bool(GROQ_API_KEY)}\")\n",
    "print(f\"✓ FinBERT loaded on {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c68069",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset (highest quality)\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320df8cb",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Simple, direct instruction with no examples. Enforces strict JSON output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b12ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a zero-shot prompt for sentiment classification.\n",
    "    No examples provided - model relies on pretrained knowledge.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze the sentiment of financial statements with precision.\n",
    "\n",
    "Task: Classify the sentiment of the following financial statement as \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Guidelines:\n",
    "- Positive: Financial improvements, growth, profits, revenue increases, cost reductions, successful expansions\n",
    "- Negative: Financial declines, losses, revenue drops, cost increases, failed ventures, layoffs, legal issues\n",
    "- Neutral: Factual statements with no clear financial impact, routine announcements, or balanced mixed signals\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation in one sentence\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"ZERO-SHOT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_zero_shot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b7e2e",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llama(prompt, model_name, temperature=0.0):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    last_error = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries} failed: {last_error}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(\"Parse error: {}\".format(str(e)[:100]))\n",
    "        print(\"Raw response was:\\n{}\".format(response_text))\n",
    "        # Fallback: try to extract sentiment with regex\n",
    "        response_lower = response_text.lower() if response_text else \"\"\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"Call FinBERT for sentiment classification\"\"\"\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence[:512])  # FinBERT max length\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "        return {\n",
    "            \"sentiment\": label_map.get(result[0][\"label\"].lower(), \"neutral\"),\n",
    "            \"confidence\": result[0][\"score\"],\n",
    "            \"rationale\": f\"FinBERT classification: {result[0]['label']}\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"FinBERT error: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Model inference functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API calls with a sample sentence\n",
    "test_sentence = \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\"\n",
    "test_prompt = create_zero_shot_prompt(test_sentence)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Testing Mixtral-8x7B...\")\n",
    "print(\"=\" * 80)\n",
    "mixtral_response = call_llama(test_prompt, model_name=\"mixtral-8x7b-32768\")\n",
    "if mixtral_response:\n",
    "    print(f\"✓ Response received (length: {len(mixtral_response)})\")\n",
    "    print(f\"Response: {mixtral_response[:500]}\")\n",
    "    parsed = parse_response(mixtral_response)\n",
    "    if parsed:\n",
    "        print(f\"✓ Parsed successfully: {parsed}\")\n",
    "    else:\n",
    "        print(\"✗ Failed to parse response\")\n",
    "else:\n",
    "    print(\"✗ Mixtral call failed - no response received\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing Llama-3.1-70B...\")\n",
    "print(\"=\" * 80)\n",
    "llama_response = call_llama(test_prompt, model_name=\"llama-3.1-70b-versatile\")\n",
    "if llama_response:\n",
    "    print(f\"✓ Response received (length: {len(llama_response)})\")\n",
    "    print(f\"Response: {llama_response[:500]}\")\n",
    "    parsed = parse_response(llama_response)\n",
    "    if parsed:\n",
    "        print(f\"✓ Parsed successfully: {parsed}\")\n",
    "    else:\n",
    "        print(\"✗ Failed to parse response\")\n",
    "else:\n",
    "    print(\"✗ Llama call failed - no response received\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing FinBERT...\")\n",
    "print(\"=\" * 80)\n",
    "finbert_result = call_finbert(test_sentence)\n",
    "if finbert_result:\n",
    "    print(f\"✓ FinBERT result: {finbert_result}\")\n",
    "else:\n",
    "    print(\"✗ FinBERT call failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac076fa6",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f7ff1",
   "metadata": {},
   "source": [
    "### E1: Mixtral-8x7B (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset\n",
    "test_df = df.copy()\n",
    "\n",
    "# E1: Mixtral-8x7B\n",
    "print(\"Running E1: Mixtral-8x7B (Zero-Shot)...\")\n",
    "e1_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E1 Progress\"):\n",
    "    prompt = create_zero_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e1_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e1_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "e1_df = pd.DataFrame(e1_results)\n",
    "print(f\"\\n✓ E1 completed: {len(e1_df)} predictions\")\n",
    "display(e1_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da33bd",
   "metadata": {},
   "source": [
    "### E2: Llama-3.1-70B (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2: Llama-3.1-70B (Zero-Shot)\n",
    "print(\"Running E2: Llama-3.1-70B (Zero-Shot)...\")\n",
    "e2_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E2 Progress\"):\n",
    "    prompt = create_zero_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt, model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e2_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e2_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e2_df = pd.DataFrame(e2_results)\n",
    "print(f\"\\n✓ E2 completed: {len(e2_df)} predictions\")\n",
    "display(e2_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49f817",
   "metadata": {},
   "source": [
    "### E3: FinBERT (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3: FinBERT (Zero-Shot)\n",
    "print(\"Running E3: FinBERT (Zero-Shot)...\")\n",
    "e3_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E3 Progress\"):\n",
    "    result = call_finbert(row[\"sentence\"])\n",
    "\n",
    "    if result:\n",
    "        e3_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": result.get(\"sentiment\", \"unknown\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0),\n",
    "                \"rationale\": result.get(\"rationale\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        e3_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"FinBERT error\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "e3_df = pd.DataFrame(e3_results)\n",
    "print(f\"\\n✓ E3 completed: {len(e3_df)} predictions\")\n",
    "display(e3_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0e570",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e1_metrics, e1_cm, e1_valid = calculate_metrics(e1_df, \"E1: Mixtral-8x7B\")\n",
    "e2_metrics, e2_cm, e2_valid = calculate_metrics(e2_df, \"E2: Llama-3.1-70B\")\n",
    "e3_metrics, e3_cm, e3_valid = calculate_metrics(e3_df, \"E3: FinBERT\")\n",
    "\n",
    "# Create metrics comparison table\n",
    "metrics_df = pd.DataFrame([e1_metrics, e2_metrics, e3_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZERO-SHOT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc0d5b",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e1_metrics, \"Mixtral-8x7B\"),\n",
    "        (e2_metrics, \"Llama-3.1-70B\"),\n",
    "        (e3_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Zero-Shot)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (e1_metrics, \"Mixtral-8x7B\"),\n",
    "        (e2_metrics, \"Llama-3.1-70B\"),\n",
    "        (e3_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Zero-Shot)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e1_cm, \"E1: Mixtral-8x7B\"),\n",
    "        (e2_cm, \"E2: Llama-3.1-70B\"),\n",
    "        (e3_cm, \"E3: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Zero-Shot Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657e204",
   "metadata": {},
   "source": [
    "## 7. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports\n",
    "for df_result, exp_name in [\n",
    "    (e1_valid, \"E1: Mixtral-8x7B\"),\n",
    "    (e2_valid, \"E2: Llama-3.1-70B\"),\n",
    "    (e3_valid, \"E3: FinBERT\"),\n",
    "]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CLASSIFICATION REPORT: {exp_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        classification_report(\n",
    "            df_result[\"true_sentiment\"],\n",
    "            df_result[\"predicted_sentiment\"],\n",
    "            labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "            target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Class-wise Metrics Summary Table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS-WISE METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for metrics, model in [\n",
    "    (e1_metrics, \"Mixtral-8x7B\"),\n",
    "    (e2_metrics, \"Llama-3.1-70B\"),\n",
    "    (e3_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Model\": model,\n",
    "            \"Pos_P\": metrics[\"Positive_Precision\"],\n",
    "            \"Pos_R\": metrics[\"Positive_Recall\"],\n",
    "            \"Pos_F1\": metrics[\"Positive_F1\"],\n",
    "            \"Neg_P\": metrics[\"Negative_Precision\"],\n",
    "            \"Neg_R\": metrics[\"Negative_Recall\"],\n",
    "            \"Neg_F1\": metrics[\"Negative_F1\"],\n",
    "            \"Neu_P\": metrics[\"Neutral_Precision\"],\n",
    "            \"Neu_R\": metrics[\"Neutral_Recall\"],\n",
    "            \"Neu_F1\": metrics[\"Neutral_F1\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nPer-Class Metrics (P=Precision, R=Recall, F1=F1-Score):\")\n",
    "display(summary_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6827921",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aede83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e1_df.to_csv(f\"e1_mixtral_8x7b_zero_shot_{timestamp}.csv\", index=False)\n",
    "e2_df.to_csv(f\"e2_llama_3_1_70b_zero_shot_{timestamp}.csv\", index=False)\n",
    "e3_df.to_csv(f\"e3_finbert_zero_shot_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df.to_csv(f\"zero_shot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - e1_mixtral_8x7b_zero_shot_{timestamp}.csv\")\n",
    "print(f\"  - e2_llama_3_1_70b_zero_shot_{timestamp}.csv\")\n",
    "print(f\"  - e3_finbert_zero_shot_{timestamp}.csv\")\n",
    "print(f\"  - zero_shot_metrics_summary_{timestamp}.csv\")\n",
    "print(f\"  - zero_shot_performance_comparison.png\")\n",
    "print(f\"  - zero_shot_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070cbea",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Most Common Misclassifications\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS: TOP MISCLASSIFIED PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (e1_valid, \"E1: Mixtral-8x7B\"),\n",
    "    (e2_valid, \"E2: Llama-3.1-70B\"),\n",
    "    (e3_valid, \"E3: FinBERT\"),\n",
    "]:\n",
    "    print(f\"\\n{exp_name}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Identify errors\n",
    "    errors = df_result[df_result[\"true_sentiment\"] != df_result[\"predicted_sentiment\"]]\n",
    "\n",
    "    # Count error types\n",
    "    error_types = (\n",
    "        errors.groupby([\"true_sentiment\", \"predicted_sentiment\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    error_types = error_types.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTotal Errors: {len(errors)} / {len(df_result)} ({len(errors) / len(df_result) * 100:.2f}%)\"\n",
    "    )\n",
    "    print(\"\\nMost Common Error Types:\")\n",
    "    display(error_types.head(5))\n",
    "\n",
    "    # Show examples of worst errors (high confidence, wrong prediction)\n",
    "    if len(errors) > 0:\n",
    "        worst_errors = errors.nlargest(3, \"confidence\")\n",
    "        print(f\"\\nTop 3 High-Confidence Errors:\")\n",
    "        for idx, row in worst_errors.iterrows():\n",
    "            print(\n",
    "                f\"\\n  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Conf: {row['confidence']:.2f}\"\n",
    "            )\n",
    "            print(f\"  Sentence: {row['sentence'][:120]}...\")\n",
    "            print(f\"  Rationale: {row['rationale']}\")\n",
    "\n",
    "# Class-wise Performance Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"CLASS-WISE PERFORMANCE BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_comparison = []\n",
    "for metrics, model_name in [\n",
    "    (e1_metrics, \"Mixtral-8x7B\"),\n",
    "    (e2_metrics, \"Llama-3.1-70B\"),\n",
    "    (e3_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    for sentiment in [\"Positive\", \"Negative\", \"Neutral\"]:\n",
    "        class_comparison.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Class\": sentiment,\n",
    "                \"Precision\": metrics[f\"{sentiment}_Precision\"],\n",
    "                \"Recall\": metrics[f\"{sentiment}_Recall\"],\n",
    "                \"F1-Score\": metrics[f\"{sentiment}_F1\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "class_df = pd.DataFrame(class_comparison)\n",
    "\n",
    "# Pivot for better visualization\n",
    "for metric in [\"Precision\", \"Recall\", \"F1-Score\"]:\n",
    "    print(f\"\\n{metric} by Class:\")\n",
    "    pivot = class_df.pivot(index=\"Class\", columns=\"Model\", values=metric)\n",
    "    display(pivot.round(4))\n",
    "\n",
    "# Identify weakest class per model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEAKEST CLASS IDENTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "for metrics, model_name in [\n",
    "    (e1_metrics, \"Mixtral-8x7B\"),\n",
    "    (e2_metrics, \"Llama-3.1-70B\"),\n",
    "    (e3_metrics, \"FinBERT\"),\n",
    "]:\n",
    "    f1_scores = {\n",
    "        \"Positive\": metrics[\"Positive_F1\"],\n",
    "        \"Negative\": metrics[\"Negative_F1\"],\n",
    "        \"Neutral\": metrics[\"Neutral_F1\"],\n",
    "    }\n",
    "    weakest = min(f1_scores, key=f1_scores.get)\n",
    "    print(f\"{model_name}: Weakest class = {weakest} (F1={f1_scores[weakest]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e5418",
   "metadata": {},
   "source": [
    "## 10. Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [(e1_valid, \"Mixtral-8x7B\"), (e2_valid, \"Llama-3.1-70B\"), (e3_valid, \"FinBERT\")]\n",
    "):\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    correct_conf = df_result[df_result[\"correct\"]][\"confidence\"]\n",
    "    incorrect_conf = df_result[~df_result[\"correct\"]][\"confidence\"]\n",
    "\n",
    "    axes[idx].hist(\n",
    "        [correct_conf, incorrect_conf],\n",
    "        bins=20,\n",
    "        label=[\"Correct\", \"Incorrect\"],\n",
    "        alpha=0.7,\n",
    "        color=[\"green\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nMean Conf: Correct={correct_conf.mean():.3f}, Incorrect={incorrect_conf.mean():.3f}\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Distribution: Correct vs Incorrect Predictions\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIDENCE CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "for df_result, exp_name in [\n",
    "    (e1_valid, \"E1: Mixtral-8x7B\"),\n",
    "    (e2_valid, \"E2: Llama-3.1-70B\"),\n",
    "    (e3_valid, \"E3: FinBERT\"),\n",
    "]:\n",
    "    df_result[\"correct\"] = (\n",
    "        df_result[\"true_sentiment\"] == df_result[\"predicted_sentiment\"]\n",
    "    )\n",
    "\n",
    "    avg_conf_correct = df_result[df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    avg_conf_incorrect = df_result[~df_result[\"correct\"]][\"confidence\"].mean()\n",
    "    calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Average Confidence (Correct): {avg_conf_correct:.4f}\")\n",
    "    print(f\"  Average Confidence (Incorrect): {avg_conf_incorrect:.4f}\")\n",
    "    print(f\"  Calibration Gap: {calibration_gap:.4f}\")\n",
    "    print(f\"  Total Correct: {df_result['correct'].sum()} / {len(df_result)}\")\n",
    "\n",
    "    # Confidence by sentiment class\n",
    "    print(f\"\\n  Confidence by Predicted Class:\")\n",
    "    for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        class_df = df_result[df_result[\"predicted_sentiment\"] == sentiment]\n",
    "        if len(class_df) > 0:\n",
    "            print(\n",
    "                f\"    {sentiment.capitalize()}: {class_df['confidence'].mean():.4f} (n={len(class_df)})\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb4400",
   "metadata": {},
   "source": [
    "## 11. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7530b",
   "metadata": {},
   "source": [
    "### Expected Conclusions from Zero-Shot Experiment\n",
    "\n",
    "### 1. **Model Performance Comparison**\n",
    "   - **Best Performer**: Identify which model (Mixtral-8x7B, 120B, or FinBERT) achieves highest Macro-F1\n",
    "   - **Accuracy vs F1**: Check if high accuracy masks poor performance on minority class (negative: 13%)\n",
    "   - **Consistency**: Compare Macro-F1 vs Weighted-F1 to assess class imbalance handling\n",
    "\n",
    "### 2. **Class-Specific Insights**\n",
    "   - **Negative Sentiment Challenge**: Expect lowest F1 on negative class (only 297/2,217 examples = 13.4%)\n",
    "   - **Positive vs Neutral Confusion**: Analyze if models confuse growth statements with neutral facts\n",
    "   - **Per-Class Ranking**: Typically: Positive > Neutral > Negative (due to data distribution)\n",
    "\n",
    "### 3. **Zero-Shot Limitations**\n",
    "   - **No Examples Handicap**: Models rely purely on pretrained financial knowledge\n",
    "   - **Expected Baseline**: Should be lower than Few-Shot, CoT, and ToT approaches\n",
    "   - **Benchmark Value**: Establishes minimum performance without prompt engineering\n",
    "\n",
    "### 4. **Prompt Engineering Impact**\n",
    "   - **Improved vs Original**: Compare new prompt results against previous 100-sample run\n",
    "   - **Negative Detection**: Check if explicit negative indicators (losses, declines) improved recall\n",
    "   - **JSON Compliance**: Monitor parsing error rate\n",
    "\n",
    "### 5. **Confidence Calibration Analysis**\n",
    "   - **Overconfidence**: Models often show high confidence on incorrect predictions\n",
    "   - **Class-Specific Confidence**: Positive predictions typically more confident than negative\n",
    "   - **Calibration Gap**: Measure difference between confidence and actual accuracy\n",
    "\n",
    "### 6. **Cross-Model Patterns**\n",
    "   - **Size Matters**: Does Llama-3.1-70B outperform 20B significantly?\n",
    "   - **Architecture Differences**: Llama vs GPT-style models on financial domain\n",
    "   - **Error Consistency**: Do all models make similar mistakes?\n",
    "\n",
    "### 7. **Actionable Recommendations**\n",
    "   - If Negative F1 < 0.3: Zero-shot insufficient, must use Few-Shot with negative examples\n",
    "   - If Macro-F1 < 0.6: Implement CoT to force reasoning about financial indicators\n",
    "   - If high confidence on errors: Add uncertainty quantification to prompts\n",
    "\n",
    "### 8. **Business Impact Assessment**\n",
    "   - **Deployment Readiness**: Macro-F1 > 0.75 needed for production use\n",
    "   - **Cost-Benefit**: Compare API costs vs performance gains from advanced prompting\n",
    "   - **Risk Analysis**: False negatives (missing bad news) more costly than false positives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
