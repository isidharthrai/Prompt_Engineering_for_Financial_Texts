{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c984f276",
   "metadata": {},
   "source": [
    "# Comprehensive Results Comparison: All 12 Experiments\n",
    "\n",
    "**Experiment Matrix**: 4 Approaches √ó 3 Models = 12 Experiments\n",
    "\n",
    "| Approach | Mixtral-8x7B-32768 | Llama-3.1-70B-Versatile | FinBERT |\n",
    "|----------|-------------------|-------------------------|---------|\n",
    "| **Zero-Shot** | E1 | E2 | E3 |\n",
    "| **Few-Shot** | E4 | E5 | E6 |\n",
    "| **Chain-of-Thought** | E7 | E8 | E9 |\n",
    "| **Tree-of-Thought** | E10 | E11 | E12 |\n",
    "\n",
    "**Dataset**: FinancialPhraseBank Sentences_AllAgree.txt (2,217 samples)\n",
    "\n",
    "This notebook provides a comprehensive comparison across all experiments with automatic data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73be17",
   "metadata": {},
   "source": [
    "## 1. Load All Results\n",
    "\n",
    "Load metrics summaries from all prompting strategy experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf84f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_id, model, strategy):\n",
    "    \"\"\"Calculate comprehensive metrics for an experiment\"\"\"\n",
    "\n",
    "    # Filter valid predictions\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    if valid_df.empty:\n",
    "        print(f\"‚ö†Ô∏è  {exp_id}: No valid predictions found\")\n",
    "        return None\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    macro_precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Parsing errors\n",
    "    parsing_errors = len(df) - len(valid_df)\n",
    "    error_rate = parsing_errors / len(df) if len(df) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Experiment\": exp_id,\n",
    "        \"Model\": model,\n",
    "        \"Strategy\": strategy,\n",
    "        \"Total_Samples\": len(df),\n",
    "        \"Valid_Predictions\": len(valid_df),\n",
    "        \"Parsing_Errors\": parsing_errors,\n",
    "        \"Error_Rate\": error_rate,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Macro-F1\": macro_f1,\n",
    "        \"Weighted-F1\": weighted_f1,\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"Positive-F1\": f1_per_class[0],\n",
    "        \"Negative-F1\": f1_per_class[1],\n",
    "        \"Neutral-F1\": f1_per_class[2],\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate metrics for all loaded experiments\n",
    "all_metrics = []\n",
    "for exp_id, df in results_data.items():\n",
    "    exp_info = experiments[exp_id]\n",
    "    metrics = calculate_metrics(df, exp_id, exp_info[\"model\"], exp_info[\"strategy\"])\n",
    "    if metrics:\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä CALCULATING METRICS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nProcessing {len(results_data)} experiments...\\n\")\n",
    "\n",
    "# Create metrics DataFrame\n",
    "if all_metrics:\n",
    "    results_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "    # Validation summary\n",
    "    total_samples = results_df[\"Total_Samples\"].sum()\n",
    "    valid_predictions = results_df[\"Valid_Predictions\"].sum()\n",
    "    parsing_errors = results_df[\"Parsing_Errors\"].sum()\n",
    "    avg_error_rate = results_df[\"Error_Rate\"].mean()\n",
    "\n",
    "    print(\"‚úÖ Metrics calculated successfully!\\n\")\n",
    "    print(f\"üìä Data Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total samples processed: {total_samples:,}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Valid predictions: {valid_predictions:,} ({valid_predictions / total_samples * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Parsing errors: {parsing_errors:,} ({avg_error_rate * 100:.2f}% average)\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üìà COMPREHENSIVE METRICS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    display(\n",
    "        results_df[\n",
    "            [\n",
    "                \"Experiment\",\n",
    "                \"Model\",\n",
    "                \"Strategy\",\n",
    "                \"Accuracy\",\n",
    "                \"Macro-F1\",\n",
    "                \"Negative-F1\",\n",
    "                \"Error_Rate\",\n",
    "            ]\n",
    "        ].round(4)\n",
    "    )\n",
    "\n",
    "    # Performance alerts\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"‚ö° PERFORMANCE ALERTS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    high_error = results_df[results_df[\"Error_Rate\"] > 0.05]\n",
    "    if len(high_error) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  HIGH PARSING ERROR RATE (>5%): {len(high_error)} experiments\")\n",
    "        for _, row in high_error.iterrows():\n",
    "            print(\n",
    "                f\"   ‚Ä¢ {row['Experiment']}: {row['Error_Rate'] * 100:.2f}% errors ({row['Parsing_Errors']} samples)\"\n",
    "            )\n",
    "\n",
    "    low_neg_f1 = results_df[results_df[\"Negative-F1\"] < 0.5]\n",
    "    if len(low_neg_f1) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  LOW NEGATIVE-F1 (<0.50): {len(low_neg_f1)} experiments\")\n",
    "        print(\"   (Critical for financial risk detection!)\")\n",
    "        for _, row in low_neg_f1.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['Experiment']}: Negative-F1 = {row['Negative-F1']:.4f}\")\n",
    "\n",
    "    if len(high_error) == 0 and len(low_neg_f1) == 0:\n",
    "        print(\"\\n‚úÖ No critical issues detected!\")\n",
    "\n",
    "else:\n",
    "    results_df = pd.DataFrame()  # Empty dataframe for safety\n",
    "    print(\"‚ùå ERROR: No metrics calculated - no valid experiment data\\n\")\n",
    "    print(\"üîç Possible causes:\")\n",
    "    print(\"   ‚Ä¢ No experiment files loaded\")\n",
    "    print(\"   ‚Ä¢ All predictions failed validation\")\n",
    "    print(\"   ‚Ä¢ Missing required columns in CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92557838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all 12 experiments with file patterns\n",
    "experiments = {\n",
    "    # Zero-Shot Experiments\n",
    "    \"E1\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e1_gpt_oss_20b_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"E2\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e2_gpt_oss_120b_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"E3\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e3_llama_zero_shot_*.csv\",\n",
    "    },\n",
    "    # Few-Shot Experiments\n",
    "    \"E4\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e4_gpt_oss_20b_few_shot_*.csv\",\n",
    "    },\n",
    "    \"E5\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e5_gpt_oss_120b_few_shot_*.csv\",\n",
    "    },\n",
    "    \"E6\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e6_llama_few_shot_*.csv\",\n",
    "    },\n",
    "    # Chain-of-Thought Experiments\n",
    "    \"E7\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e7_GPT_OSS_20B_cot_*.csv\",\n",
    "    },\n",
    "    \"E8\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e8_GPT_OSS_120B_cot_*.csv\",\n",
    "    },\n",
    "    \"E9\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e9_Llama-3.3-70B_cot_*.csv\",\n",
    "    },\n",
    "    # Tree-of-Thought Experiments\n",
    "    \"E10\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e10_GPT_OSS_20B_tot_*.csv\",\n",
    "    },\n",
    "    \"E11\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e11_GPT_OSS_120B_flash_tot_*.csv\",\n",
    "    },\n",
    "    \"E12\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e12_Llama_3.3_70B_tot_*.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Function to load experiment results with robust error handling\n",
    "def load_experiment_data(exp_id, exp_info):\n",
    "    \"\"\"Load experiment data with error handling and validation\"\"\"\n",
    "    try:\n",
    "        files = glob(exp_info[\"file\"])\n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è  {exp_id}: No files found matching '{exp_info['file']}'\")\n",
    "            return None\n",
    "\n",
    "        # Get most recent file\n",
    "        latest_file = max(files)\n",
    "        df = pd.read_csv(latest_file)\n",
    "\n",
    "        # Validate required columns\n",
    "        required_cols = [\"true_sentiment\", \"predicted_sentiment\"]\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            print(f\"‚ùå {exp_id}: Missing required columns in {latest_file}\")\n",
    "            return None\n",
    "\n",
    "        print(\n",
    "            f\"‚úì {exp_id}: Loaded {len(df):,} samples from {os.path.basename(latest_file)}\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {exp_id}: Error - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load all experiment results\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä LOADING EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nAttempting to load 12 experiments (E1-E12)...\\n\")\n",
    "\n",
    "results_data = {}\n",
    "loading_errors = []\n",
    "missing_files = []\n",
    "\n",
    "for exp_id, exp_info in experiments.items():\n",
    "    df = load_experiment_data(exp_id, exp_info)\n",
    "    if df is not None:\n",
    "        results_data[exp_id] = df\n",
    "    else:\n",
    "        if not glob(exp_info[\"file\"]):\n",
    "            missing_files.append(\n",
    "                f\"{exp_id} ({exp_info['model']} + {exp_info['strategy']})\"\n",
    "            )\n",
    "        else:\n",
    "            loading_errors.append(exp_id)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà LOADING SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "loaded_count = len(results_data)\n",
    "total_count = len(experiments)\n",
    "success_rate = (loaded_count / total_count) * 100\n",
    "\n",
    "if loaded_count == total_count:\n",
    "    print(f\"\\n‚úÖ SUCCESS: All {total_count} experiments loaded ({success_rate:.0f}%)\")\n",
    "    print(f\"\\n‚úì Loaded: {', '.join(sorted(results_data.keys()))}\")\n",
    "elif loaded_count > 0:\n",
    "    print(\n",
    "        f\"\\n‚ö†Ô∏è  PARTIAL: {loaded_count}/{total_count} experiments loaded ({success_rate:.0f}%)\"\n",
    "    )\n",
    "    print(f\"\\n‚úì Loaded: {', '.join(sorted(results_data.keys()))}\")\n",
    "\n",
    "    if loading_errors:\n",
    "        print(\n",
    "            f\"\\n‚ö†Ô∏è  Loading errors ({len(loading_errors)}): {', '.join(loading_errors)}\"\n",
    "        )\n",
    "        print(\"   Check file format and required columns.\")\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"\\n‚ùå Missing files ({len(missing_files)}):\")\n",
    "        for mf in missing_files:\n",
    "            print(f\"   ‚Ä¢ {mf}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: No experiments loaded (0/{total_count})\")\n",
    "    print(\"\\nüîç Troubleshooting:\")\n",
    "    print(\"   1. Run experiment notebooks (E1-E12) to generate CSV files\")\n",
    "    print(\"   2. Check that notebooks are in correct directories:\")\n",
    "    print(\"      ‚Ä¢ Zero_Shot/E1_E2_E3_zero_shot_sentiment_All_agree.ipynb\")\n",
    "    print(\"      ‚Ä¢ Few_Shot/E4_E5_E6_few_shot_sentiment.ipynb\")\n",
    "    print(\"      ‚Ä¢ Chain_of_Thought/E7_E8_E9_cot_sentiment.ipynb\")\n",
    "    print(\"      ‚Ä¢ Tree_of_Thought/E10_tot_sentiment.ipynb\")\n",
    "    print(\n",
    "        \"   3. Verify CSV files contain 'true_sentiment' and 'predicted_sentiment' columns\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úì Loaded ({loaded_count}): {', '.join(sorted(results_data.keys()))}\")\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"\\n‚ùå Missing files ({len(missing_files)}):\")\n",
    "        for exp in missing_files:\n",
    "            print(f\"   ‚Ä¢ {exp}\")\n",
    "        print(\n",
    "            \"\\nüí° TIP: Run the corresponding experiment notebooks to generate these files.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb33680",
   "metadata": {},
   "source": [
    "## 2. Calculate Metrics for All Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfae4a",
   "metadata": {},
   "source": [
    "## 3. Strategy-wise Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9080a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üîÑ Analyzing strategy performance...\")\n",
    "\n",
    "    # Group by strategy\n",
    "    strategy_summary = results_df.groupby(\"Strategy\")[\n",
    "        [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\", \"Negative-F1\"]\n",
    "    ].mean()\n",
    "\n",
    "    strategies_found = len(strategy_summary)\n",
    "    print(f\"‚úì Found {strategies_found} unique strategies in data\")\n",
    "\n",
    "    # Order strategies by complexity\n",
    "    strategy_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "    strategy_summary = strategy_summary.reindex(\n",
    "        [s for s in strategy_order if s in strategy_summary.index]\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AVERAGE PERFORMANCE BY PROMPTING STRATEGY\")\n",
    "    print(\"=\" * 80)\n",
    "    display(strategy_summary.round(4))\n",
    "\n",
    "    # Visualize strategy comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Performance metrics\n",
    "    strategy_summary[[\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\"]].plot(\n",
    "        kind=\"bar\", ax=axes[0], width=0.8, alpha=0.8\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Prompting Strategy\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_title(\n",
    "        \"Performance Comparison Across Prompting Strategies\",\n",
    "        fontsize=15,\n",
    "        weight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    axes[0].set_xticklabels(strategy_summary.index, rotation=45, ha=\"right\")\n",
    "    axes[0].legend(title=\"Metrics\", fontsize=11)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Negative-F1 trend (critical metric)\n",
    "    x = np.arange(len(strategy_summary))\n",
    "    axes[1].plot(\n",
    "        x,\n",
    "        strategy_summary[\"Negative-F1\"],\n",
    "        marker=\"o\",\n",
    "        linewidth=3,\n",
    "        markersize=10,\n",
    "        color=\"#E74C3C\",\n",
    "    )\n",
    "    axes[1].axhline(\n",
    "        y=0.5, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Minimum Threshold (0.5)\"\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Prompting Complexity ‚Üí\", fontsize=13, weight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Negative-F1 Score\", fontsize=13, weight=\"bold\")\n",
    "    axes[1].set_title(\n",
    "        \"Negative Class Detection (Critical Metric)\", fontsize=15, weight=\"bold\", pad=20\n",
    "    )\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(strategy_summary.index, rotation=45, ha=\"right\")\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"strategy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping strategy comparison - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810571a",
   "metadata": {},
   "source": [
    "## 4. Model-wise Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74301217",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üîÑ Analyzing model performance...\")\n",
    "\n",
    "    # Group by model\n",
    "    model_summary = results_df.groupby(\"Model\")[\n",
    "        [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\", \"Negative-F1\", \"Error_Rate\"]\n",
    "    ].mean()\n",
    "\n",
    "    models_found = len(model_summary)\n",
    "    print(f\"‚úì Found {models_found} unique models in data\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AVERAGE PERFORMANCE BY MODEL (across all strategies)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(model_summary.round(4))\n",
    "\n",
    "    # Visualize model comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Performance metrics\n",
    "    model_summary[[\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\"]].plot(\n",
    "        kind=\"bar\",\n",
    "        ax=axes[0],\n",
    "        width=0.7,\n",
    "        alpha=0.8,\n",
    "        color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"],\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Model\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_title(\n",
    "        \"Performance Comparison Across Models\", fontsize=15, weight=\"bold\", pad=20\n",
    "    )\n",
    "    axes[0].set_xticklabels(model_summary.index, rotation=45, ha=\"right\")\n",
    "    axes[0].legend(title=\"Metrics\", fontsize=11)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Negative-F1 and Error Rate\n",
    "    x = np.arange(len(model_summary))\n",
    "    width = 0.35\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    bars1 = ax2.bar(\n",
    "        x - width / 2,\n",
    "        model_summary[\"Negative-F1\"],\n",
    "        width,\n",
    "        label=\"Negative-F1\",\n",
    "        color=\"#27AE60\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    ax2_twin = ax2.twinx()\n",
    "    bars2 = ax2_twin.bar(\n",
    "        x + width / 2,\n",
    "        model_summary[\"Error_Rate\"],\n",
    "        width,\n",
    "        label=\"Error Rate\",\n",
    "        color=\"#E74C3C\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    ax2.set_xlabel(\"Model\", fontsize=13, weight=\"bold\")\n",
    "    ax2.set_ylabel(\"Negative-F1 Score\", fontsize=12, weight=\"bold\", color=\"#27AE60\")\n",
    "    ax2_twin.set_ylabel(\n",
    "        \"Parsing Error Rate\", fontsize=12, weight=\"bold\", color=\"#E74C3C\"\n",
    "    )\n",
    "    ax2.set_title(\n",
    "        \"Model Reliability: Detection vs Errors\", fontsize=15, weight=\"bold\", pad=20\n",
    "    )\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(model_summary.index, rotation=45, ha=\"right\")\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2_twin.set_ylim([0, max(0.5, model_summary[\"Error_Rate\"].max() * 1.2)])\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"#27AE60\")\n",
    "    ax2_twin.tick_params(axis=\"y\", labelcolor=\"#E74C3C\")\n",
    "    ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Combined legend\n",
    "    lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model comparison - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d152cc8",
   "metadata": {},
   "source": [
    "## 5. Heatmap: Model √ó Strategy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and len(results_df) >= 3:\n",
    "    # Create pivot tables for heatmaps\n",
    "    strategy_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "\n",
    "    pivot_accuracy = results_df.pivot(\n",
    "        index=\"Model\", columns=\"Strategy\", values=\"Accuracy\"\n",
    "    )\n",
    "    pivot_accuracy = pivot_accuracy.reindex(\n",
    "        columns=[s for s in strategy_order if s in pivot_accuracy.columns]\n",
    "    )\n",
    "\n",
    "    pivot_f1 = results_df.pivot(index=\"Model\", columns=\"Strategy\", values=\"Macro-F1\")\n",
    "    pivot_f1 = pivot_f1.reindex(\n",
    "        columns=[s for s in strategy_order if s in pivot_f1.columns]\n",
    "    )\n",
    "\n",
    "    pivot_neg_f1 = results_df.pivot(\n",
    "        index=\"Model\", columns=\"Strategy\", values=\"Negative-F1\"\n",
    "    )\n",
    "    pivot_neg_f1 = pivot_neg_f1.reindex(\n",
    "        columns=[s for s in strategy_order if s in pivot_neg_f1.columns]\n",
    "    )\n",
    "\n",
    "    # Visualize as heatmaps\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    # Accuracy heatmap\n",
    "    sns.heatmap(\n",
    "        pivot_accuracy,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar_kws={\"label\": \"Accuracy\"},\n",
    "        ax=axes[0],\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[0].set_title(\n",
    "        \"Accuracy by Model and Strategy\", fontsize=13, weight=\"bold\", pad=15\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Prompting Strategy\", fontsize=11, weight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Model\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "    # Macro-F1 heatmap\n",
    "    sns.heatmap(\n",
    "        pivot_f1,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"RdYlGn\",\n",
    "        cbar_kws={\"label\": \"Macro-F1\"},\n",
    "        ax=axes[1],\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[1].set_title(\n",
    "        \"Macro-F1 by Model and Strategy\", fontsize=13, weight=\"bold\", pad=15\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Prompting Strategy\", fontsize=11, weight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Model\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "    # Negative-F1 heatmap (CRITICAL)\n",
    "    sns.heatmap(\n",
    "        pivot_neg_f1,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Reds\",\n",
    "        cbar_kws={\"label\": \"Negative-F1\"},\n",
    "        ax=axes[2],\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[2].set_title(\n",
    "        \"Negative-F1 by Model and Strategy (Critical)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "        pad=15,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"Prompting Strategy\", fontsize=11, weight=\"bold\")\n",
    "    axes[2].set_ylabel(\"Model\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"performance_heatmaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Best performing combinations\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BEST PERFORMING COMBINATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    best_acc = results_df.loc[results_df[\"Accuracy\"].idxmax()]\n",
    "    best_f1 = results_df.loc[results_df[\"Macro-F1\"].idxmax()]\n",
    "    best_neg = results_df.loc[results_df[\"Negative-F1\"].idxmax()]\n",
    "\n",
    "    print(\n",
    "        f\"Best Accuracy: {best_acc['Experiment']} ({best_acc['Model']} + {best_acc['Strategy']}) = {best_acc['Accuracy']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Best Macro-F1: {best_f1['Experiment']} ({best_f1['Model']} + {best_f1['Strategy']}) = {best_f1['Macro-F1']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Best Negative-F1: {best_neg['Experiment']} ({best_neg['Model']} + {best_neg['Strategy']}) = {best_neg['Negative-F1']:.4f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping heatmaps - insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50d0e9",
   "metadata": {},
   "source": [
    "## 6. Best Performing Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0842ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    # Find top 5 configurations by different metrics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP 5 CONFIGURATIONS BY ACCURACY\")\n",
    "    print(\"=\" * 80)\n",
    "    top_accuracy = results_df.nlargest(min(5, len(results_df)), \"Accuracy\")[\n",
    "        [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"Negative-F1\"]\n",
    "    ]\n",
    "    display(top_accuracy.round(4))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP 5 CONFIGURATIONS BY MACRO-F1\")\n",
    "    print(\"=\" * 80)\n",
    "    top_f1 = results_df.nlargest(min(5, len(results_df)), \"Macro-F1\")[\n",
    "        [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"Negative-F1\"]\n",
    "    ]\n",
    "    display(top_f1.round(4))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP 5 CONFIGURATIONS BY NEGATIVE-F1 (Critical for Financial Risk)\")\n",
    "    print(\"=\" * 80)\n",
    "    top_neg = results_df.nlargest(min(5, len(results_df)), \"Negative-F1\")[\n",
    "        [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"Negative-F1\"]\n",
    "    ]\n",
    "    display(top_neg.round(4))\n",
    "\n",
    "    # Overall best configuration\n",
    "    best_overall = results_df.loc[results_df[\"Macro-F1\"].idxmax()]\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üèÜ BEST OVERALL CONFIGURATION (by Macro-F1)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Experiment: {best_overall['Experiment']}\")\n",
    "    print(f\"Model: {best_overall['Model']}\")\n",
    "    print(f\"Strategy: {best_overall['Strategy']}\")\n",
    "    print(f\"Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "    print(f\"Macro-F1: {best_overall['Macro-F1']:.4f}\")\n",
    "    print(f\"Negative-F1: {best_overall['Negative-F1']:.4f}\")\n",
    "    print(f\"Error Rate: {best_overall['Error_Rate']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No configurations to rank - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637bb0c",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis & Improvement Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and \"Zero-Shot\" in results_df[\"Strategy\"].values:\n",
    "    # Calculate improvement from baseline (zero-shot)\n",
    "    baseline_acc = results_df[results_df[\"Strategy\"] == \"Zero-Shot\"][\"Accuracy\"].mean()\n",
    "    baseline_f1 = results_df[results_df[\"Strategy\"] == \"Zero-Shot\"][\"Macro-F1\"].mean()\n",
    "\n",
    "    improvements = []\n",
    "    for strategy in results_df[\"Strategy\"].unique():\n",
    "        strategy_data = results_df[results_df[\"Strategy\"] == strategy]\n",
    "        strategy_acc = strategy_data[\"Accuracy\"].mean()\n",
    "        strategy_f1 = strategy_data[\"Macro-F1\"].mean()\n",
    "        strategy_neg = strategy_data[\"Negative-F1\"].mean()\n",
    "\n",
    "        acc_improvement = (\n",
    "            ((strategy_acc - baseline_acc) / baseline_acc * 100)\n",
    "            if baseline_acc > 0\n",
    "            else 0\n",
    "        )\n",
    "        f1_improvement = (\n",
    "            ((strategy_f1 - baseline_f1) / baseline_f1 * 100) if baseline_f1 > 0 else 0\n",
    "        )\n",
    "\n",
    "        improvements.append(\n",
    "            {\n",
    "                \"Strategy\": strategy,\n",
    "                \"Mean_Accuracy\": strategy_acc,\n",
    "                \"Mean_Macro-F1\": strategy_f1,\n",
    "                \"Mean_Negative-F1\": strategy_neg,\n",
    "                \"Accuracy_Improvement_%\": acc_improvement,\n",
    "                \"F1_Improvement_%\": f1_improvement,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    improvement_df = pd.DataFrame(improvements)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"IMPROVEMENT OVER ZERO-SHOT BASELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    display(improvement_df.round(2))\n",
    "\n",
    "    # Visualize improvement trend\n",
    "    strategy_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "    improvement_df[\"Strategy\"] = pd.Categorical(\n",
    "        improvement_df[\"Strategy\"], categories=strategy_order, ordered=True\n",
    "    )\n",
    "    improvement_df = improvement_df.sort_values(\"Strategy\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    x = np.arange(len(improvement_df))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(\n",
    "        x - width / 2,\n",
    "        improvement_df[\"Accuracy_Improvement_%\"],\n",
    "        width,\n",
    "        label=\"Accuracy Improvement\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    bars2 = ax.bar(\n",
    "        x + width / 2,\n",
    "        improvement_df[\"F1_Improvement_%\"],\n",
    "        width,\n",
    "        label=\"Macro-F1 Improvement\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Prompting Strategy Complexity ‚Üí\", fontsize=13, weight=\"bold\")\n",
    "    ax.set_ylabel(\"% Improvement over Zero-Shot\", fontsize=13, weight=\"bold\")\n",
    "    ax.set_title(\"Performance Improvement Trend\", fontsize=15, weight=\"bold\", pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(improvement_df[\"Strategy\"], rotation=30, ha=\"right\")\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.8)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"improvement_trend.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  Skipping improvement analysis - insufficient data or no Zero-Shot baseline\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a5ab8",
   "metadata": {},
   "source": [
    "## 8. Radar Chart: Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    from math import pi\n",
    "\n",
    "    # Create radar chart for strategy comparison\n",
    "    categories = [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\", \"Negative-F1\"]\n",
    "    N = len(categories)\n",
    "\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection=\"polar\"))\n",
    "\n",
    "    strategy_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "    colors = [\"#3498DB\", \"#2ECC71\", \"#F39C12\", \"#E74C3C\"]\n",
    "\n",
    "    for i, strategy in enumerate(strategy_order):\n",
    "        if strategy in results_df[\"Strategy\"].values:\n",
    "            strategy_data = (\n",
    "                results_df[results_df[\"Strategy\"] == strategy][categories]\n",
    "                .mean()\n",
    "                .values.tolist()\n",
    "            )\n",
    "            strategy_data += strategy_data[:1]\n",
    "            ax.plot(\n",
    "                angles,\n",
    "                strategy_data,\n",
    "                \"o-\",\n",
    "                linewidth=2,\n",
    "                label=strategy,\n",
    "                color=colors[i],\n",
    "            )\n",
    "            ax.fill(angles, strategy_data, alpha=0.15, color=colors[i])\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, size=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\n",
    "        \"Multi-Metric Comparison of Prompting Strategies\",\n",
    "        size=15,\n",
    "        weight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"radar_chart_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping radar chart - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d98dd",
   "metadata": {},
   "source": [
    "## 9. Cost-Performance Analysis\n",
    "\n",
    "Estimate computational costs based on token usage and compare with performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    # Relative cost estimates (based on token usage and API costs)\n",
    "    cost_mapping = {\n",
    "        \"Zero-Shot\": 1.0,  # Baseline\n",
    "        \"Few-Shot\": 2.5,  # ~2.5x more tokens for examples\n",
    "        \"Chain-of-Thought\": 3.0,  # ~3x for reasoning\n",
    "        \"Tree-of-Thought\": 4.5,  # ~4.5x for multi-path exploration\n",
    "    }\n",
    "\n",
    "    # Model-specific cost multipliers (Groq API pricing)\n",
    "    model_cost_multiplier = {\n",
    "        \"Mixtral-8x7B\": 1.0,  # $0.24/1M tokens (baseline)\n",
    "        \"Llama-3.1-70B\": 2.5,  # $0.59/1M tokens (~2.5x)\n",
    "        \"FinBERT\": 0.0,  # Free (local)\n",
    "    }\n",
    "\n",
    "    results_df[\"Strategy_Cost\"] = results_df[\"Strategy\"].map(cost_mapping)\n",
    "    results_df[\"Model_Cost\"] = results_df[\"Model\"].map(model_cost_multiplier)\n",
    "    results_df[\"Relative_Total_Cost\"] = (\n",
    "        results_df[\"Strategy_Cost\"] * results_df[\"Model_Cost\"]\n",
    "    )\n",
    "    results_df[\"Cost_Efficiency\"] = results_df.apply(\n",
    "        lambda row: (\n",
    "            row[\"Macro-F1\"] / row[\"Relative_Total_Cost\"]\n",
    "            if row[\"Relative_Total_Cost\"] > 0\n",
    "            else row[\"Macro-F1\"] * 1000\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Plot cost vs performance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Scatter plot: Cost vs Performance\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        axes[0].scatter(\n",
    "            model_data[\"Relative_Total_Cost\"],\n",
    "            model_data[\"Macro-F1\"],\n",
    "            s=200,\n",
    "            alpha=0.6,\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "        # Add experiment labels\n",
    "        for _, row in model_data.iterrows():\n",
    "            axes[0].annotate(\n",
    "                row[\"Experiment\"],\n",
    "                (row[\"Relative_Total_Cost\"], row[\"Macro-F1\"]),\n",
    "                fontsize=9,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    axes[0].set_xlabel(\"Relative Computational Cost\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Macro-F1 Score\", fontsize=13, weight=\"bold\")\n",
    "    axes[0].set_title(\n",
    "        \"Cost-Performance Trade-off Analysis\", fontsize=15, weight=\"bold\", pad=20\n",
    "    )\n",
    "    axes[0].legend(title=\"Model\", fontsize=11)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "\n",
    "    # Cost efficiency ranking\n",
    "    top_efficient = results_df.nlargest(min(8, len(results_df)), \"Cost_Efficiency\")\n",
    "\n",
    "    bars = axes[1].barh(\n",
    "        range(len(top_efficient)), top_efficient[\"Cost_Efficiency\"], alpha=0.8\n",
    "    )\n",
    "    axes[1].set_yticks(range(len(top_efficient)))\n",
    "    axes[1].set_yticklabels(\n",
    "        [\n",
    "            f\"{row['Experiment']} ({row['Model'][:10]}+{row['Strategy'][:5]})\"\n",
    "            for _, row in top_efficient.iterrows()\n",
    "        ],\n",
    "        fontsize=10,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Cost Efficiency (F1 / Cost)\", fontsize=13, weight=\"bold\")\n",
    "    axes[1].set_title(\n",
    "        \"Most Cost-Efficient Experiments\", fontsize=15, weight=\"bold\", pad=20\n",
    "    )\n",
    "    axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    # Color bars by model\n",
    "    model_colors = {\n",
    "        \"Mixtral-8x7B\": \"#FF6B6B\",\n",
    "        \"Llama-3.1-70B\": \"#4ECDC4\",\n",
    "        \"FinBERT\": \"#45B7D1\",\n",
    "    }\n",
    "    for i, (_, row) in enumerate(top_efficient.iterrows()):\n",
    "        bars[i].set_color(model_colors.get(row[\"Model\"], \"#95A5A6\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cost_performance_tradeoff.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Cost efficiency ranking table\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COST EFFICIENCY RANKING (F1 / Relative Cost)\")\n",
    "    print(\"=\" * 100)\n",
    "    cost_ranked = results_df[\n",
    "        [\n",
    "            \"Experiment\",\n",
    "            \"Model\",\n",
    "            \"Strategy\",\n",
    "            \"Macro-F1\",\n",
    "            \"Relative_Total_Cost\",\n",
    "            \"Cost_Efficiency\",\n",
    "        ]\n",
    "    ].sort_values(\"Cost_Efficiency\", ascending=False)\n",
    "    display(cost_ranked.round(4))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping cost analysis - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b19da",
   "metadata": {},
   "source": [
    "## 10. Key Findings & Recommendations\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "This section will be populated with insights once experiments are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"KEY FINDINGS & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # 1. Best model across all strategies\n",
    "    best_model = results_df.groupby(\"Model\")[\"Macro-F1\"].mean().idxmax()\n",
    "    best_model_f1 = results_df.groupby(\"Model\")[\"Macro-F1\"].mean().max()\n",
    "\n",
    "    print(f\"\\n1Ô∏è‚É£  BEST OVERALL MODEL: {best_model}\")\n",
    "    print(f\"   Average Macro-F1 across all strategies: {best_model_f1:.4f}\")\n",
    "\n",
    "    # 2. Best strategy across all models\n",
    "    best_strategy = results_df.groupby(\"Strategy\")[\"Macro-F1\"].mean().idxmax()\n",
    "    best_strategy_f1 = results_df.groupby(\"Strategy\")[\"Macro-F1\"].mean().max()\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  BEST PROMPTING STRATEGY: {best_strategy}\")\n",
    "    print(f\"   Average Macro-F1 across all models: {best_strategy_f1:.4f}\")\n",
    "\n",
    "    # 3. Best combination\n",
    "    best_combo = results_df.loc[results_df[\"Macro-F1\"].idxmax()]\n",
    "\n",
    "    print(f\"\\n3Ô∏è‚É£  BEST OVERALL COMBINATION: {best_combo['Experiment']}\")\n",
    "    print(f\"   Model: {best_combo['Model']}\")\n",
    "    print(f\"   Strategy: {best_combo['Strategy']}\")\n",
    "    print(f\"   Macro-F1: {best_combo['Macro-F1']:.4f}\")\n",
    "    print(f\"   Accuracy: {best_combo['Accuracy']:.4f}\")\n",
    "    print(f\"   Negative-F1: {best_combo['Negative-F1']:.4f}\")\n",
    "\n",
    "    # 4. Most cost-efficient\n",
    "    most_efficient = results_df.loc[results_df[\"Cost_Efficiency\"].idxmax()]\n",
    "\n",
    "    print(f\"\\n4Ô∏è‚É£  MOST COST-EFFICIENT: {most_efficient['Experiment']}\")\n",
    "    print(f\"   {most_efficient['Model']} + {most_efficient['Strategy']}\")\n",
    "    print(f\"   Macro-F1: {most_efficient['Macro-F1']:.4f}\")\n",
    "    print(f\"   Relative Cost: {most_efficient['Relative_Total_Cost']:.2f}x baseline\")\n",
    "\n",
    "    # 5. Negative class detection champion\n",
    "    best_negative = results_df.loc[results_df[\"Negative-F1\"].idxmax()]\n",
    "\n",
    "    print(f\"\\n5Ô∏è‚É£  BEST NEGATIVE CLASS DETECTION: {best_negative['Experiment']}\")\n",
    "    print(f\"   {best_negative['Model']} + {best_negative['Strategy']}\")\n",
    "    print(\n",
    "        f\"   Negative-F1: {best_negative['Negative-F1']:.4f} (Critical for financial risk)\"\n",
    "    )\n",
    "\n",
    "    # 6. Production recommendations\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Production criteria\n",
    "    production_candidates = results_df[\n",
    "        (results_df[\"Macro-F1\"] >= 0.70)\n",
    "        & (results_df[\"Negative-F1\"] >= 0.50)\n",
    "        & (results_df[\"Error_Rate\"] <= 0.05)\n",
    "    ].sort_values(\"Macro-F1\", ascending=False)\n",
    "\n",
    "    if len(production_candidates) > 0:\n",
    "        print(\"\\n‚úÖ PRODUCTION-READY CONFIGURATIONS:\")\n",
    "        print(\"   (Macro-F1 ‚â• 0.70, Negative-F1 ‚â• 0.50, Error Rate ‚â§ 5%)\\n\")\n",
    "        for idx, row in production_candidates.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['Experiment']}: {row['Model']} + {row['Strategy']}\")\n",
    "            print(\n",
    "                f\"     Macro-F1: {row['Macro-F1']:.4f}, Negative-F1: {row['Negative-F1']:.4f}, Cost: {row['Relative_Total_Cost']:.2f}x\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No configurations meet all production criteria\")\n",
    "        print(\"   Consider relaxing thresholds or improving prompts\")\n",
    "\n",
    "    print(\"\\nüìä SCENARIO-BASED RECOMMENDATIONS:\")\n",
    "    print(f\"\\n   üí∞ Budget Unlimited (Best Performance):\")\n",
    "    print(\n",
    "        f\"      ‚Üí Use {best_combo['Experiment']}: {best_combo['Model']} + {best_combo['Strategy']}\"\n",
    "    )\n",
    "    print(f\"      ‚Üí Macro-F1: {best_combo['Macro-F1']:.4f}\")\n",
    "\n",
    "    print(f\"\\n   üí∏ Cost-Conscious (Best Value):\")\n",
    "    print(\n",
    "        f\"      ‚Üí Use {most_efficient['Experiment']}: {most_efficient['Model']} + {most_efficient['Strategy']}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"      ‚Üí Macro-F1: {most_efficient['Macro-F1']:.4f} at {most_efficient['Relative_Total_Cost']:.2f}x cost\"\n",
    "    )\n",
    "\n",
    "    finbert_exps = results_df[results_df[\"Model\"] == \"FinBERT\"].sort_values(\n",
    "        \"Macro-F1\", ascending=False\n",
    "    )\n",
    "    if len(finbert_exps) > 0:\n",
    "        best_finbert = finbert_exps.iloc[0]\n",
    "        print(f\"\\n   üÜì Free/Local Deployment:\")\n",
    "        print(\n",
    "            f\"      ‚Üí Use {best_finbert['Experiment']}: FinBERT + {best_finbert['Strategy']}\"\n",
    "        )\n",
    "        print(f\"      ‚Üí Macro-F1: {best_finbert['Macro-F1']:.4f} (No API costs)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for generating recommendations\")\n",
    "    print(\"Please run experiment notebooks first to generate results files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430708d",
   "metadata": {},
   "source": [
    "## 11. Export Complete Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üíæ EXPORTING RESULTS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\nTimestamp: {timestamp}\\n\")\n",
    "\n",
    "    csv_files = []\n",
    "    viz_files = []\n",
    "\n",
    "    # Save complete results\n",
    "    print(\"Saving CSV files...\")\n",
    "    results_file = f\"complete_results_comparison_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    csv_files.append(\n",
    "        (results_file, len(results_df), \"Complete results with all metrics\")\n",
    "    )\n",
    "    print(f\"  ‚úì {results_file} ({len(results_df)} experiments)\")\n",
    "\n",
    "    # Save summaries if they exist\n",
    "    if \"strategy_summary\" in locals() and not strategy_summary.empty:\n",
    "        strategy_file = f\"strategy_summary_{timestamp}.csv\"\n",
    "        strategy_summary.to_csv(strategy_file)\n",
    "        csv_files.append(\n",
    "            (strategy_file, len(strategy_summary), \"Strategy-wise averages\")\n",
    "        )\n",
    "        print(f\"  ‚úì {strategy_file} ({len(strategy_summary)} strategies)\")\n",
    "\n",
    "    if \"model_summary\" in locals() and not model_summary.empty:\n",
    "        model_file = f\"model_summary_{timestamp}.csv\"\n",
    "        model_summary.to_csv(model_file)\n",
    "        csv_files.append((model_file, len(model_summary), \"Model-wise averages\"))\n",
    "        print(f\"  ‚úì {model_file} ({len(model_summary)} models)\")\n",
    "\n",
    "    if \"improvement_df\" in locals() and not improvement_df.empty:\n",
    "        improvement_file = f\"improvement_analysis_{timestamp}.csv\"\n",
    "        improvement_df.to_csv(improvement_file, index=False)\n",
    "        csv_files.append(\n",
    "            (improvement_file, len(improvement_df), \"Improvement over baseline\")\n",
    "        )\n",
    "        print(f\"  ‚úì {improvement_file} ({len(improvement_df)} strategies)\")\n",
    "\n",
    "    # List visualizations\n",
    "    print(\"\\nChecking visualizations...\")\n",
    "    potential_viz = [\n",
    "        \"strategy_comparison.png\",\n",
    "        \"model_comparison.png\",\n",
    "        \"performance_heatmaps.png\",\n",
    "        \"improvement_trend.png\",\n",
    "        \"radar_chart_comparison.png\",\n",
    "        \"cost_performance_tradeoff.png\",\n",
    "    ]\n",
    "\n",
    "    for viz in potential_viz:\n",
    "        if os.path.exists(viz):\n",
    "            size_kb = os.path.getsize(viz) / 1024\n",
    "            viz_files.append((viz, size_kb))\n",
    "            print(f\"  ‚úì {viz} ({size_kb:.1f} KB)\")\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"‚úÖ EXPORT COMPLETE\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    print(f\"\\nüìÑ CSV Files ({len(csv_files)}):\")\n",
    "    for filename, count, desc in csv_files:\n",
    "        print(f\"   ‚Ä¢ {filename}\")\n",
    "        print(f\"     ‚îî‚îÄ {desc}\")\n",
    "\n",
    "    if viz_files:\n",
    "        total_size = sum(size for _, size in viz_files)\n",
    "        print(f\"\\nüìä Visualizations ({len(viz_files)}):\")\n",
    "        for filename, size in viz_files:\n",
    "            print(f\"   ‚Ä¢ {filename} ({size:.1f} KB)\")\n",
    "        print(f\"\\n   Total size: {total_size:.1f} KB ({total_size / 1024:.2f} MB)\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nüéâ Analysis complete! {len(csv_files)} CSV files and {len(viz_files)} visualizations ready.\"\n",
    "    )\n",
    "    print(f\"üìÅ Location: {os.getcwd()}/Results/\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"‚ùå EXPORT FAILED\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\n‚ö†Ô∏è  No results to export - no experiment data loaded\\n\")\n",
    "    print(\"üìã Next steps:\")\n",
    "    print(\"   1. Navigate to experiment folders (Zero_Shot, Few_Shot, etc.)\")\n",
    "    print(\"   2. Run notebooks E1-E12 to generate CSV results\")\n",
    "    print(\"   3. Return to this notebook to perform comparative analysis\")\n",
    "    print(\n",
    "        \"\\nüí° TIP: You can run experiments incrementally - this notebook will analyze whatever is available.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
