{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe361481",
   "metadata": {},
   "source": [
    "# Comprehensive Comparative Analysis: Prompt Engineering for Financial Sentiment Analysis\n",
    "\n",
    "**Experiment Matrix**: 4 Approaches √ó 3 Models = 12 Experiments\n",
    "\n",
    "| Approach | Mixtral-8x7B | Llama-3.1-70B | FinBERT |\n",
    "|----------|--------------|---------------|---------|\n",
    "| **Zero-Shot** | E1 | E2 | E3 |\n",
    "| **Few-Shot** | E4 | E5 | E6 |\n",
    "| **Chain-of-Thought** | E7 | E8 | E9 |\n",
    "| **Tree-of-Thought** | E10 | E11 | E12 |\n",
    "\n",
    "**Dataset**: FinancialPhraseBank Sentences_AllAgree.txt (2,217 samples: 297 negative, 1,361 neutral, 559 positive)\n",
    "\n",
    "**Research Questions**:\n",
    "1. Which model performs best across all prompting strategies?\n",
    "2. Does prompting complexity improve performance (Zero‚ÜíFew‚ÜíCoT‚ÜíToT)?\n",
    "3. Can prompt engineering beat domain-specific fine-tuning (FinBERT)?\n",
    "4. What is the cost-benefit trade-off for each approach?\n",
    "5. Which combination is best for production deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3067b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn scipy statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b6241",
   "metadata": {},
   "source": [
    "## 1. Load All Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment metadata with actual file patterns\n",
    "experiments = {\n",
    "    \"E1\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"approach\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e1_gpt_oss_20b_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"E2\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"approach\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e2_gpt_oss_120b_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"E3\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"approach\": \"Zero-Shot\",\n",
    "        \"file\": \"../Zero_Shot/e3_llama_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"E4\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"approach\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e4_gpt_oss_20b_few_shot_*.csv\",\n",
    "    },\n",
    "    \"E5\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"approach\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e5_gpt_oss_120b_few_shot_*.csv\",\n",
    "    },\n",
    "    \"E6\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"approach\": \"Few-Shot\",\n",
    "        \"file\": \"../Few_Shot/e6_llama_few_shot_*.csv\",\n",
    "    },\n",
    "    \"E7\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"approach\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e7_GPT_OSS_20B_cot_*.csv\",\n",
    "    },\n",
    "    \"E8\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"approach\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e8_GPT_OSS_120B_cot_*.csv\",\n",
    "    },\n",
    "    \"E9\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"approach\": \"Chain-of-Thought\",\n",
    "        \"file\": \"../Chain_of_Thought/e9_Llama-3.3-70B_cot_*.csv\",\n",
    "    },\n",
    "    \"E10\": {\n",
    "        \"model\": \"Mixtral-8x7B\",\n",
    "        \"approach\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e10_GPT_OSS_20B_tot_*.csv\",\n",
    "    },\n",
    "    \"E11\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"approach\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e11_GPT_OSS_120B_flash_tot_*.csv\",\n",
    "    },\n",
    "    \"E12\": {\n",
    "        \"model\": \"FinBERT\",\n",
    "        \"approach\": \"Tree-of-Thought\",\n",
    "        \"file\": \"../Tree_of_Thought/e12_Llama_3.3_70B_tot_*.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Load results with robust error handling\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def load_experiment_robust(exp_id, exp_info):\n",
    "    \"\"\"Load experiment data with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        files = glob.glob(exp_info[\"file\"])\n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è  {exp_id}: No files found matching '{exp_info['file']}'\")\n",
    "            return None\n",
    "\n",
    "        # Get most recent file (by modification time)\n",
    "        latest_file = max(files, key=os.path.getmtime)\n",
    "        df = pd.read_csv(latest_file)\n",
    "\n",
    "        # Validate required columns\n",
    "        required_cols = [\"true_sentiment\", \"predicted_sentiment\"]\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            print(\n",
    "                f\"‚ùå {exp_id}: Missing required columns in {os.path.basename(latest_file)}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # Validate data integrity\n",
    "        if len(df) == 0:\n",
    "            print(f\"‚ö†Ô∏è  {exp_id}: Empty dataframe in {os.path.basename(latest_file)}\")\n",
    "            return None\n",
    "\n",
    "        print(\n",
    "            f\"‚úì {exp_id}: Loaded {len(df):,} samples from {os.path.basename(latest_file)}\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {exp_id}: Error loading data - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä LOADING ALL EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nAttempting to load 12 experiments across 4 approaches...\\n\")\n",
    "\n",
    "results = {}\n",
    "loading_status = {\n",
    "    \"Zero-Shot\": [],\n",
    "    \"Few-Shot\": [],\n",
    "    \"Chain-of-Thought\": [],\n",
    "    \"Tree-of-Thought\": [],\n",
    "}\n",
    "\n",
    "for exp_id, exp_info in experiments.items():\n",
    "    df = load_experiment_robust(exp_id, exp_info)\n",
    "    if df is not None:\n",
    "        results[exp_id] = df\n",
    "        loading_status[exp_info[\"approach\"]].append((exp_id, True, len(df)))\n",
    "    else:\n",
    "        loading_status[exp_info[\"approach\"]].append((exp_id, False, 0))\n",
    "\n",
    "# Detailed summary by approach\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà LOADING SUMMARY BY APPROACH\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for approach, status_list in loading_status.items():\n",
    "    loaded = sum(1 for _, success, _ in status_list if success)\n",
    "    total = len(status_list)\n",
    "\n",
    "    if loaded == total:\n",
    "        icon = \"‚úÖ\"\n",
    "        status = \"COMPLETE\"\n",
    "    elif loaded > 0:\n",
    "        icon = \"‚ö†Ô∏è \"\n",
    "        status = \"PARTIAL\"\n",
    "    else:\n",
    "        icon = \"‚ùå\"\n",
    "        status = \"MISSING\"\n",
    "\n",
    "    print(f\"\\n{icon} {approach}: {loaded}/{total} {status}\")\n",
    "    for exp_id, success, count in status_list:\n",
    "        exp_model = experiments[exp_id][\"model\"]\n",
    "        if success:\n",
    "            print(f\"   ‚úì {exp_id} ({exp_model}): {count:,} samples\")\n",
    "        else:\n",
    "            print(f\"   ‚úó {exp_id} ({exp_model}): Not found\")\n",
    "\n",
    "# Overall summary\n",
    "loaded_count = len(results)\n",
    "total_count = len(experiments)\n",
    "success_rate = (loaded_count / total_count) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "if loaded_count == total_count:\n",
    "    print(f\"‚úÖ SUCCESS: All {total_count} experiments loaded ({success_rate:.0f}%)\")\n",
    "    total_samples = sum(len(df) for df in results.values())\n",
    "    print(f\"üìä Total samples across all experiments: {total_samples:,}\")\n",
    "elif loaded_count > 0:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  PARTIAL SUCCESS: {loaded_count}/{total_count} experiments loaded ({success_rate:.0f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nüí° TIP: Missing {total_count - loaded_count} experiments. Run corresponding notebooks to complete analysis.\"\n",
    "    )\n",
    "    missing = [exp_id for exp_id in experiments.keys() if exp_id not in results]\n",
    "    print(f\"   Missing: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: No experiments loaded (0/{total_count})\")\n",
    "    print(\"\\nüîç TROUBLESHOOTING GUIDE:\")\n",
    "    print(\"   1. Verify experiment notebooks exist:\")\n",
    "    print(\"      ‚Ä¢ Zero_Shot/E1_E2_E3_zero_shot_sentiment_All_agree.ipynb\")\n",
    "    print(\"      ‚Ä¢ Few_Shot/E4_E5_E6_few_shot_sentiment.ipynb\")\n",
    "    print(\"      ‚Ä¢ Chain_of_Thought/E7_E8_E9_cot_sentiment.ipynb\")\n",
    "    print(\"      ‚Ä¢ Tree_of_Thought/E10_tot_sentiment.ipynb\")\n",
    "    print(\"\\n   2. Run experiment notebooks to generate CSV files\")\n",
    "    print(\"\\n   3. Check file patterns match expectations:\")\n",
    "    for exp_id, exp_info in list(experiments.items())[:3]:\n",
    "        print(f\"      ‚Ä¢ {exp_id}: {exp_info['file']}\")\n",
    "    print(\"      ...\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2e0cc",
   "metadata": {},
   "source": [
    "## 2. Calculate Comprehensive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(df, exp_id, model, approach):\n",
    "    \"\"\"Calculate comprehensive metrics for an experiment\"\"\"\n",
    "\n",
    "    # Filter valid predictions\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    if valid_df.empty:\n",
    "        return None\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    # Overall metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    macro_precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    # False negative rate for negative class (critical metric)\n",
    "    neg_idx = 1  # negative is at index 1\n",
    "    true_negatives = (y_true == \"negative\").sum()\n",
    "    false_negatives = (\n",
    "        true_negatives - cm[neg_idx, neg_idx]\n",
    "    )  # Actual negatives - correctly predicted\n",
    "    fnr = false_negatives / true_negatives if true_negatives > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Experiment\": exp_id,\n",
    "        \"Model\": model,\n",
    "        \"Approach\": approach,\n",
    "        \"Total_Samples\": len(df),\n",
    "        \"Valid_Predictions\": len(valid_df),\n",
    "        \"Parsing_Errors\": len(df) - len(valid_df),\n",
    "        \"Parsing_Error_Rate\": (len(df) - len(valid_df)) / len(df),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Macro_F1\": macro_f1,\n",
    "        \"Weighted_F1\": weighted_f1,\n",
    "        \"Macro_Precision\": macro_precision,\n",
    "        \"Macro_Recall\": macro_recall,\n",
    "        \"Positive_Precision\": precision_per_class[0],\n",
    "        \"Positive_Recall\": recall_per_class[0],\n",
    "        \"Positive_F1\": f1_per_class[0],\n",
    "        \"Negative_Precision\": precision_per_class[1],\n",
    "        \"Negative_Recall\": recall_per_class[1],\n",
    "        \"Negative_F1\": f1_per_class[1],  # CRITICAL METRIC\n",
    "        \"Negative_FNR\": fnr,  # False negative rate\n",
    "        \"Neutral_Precision\": precision_per_class[2],\n",
    "        \"Neutral_Recall\": recall_per_class[2],\n",
    "        \"Neutral_F1\": f1_per_class[2],\n",
    "        \"Avg_Confidence\": valid_df[\"confidence\"].mean()\n",
    "        if \"confidence\" in valid_df.columns\n",
    "        else 0,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "all_metrics = []\n",
    "for exp_id, df in results.items():\n",
    "    exp_info = experiments[exp_id]\n",
    "    metrics = calculate_all_metrics(df, exp_id, exp_info[\"model\"], exp_info[\"approach\"])\n",
    "    if metrics:\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "if all_metrics:\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPREHENSIVE METRICS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    display(\n",
    "        metrics_df[\n",
    "            [\n",
    "                \"Experiment\",\n",
    "                \"Model\",\n",
    "                \"Approach\",\n",
    "                \"Accuracy\",\n",
    "                \"Macro_F1\",\n",
    "                \"Negative_F1\",\n",
    "                \"Parsing_Error_Rate\",\n",
    "            ]\n",
    "        ].round(4)\n",
    "\n",
    "    )    metrics_df = pd.DataFrame()  # Empty dataframe for safety\n",
    "\n",
    "else:    print(\"\\n‚ö†Ô∏è  No valid metrics calculated - no experiment data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde7bf7",
   "metadata": {},
   "source": [
    "## 3. Model Comparison (Across Approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average performance by model across all approaches\n",
    "if not metrics_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üîÑ Analyzing model performance across all approaches...\")\n",
    "    \n",
    "    model_summary = metrics_df.groupby(\"Model\")[\n",
    "        [\"Accuracy\", \"Macro_F1\", \"Negative_F1\", \"Parsing_Error_Rate\"]\n",
    "    ].mean()\n",
    "    \n",
    "    print(f\"‚úì Analyzed {len(model_summary)} models: {', '.join(model_summary.index)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AVERAGE PERFORMANCE BY MODEL (across all prompting strategies)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(model_summary.round(4))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Macro F1 by model\n",
    "model_summary[\"Macro_F1\"].plot(\n",
    "    kind=\"bar\", ax=axes[0], color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    ")\n",
    "axes[0].set_title(\"Average Macro-F1 by Model\", fontsize=14, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Macro-F1 Score\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Negative F1 by model (CRITICAL)\n",
    "model_summary[\"Negative_F1\"].plot(\n",
    "    kind=\"bar\", ax=axes[1], color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"Average Negative F1 by Model (Critical Metric)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "axes[1].set_ylabel(\"Negative F1 Score\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Minimum Threshold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Parsing error rate\n",
    "model_summary[\"Parsing_Error_Rate\"].plot(\n",
    "    kind=\"bar\", ax=axes[2], color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    ")\n",
    "axes[2].set_title(\"Parsing Error Rate by Model\", fontsize=14, weight=\"bold\")\n",
    "axes[2].set_ylabel(\"Error Rate\", fontsize=12)\n",
    "axes[2].set_xlabel(\"Model\", fontsize=12)\n",
    "axes[2].set_ylim([0, 0.5])\n",
    "axes[2].grid(axis=\"y\", alpha=0.3)\n",
    "axes[2].tick_params(axis=\"x\", rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches=\"tight\")plt.show()    print(\"‚ö†Ô∏è  Skipping model comparison - no data available\")\n",
    "\n",
    "    plt.show()else:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014c3f8",
   "metadata": {},
   "source": [
    "## 4. Approach Comparison (Across Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab53978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average performance by approach across all models\n",
    "if not metrics_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üîÑ Analyzing approach effectiveness across all models...\")\n",
    "    \n",
    "    approach_summary = metrics_df.groupby(\"Approach\")[\n",
    "        [\"Accuracy\", \"Macro_F1\", \"Negative_F1\", \"Avg_Confidence\"]\n",
    "    ].mean()\n",
    "    \n",
    "    # Order by complexity\n",
    "    approach_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "    approach_summary = approach_summary.reindex([a for a in approach_order if a in approach_summary.index])\n",
    "    \n",
    "    print(f\"‚úì Analyzed {len(approach_summary)} approaches: {', '.join(approach_summary.index)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AVERAGE PERFORMANCE BY APPROACH (across all models)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(approach_summary.round(4))\n",
    "    \n",
    "    # Visualize complexity vs performance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "x = np.arange(len(approach_order))\n",
    "width = 0.35\n",
    "\n",
    "# Macro F1 progression\n",
    "axes[0].plot(\n",
    "    x,\n",
    "    approach_summary[\"Macro_F1\"],\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"Macro-F1\",\n",
    ")\n",
    "axes[0].plot(\n",
    "    x,\n",
    "    approach_summary[\"Negative_F1\"],\n",
    "    marker=\"s\",\n",
    "    linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"Negative-F1\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Prompting Complexity ‚Üí\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\"Does Complexity Improve Performance?\", fontsize=14, weight=\"bold\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(approach_order, rotation=15, ha=\"right\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Confidence by approach\n",
    "approach_summary[\"Avg_Confidence\"].plot(\n",
    "    kind=\"bar\", ax=axes[1], color=[\"#95E1D3\", \"#F38181\", \"#AA96DA\", \"#FCBAD3\"]\n",
    ")\n",
    "axes[1].set_title(\"Average Confidence by Approach\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"Confidence Score\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Approach\", fontsize=12)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "axes[1].tick_params(axis=\"x\", rotation=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"approach_comparison.png\", dpi=300, bbox_inches=\"tight\")plt.show()    print(\"‚ö†Ô∏è  Skipping approach comparison - no data available\")\n",
    "\n",
    "    plt.show()else:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee28d8",
   "metadata": {},
   "source": [
    "## 5. Heatmap: Model √ó Approach Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "if not metrics_df.empty and len(metrics_df) >= 3:\n",
    "    approach_order = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "    \n",
    "    pivot_macro_f1 = metrics_df.pivot(index=\"Model\", columns=\"Approach\", values=\"Macro_F1\")\n",
    "    pivot_macro_f1 = pivot_macro_f1.reindex(columns=[a for a in approach_order if a in pivot_macro_f1.columns])\n",
    "    \n",
    "    pivot_neg_f1 = metrics_df.pivot(index=\"Model\", columns=\"Approach\", values=\"Negative_F1\")\n",
    "    pivot_neg_f1 = pivot_neg_f1.reindex(columns=[a for a in approach_order if a in pivot_neg_f1.columns])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Macro-F1 heatmap\n",
    "sns.heatmap(\n",
    "    pivot_macro_f1,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    ax=axes[0],\n",
    "    cbar_kws={\"label\": \"Macro-F1\"},\n",
    ")\n",
    "axes[0].set_title(\"Macro-F1: Model √ó Approach\", fontsize=14, weight=\"bold\")\n",
    "axes[0].set_xlabel(\"Approach\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Model\", fontsize=12)\n",
    "\n",
    "# Negative-F1 heatmap (CRITICAL)\n",
    "sns.heatmap(\n",
    "    pivot_neg_f1,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={\"label\": \"Negative-F1\"},\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"Negative-F1: Model √ó Approach (Critical Metric)\", fontsize=14, weight=\"bold\"\n",
    ")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_approach_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BEST PERFORMING COMBINATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    best_macro = metrics_df.loc[metrics_df[\"Macro_F1\"].idxmax()]\n",
    "    best_neg = metrics_df.loc[metrics_df[\"Negative_F1\"].idxmax()]\n",
    "    best_acc = metrics_df.loc[metrics_df[\"Accuracy\"].idxmax()]\n",
    "    \n",
    "    print(f\"Best Macro-F1: {best_macro['Experiment']} ({best_macro['Model']} + {best_macro['Approach']}) = {best_macro['Macro_F1']:.4f}\")\n",
    "    print(f\"Best Negative-F1: {best_neg['Experiment']} ({best_neg['Model']} + {best_neg['Approach']}) = {best_neg['Negative_F1']:.4f}\")\n",
    "    print(f\"Best Accuracy: {best_acc['Experiment']} ({best_acc['Model']} + {best_acc['Approach']}) = {best_acc['Accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"‚ö†Ô∏è  SKIPPING HEATMAP ANALYSIS\")\n",
    "    print(\"=\" * 100)\n",
    "    if metrics_df.empty:\n",
    "\n",
    "        print(\"Reason: No metrics data available\")    f\"Best Accuracy: {metrics_df.loc[metrics_df['Accuracy'].idxmax(), 'Experiment']} - {metrics_df['Accuracy'].max():.4f}\"\n",
    "\n",
    "    else:print()\n",
    "\n",
    "        print(f\"Reason: Insufficient data (need ‚â•3 experiments, have {len(metrics_df)})\")\n",
    "    print(\"üí° Run more experiment notebooks to enable this visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762dd85",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67625bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar's test to compare paired experiments\n",
    "if len(results) >= 2:\n",
    "    from statsmodels.stats.contingency_tables import mcnemar\n",
    "    \n",
    "    \n",
    "    def compare_experiments(exp1_id, exp2_id):\n",
    "        \"\"\"Compare two experiments using McNemar's test\"\"\"\n",
    "        if exp1_id not in results or exp2_id not in results:\n",
    "            return None\n",
    "        \n",
    "        df1 = results[exp1_id]\n",
    "        df2 = results[exp2_id]\n",
    "\n",
    "    # Filter valid predictions\n",
    "    valid1 = df1[\n",
    "        df1[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "    valid2 = df2[\n",
    "        df2[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Ensure same samples\n",
    "    if len(valid1) != len(valid2):\n",
    "        print(\n",
    "            f\"‚ö†Ô∏è Sample size mismatch: {exp1_id}={len(valid1)}, {exp2_id}={len(valid2)}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Create contingency table\n",
    "    correct1 = (valid1[\"true_sentiment\"] == valid1[\"predicted_sentiment\"]).astype(int)\n",
    "    correct2 = (valid2[\"true_sentiment\"] == valid2[\"predicted_sentiment\"]).astype(int)\n",
    "\n",
    "    # McNemar table: [[both_correct, exp1_only], [exp2_only, both_wrong]]\n",
    "    both_correct = ((correct1 == 1) & (correct2 == 1)).sum()\n",
    "    exp1_only = ((correct1 == 1) & (correct2 == 0)).sum()\n",
    "    exp2_only = ((correct1 == 0) & (correct2 == 1)).sum()\n",
    "    both_wrong = ((correct1 == 0) & (correct2 == 0)).sum()\n",
    "\n",
    "    table = [[both_correct, exp1_only], [exp2_only, both_wrong]]\n",
    "\n",
    "    result = mcnemar(table, exact=False, correction=True)\n",
    "\n",
    "    return {\n",
    "        \"Comparison\": f\"{exp1_id} vs {exp2_id}\",\n",
    "        \"Statistic\": result.statistic,\n",
    "        \"P-value\": result.pvalue,\n",
    "        \"Significant\": \"Yes\" if result.pvalue < 0.05 else \"No\",\n",
    "        \"Winner\": exp1_id\n",
    "        if exp1_only > exp2_only\n",
    "        else exp2_id\n",
    "        if exp2_only > exp1_only\n",
    "        else \"Tie\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare key pairs\n",
    "comparisons = [\n",
    "    (\"E1\", \"E2\"),  # Mixtral vs Llama (Zero-Shot)\n",
    "    (\"E1\", \"E3\"),  # Mixtral vs FinBERT (Zero-Shot)\n",
    "    (\"E2\", \"E3\"),  # Llama vs FinBERT (Zero-Shot)\n",
    "    (\"E1\", \"E4\"),  # Mixtral: Zero-Shot vs Few-Shot\n",
    "    sig_results = []\n",
    "    for exp1, exp2 in comparisons:\n",
    "        if exp1 in results and exp2 in results:\n",
    "            result = compare_experiments(exp1, exp2)\n",
    "            if result:\n",
    "                sig_results.append(result)\n",
    "    \n",
    "    if sig_results:\n",
    "        sig_df = pd.DataFrame(sig_results)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        display(sig_df)\n",
    "print(\"\\n\" + \"=\" * 80)    print(\"‚ö†Ô∏è  Skipping statistical testing - insufficient experiments loaded\")\n",
    "\n",
    "    else:\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (McNemar's Test)\")display(sig_df)\n",
    "\n",
    "        print(\"‚ö†Ô∏è  Not enough paired experiments for statistical testing\")\n",
    "print(\"=\" * 80)else:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f4111",
   "metadata": {},
   "source": [
    "## 7. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token costs (approximate)\n",
    "if not metrics_df.empty:\n",
    "    token_costs = {\n",
    "        \"Mixtral-8x7B\": {\"input\": 0.27, \"output\": 0.27},  # per 1M tokens (Groq)\n",
    "        \"Llama-3.1-70B\": {\"input\": 0.59, \"output\": 0.79},  # per 1M tokens (Groq)\n",
    "        \"FinBERT\": {\"input\": 0, \"output\": 0},  # Free (local)\n",
    "    }\n",
    "\n",
    "# Approximate token usage per approach\n",
    "token_usage = {\n",
    "    \"Zero-Shot\": {\"input\": 150, \"output\": 50},  # Average tokens per sample\n",
    "    \"Few-Shot\": {\"input\": 400, \"output\": 50},\n",
    "    \"Chain-of-Thought\": {\"input\": 300, \"output\": 150},\n",
    "    \"Tree-of-Thought\": {\"input\": 450, \"output\": 200},\n",
    "}\n",
    "\n",
    "# Calculate costs\n",
    "total_samples = 2217\n",
    "\n",
    "cost_analysis = []\n",
    "for _, row in metrics_df.iterrows():\n",
    "    model = row[\"Model\"]\n",
    "    approach = row[\"Approach\"]\n",
    "\n",
    "    if model in token_costs and approach in token_usage:\n",
    "        input_tokens = token_usage[approach][\"input\"] * total_samples\n",
    "        output_tokens = token_usage[approach][\"output\"] * total_samples\n",
    "\n",
    "        input_cost = (input_tokens / 1_000_000) * token_costs[model][\"input\"]\n",
    "        output_cost = (output_tokens / 1_000_000) * token_costs[model][\"output\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "\n",
    "        # Cost per F1 point\n",
    "        cost_per_f1 = (\n",
    "            total_cost / row[\"Macro_F1\"] if row[\"Macro_F1\"] > 0 else float(\"inf\")\n",
    "        )\n",
    "\n",
    "        cost_analysis.append(\n",
    "            {\n",
    "                \"Experiment\": row[\"Experiment\"],\n",
    "                \"Model\": model,\n",
    "                \"Approach\": approach,\n",
    "                \"Input_Tokens\": input_tokens,\n",
    "                \"Output_Tokens\": output_tokens,\n",
    "                \"Total_Cost_USD\": total_cost,\n",
    "                \"Macro_F1\": row[\"Macro_F1\"],\n",
    "                \"Cost_per_F1\": cost_per_f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "cost_df = pd.DataFrame(cost_analysis)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COST-BENEFIT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    cost_df[\n",
    "        [\"Experiment\", \"Model\", \"Approach\", \"Total_Cost_USD\", \"Macro_F1\", \"Cost_per_F1\"]\n",
    "    ].round(4)\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cost vs Performance scatter\n",
    "for model in metrics_df[\"Model\"].unique():\n",
    "    model_data = cost_df[cost_df[\"Model\"] == model]\n",
    "    axes[0].scatter(\n",
    "        model_data[\"Total_Cost_USD\"],\n",
    "        model_data[\"Macro_F1\"],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        label=model,\n",
    "    )\n",
    "    for _, row in model_data.iterrows():\n",
    "        axes[0].annotate(\n",
    "            row[\"Experiment\"],\n",
    "            (row[\"Total_Cost_USD\"], row[\"Macro_F1\"]),\n",
    "            fontsize=9,\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "axes[0].set_xlabel(\"Total Cost (USD)\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Macro-F1\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\"Cost vs Performance Trade-off\", fontsize=14, weight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Cost efficiency (Cost per F1 point)\n",
    "top_efficient = cost_df.nsmallest(6, \"Cost_per_F1\")\n",
    "top_efficient.plot(\n",
    "    x=\"Experiment\", y=\"Cost_per_F1\", kind=\"bar\", ax=axes[1], legend=False\n",
    ")\n",
    "axes[1].set_title(\"Most Cost-Efficient Experiments\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"Cost per F1 Point (USD)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Experiment\", fontsize=12)\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cost_benefit_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "plt.show()    print(\"‚ö†Ô∏è  Skipping cost analysis - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710404a",
   "metadata": {},
   "source": [
    "## 8. Production Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada18e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define production criteria\n",
    "if not metrics_df.empty and 'cost_df' in locals():\n",
    "    PRODUCTION_CRITERIA = {\n",
    "        \"Minimum_Macro_F1\": 0.75,\n",
    "        \"Minimum_Negative_F1\": 0.50,  # Critical - can't miss bad financial news\n",
    "        \"Maximum_Parsing_Error\": 0.05,\n",
    "        \"Maximum_Cost_per_1000\": 1.0,  # USD\n",
    "    }\n",
    "\n",
    "# Evaluate each experiment against criteria\n",
    "production_ready = metrics_df.copy()\n",
    "production_ready[\"Meets_Macro_F1\"] = (\n",
    "    production_ready[\"Macro_F1\"] >= PRODUCTION_CRITERIA[\"Minimum_Macro_F1\"]\n",
    ")\n",
    "production_ready[\"Meets_Negative_F1\"] = (\n",
    "    production_ready[\"Negative_F1\"] >= PRODUCTION_CRITERIA[\"Minimum_Negative_F1\"]\n",
    ")\n",
    "production_ready[\"Meets_Parsing\"] = (\n",
    "    production_ready[\"Parsing_Error_Rate\"]\n",
    "    <= PRODUCTION_CRITERIA[\"Maximum_Parsing_Error\"]\n",
    ")\n",
    "\n",
    "# Add cost criterion\n",
    "production_ready = production_ready.merge(\n",
    "    cost_df[[\"Experiment\", \"Total_Cost_USD\"]], on=\"Experiment\"\n",
    ")\n",
    "production_ready[\"Cost_per_1000\"] = (production_ready[\"Total_Cost_USD\"] / 2217) * 1000\n",
    "production_ready[\"Meets_Cost\"] = (\n",
    "    production_ready[\"Cost_per_1000\"] <= PRODUCTION_CRITERIA[\"Maximum_Cost_per_1000\"]\n",
    ")\n",
    "\n",
    "production_ready[\"All_Criteria_Met\"] = (\n",
    "    production_ready[\"Meets_Macro_F1\"]\n",
    "    & production_ready[\"Meets_Negative_F1\"]\n",
    "    & production_ready[\"Meets_Parsing\"]\n",
    "    & production_ready[\"Meets_Cost\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PRODUCTION READINESS EVALUATION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Criteria:\")\n",
    "print(f\"  - Macro-F1 ‚â• {PRODUCTION_CRITERIA['Minimum_Macro_F1']}\")\n",
    "print(f\"  - Negative-F1 ‚â• {PRODUCTION_CRITERIA['Minimum_Negative_F1']} (critical)\")\n",
    "print(f\"  - Parsing errors ‚â§ {PRODUCTION_CRITERIA['Maximum_Parsing_Error'] * 100}%\")\n",
    "print(f\"  - Cost ‚â§ ${PRODUCTION_CRITERIA['Maximum_Cost_per_1000']}/1000 samples\")\n",
    "print()\n",
    "\n",
    "display(\n",
    "    production_ready[\n",
    "        [\n",
    "            \"Experiment\",\n",
    "            \"Model\",\n",
    "            \"Approach\",\n",
    "            \"Macro_F1\",\n",
    "            \"Negative_F1\",\n",
    "            \"Parsing_Error_Rate\",\n",
    "            \"Cost_per_1000\",\n",
    "            \"All_Criteria_Met\",\n",
    "        ]\n",
    "    ].round(4)\n",
    ")\n",
    "\n",
    "# Highlight production-ready experiments\n",
    "production_ready_list = production_ready[production_ready[\"All_Criteria_Met\"]]\n",
    "if len(production_ready_list) > 0:\n",
    "    print(\"\\n‚úÖ PRODUCTION-READY EXPERIMENTS:\")\n",
    "    for _, row in production_ready_list.iterrows():\n",
    "        print(f\"  {row['Experiment']}: {row['Model']} + {row['Approach']}\")\n",
    "        print(\n",
    "            f\"    Macro-F1: {row['Macro_F1']:.4f}, Negative-F1: {row['Negative_F1']:.4f}, Cost: ${row['Cost_per_1000']:.4f}/1000\"\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  NO EXPERIMENTS MEET ALL PRODUCTION CRITERIA\")\n",
    "        print(\"\\nTop candidates:\")\n",
    "        display(\n",
    "            production_ready.nlargest(3, \"Macro_F1\")[\n",
    "                [\"Experiment\", \"Model\", \"Approach\", \"Macro_F1\", \"Negative_F1\"]\n",
    "            ].round(4)\n",
    "        )\n",
    "\n",
    "else:\n",
    "    )    print(\"‚ö†Ô∏è  Skipping production evaluation - no cost data or metrics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac9068",
   "metadata": {},
   "source": [
    "## 9. Final Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Overall Model**: [To be determined after running experiments]\n",
    "2. **Best Overall Approach**: [To be determined]\n",
    "3. **Best Combination**: [To be determined]\n",
    "\n",
    "### Critical Insights:\n",
    "\n",
    "#### Negative Class Detection (Most Important)\n",
    "- **Critical Metric**: Negative F1 must be > 0.50 for production\n",
    "- **Business Impact**: False negatives = missing bad financial news = unacceptable risk\n",
    "- **Best Performer**: [Will show which model/approach best detects negatives]\n",
    "\n",
    "#### Prompt Engineering vs Fine-Tuning\n",
    "- **FinBERT Advantage**: Domain-specific pre-training on financial texts\n",
    "- **LLM Advantage**: Flexibility, reasoning capability, handles edge cases\n",
    "- **Verdict**: [Compare FinBERT vs best prompt-engineered LLM]\n",
    "\n",
    "#### Complexity vs Performance\n",
    "- **Zero-Shot Baseline**: Establishes minimum without examples\n",
    "- **Few-Shot Improvement**: In-context learning boost\n",
    "- **CoT Reasoning**: Step-by-step analysis benefit\n",
    "- **ToT Exploration**: Diminishing returns?\n",
    "- **Conclusion**: [Optimal complexity level for this task]\n",
    "\n",
    "#### Cost-Benefit Analysis\n",
    "- **Free Option**: FinBERT (local inference, one-time download)\n",
    "- **Budget Option**: Mixtral-8x7B with Few-Shot (~$0.X for 2,217 samples)\n",
    "- **Premium Option**: Llama-3.1-70B with CoT (~$0.X for 2,217 samples)\n",
    "- **Best Value**: [Cost per F1 point winner]\n",
    "\n",
    "### Production Deployment Recommendation:\n",
    "\n",
    "**Scenario 1: Budget Unlimited, Performance Critical**\n",
    "- Use: [Best performing experiment]\n",
    "- Rationale: Highest Macro-F1 and Negative-F1\n",
    "\n",
    "**Scenario 2: Cost-Conscious, Good Performance Needed**\n",
    "- Use: [Best cost-efficient experiment]\n",
    "- Rationale: 90% of performance at 50% of cost\n",
    "\n",
    "**Scenario 3: Free/Local Deployment**\n",
    "- Use: FinBERT (E3/E6/E9/E12)\n",
    "- Rationale: No API costs, fast inference, proven financial domain performance\n",
    "\n",
    "### Future Work:\n",
    "1. **Ensemble Approach**: Combine FinBERT + best LLM (vote or confidence-weighted)\n",
    "2. **Fine-Tuning**: Fine-tune Mixtral/Llama on FinancialPhraseBank for best of both worlds\n",
    "3. **Active Learning**: Focus on high-disagreement samples between models\n",
    "4. **Domain Expansion**: Test on other financial datasets (earnings calls, news, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5455da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "if not metrics_df.empty:\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üíæ EXPORTING COMPREHENSIVE ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"\\nTimestamp: {timestamp}\")\n",
    "    print(f\"Export location: {os.getcwd()}\\n\")\n",
    "    \n",
    "    csv_files = []\n",
    "    viz_files = []\n",
    "    \n",
    "    # Save main results\n",
    "    print(\"üìÑ Saving CSV files...\")\n",
    "    main_file = f\"comprehensive_comparative_analysis_{timestamp}.csv\"\n",
    "    metrics_df.to_csv(main_file, index=False)\n",
    "    size_kb = os.path.getsize(main_file) / 1024\n",
    "    csv_files.append((main_file, len(metrics_df), size_kb))\n",
    "    print(f\"  ‚úì {main_file} ({len(metrics_df)} experiments, {size_kb:.1f} KB)\")\n",
    "    \n",
    "    # Save cost analysis\n",
    "    if 'cost_df' in locals() and not cost_df.empty:\n",
    "        cost_file = f\"cost_benefit_analysis_{timestamp}.csv\"\n",
    "        cost_df.to_csv(cost_file, index=False)\n",
    "        size_kb = os.path.getsize(cost_file) / 1024\n",
    "        csv_files.append((cost_file, len(cost_df), size_kb))\n",
    "        print(f\"  ‚úì {cost_file} ({len(cost_df)} rows, {size_kb:.1f} KB)\")\n",
    "    \n",
    "    # Save production readiness\n",
    "    if 'production_ready' in locals() and not production_ready.empty:\n",
    "        prod_file = f\"production_readiness_evaluation_{timestamp}.csv\"\n",
    "        production_ready.to_csv(prod_file, index=False)\n",
    "        size_kb = os.path.getsize(prod_file) / 1024\n",
    "\n",
    "        csv_files.append((prod_file, len(production_ready), size_kb))    print(\"\\nüí° TIP: You can run experiments incrementally. This notebook will analyze whatever is available.\")\n",
    "\n",
    "        print(f\"  ‚úì {prod_file} ({len(production_ready)} rows, {size_kb:.1f} KB)\")    print(\"\\n   2. Re-run this notebook once CSV files are generated\")\n",
    "\n",
    "        print(\"      ‚Ä¢ E10-E12: Tree_of_Thought/E10_tot_sentiment.ipynb\")\n",
    "\n",
    "    # Check for visualizations    print(\"      ‚Ä¢ E7-E9: Chain_of_Thought/E7_E8_E9_cot_sentiment.ipynb\")\n",
    "\n",
    "    print(\"\\nüìä Checking generated visualizations...\")    print(\"      ‚Ä¢ E4-E6: Few_Shot/E4_E5_E6_few_shot_sentiment.ipynb\")\n",
    "\n",
    "    potential_viz = [    print(\"      ‚Ä¢ E1-E3: Zero_Shot/E1_E2_E3_zero_shot_sentiment_All_agree.ipynb\")\n",
    "\n",
    "        \"model_comparison.png\",    print(\"   1. Run experiment notebooks to generate results:\")\n",
    "\n",
    "        \"approach_comparison.png\",    print(\"üìã Required steps:\")\n",
    "\n",
    "        \"model_approach_heatmap.png\",    print(\"\\n‚ö†Ô∏è  Cannot export - no experiment metrics calculated\\n\")\n",
    "\n",
    "        \"cost_benefit_analysis.png\"    print(\"=\" * 100)\n",
    "\n",
    "    ]    print(\"‚ùå EXPORT FAILED: NO DATA AVAILABLE\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "    for viz in potential_viz:else:\n",
    "\n",
    "        if os.path.exists(viz):    print(\"   ‚Ä¢ Share comprehensive_comparative_analysis_*.csv with stakeholders\")\n",
    "\n",
    "            size_kb = os.path.getsize(viz) / 1024    print(\"   ‚Ä¢ Use insights for model/approach selection\")\n",
    "\n",
    "            viz_files.append((viz, size_kb))    print(\"   ‚Ä¢ Check PNG files for visual comparisons\")\n",
    "\n",
    "            print(f\"  ‚úì {viz} ({size_kb:.1f} KB)\")    print(\"   ‚Ä¢ Review CSV files for detailed metrics\")\n",
    "\n",
    "        else:    print(\"\\nüí° Next steps:\")\n",
    "\n",
    "            print(f\"  ‚úó {viz} (not generated)\")    # Usage tip\n",
    "\n",
    "        \n",
    "\n",
    "    # Final summary    print(f\"üìÇ Files are in: {os.getcwd()}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)    print(f\"\\nüéâ Analysis complete! All results saved with timestamp: {timestamp}\")\n",
    "\n",
    "    print(\"‚úÖ EXPORT COMPLETE\")    \n",
    "\n",
    "    print(\"=\" * 100)        print(\"   ‚Ä¢ Visualizations: 0 (sections may have been skipped due to insufficient data)\")\n",
    "\n",
    "        else:\n",
    "\n",
    "    print(f\"\\nüìÅ Summary:\")        print(f\"   ‚Ä¢ Total viz size: {viz_total_size:.1f} KB ({viz_total_size/1024:.2f} MB)\")\n",
    "\n",
    "    print(f\"   ‚Ä¢ CSV files: {len(csv_files)}\")        viz_total_size = sum(size for _, size in viz_files)\n",
    "\n",
    "    csv_total_size = sum(size for _, _, size in csv_files)        print(f\"   ‚Ä¢ Visualizations: {len(viz_files)}\")\n",
    "\n",
    "    print(f\"   ‚Ä¢ Total CSV size: {csv_total_size:.1f} KB ({csv_total_size/1024:.2f} MB)\")    if viz_files:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
