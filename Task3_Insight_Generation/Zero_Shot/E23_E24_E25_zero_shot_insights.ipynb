{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ec460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "# Fix SSL/TLS certificate verification for gRPC (required for Google Gemini API on macOS)\n",
    "os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''\n",
    "os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import ssl\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix SSL certificate verification issue on macOS\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# API setup\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure APIs\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(\"âœ“ SSL certificate verification disabled for macOS compatibility\")\n",
    "print(f\"âœ“ Google API configured: {bool(GOOGLE_API_KEY)}\")\n",
    "print(f\"âœ“ Groq API configured: {bool(GROQ_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b805cd",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fffb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"sentiment\"].value_counts())\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b9176",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Insight Generation Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Generate comprehensive financial insights without examples.\n",
    "\n",
    "**Insight Categories**:\n",
    "1. **Financial Trend**: Direction and magnitude of change\n",
    "2. **Business Impact**: Implications for operations and strategy\n",
    "3. **Stakeholder Perspective**: Impact on investors, employees, customers\n",
    "4. **Risk/Opportunity**: Potential concerns or advantages\n",
    "5. **Recommended Actions**: Suggested next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdbb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_insight_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a zero-shot prompt for financial insight generation.\n",
    "    No examples provided - model relies on pretrained knowledge.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial analyst expert. Generate comprehensive, actionable insights from the following financial statement.\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "Provide detailed analysis covering:\n",
    "1. **Key Trend**: What financial trend or change does this indicate?\n",
    "2. **Business Impact**: What are the implications for the company's operations and strategy?\n",
    "3. **Stakeholder Impact**: How does this affect investors, employees, and customers?\n",
    "4. **Opportunities/Risks**: What opportunities or risks does this present?\n",
    "5. **Recommended Actions**: What should management or investors consider?\n",
    "\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "    \"key_trend\": \"Concise description of the main trend\",\n",
    "    \"business_impact\": \"Impact on company operations and strategy\",\n",
    "    \"stakeholder_impact\": \"Effects on different stakeholder groups\",\n",
    "    \"opportunities\": \"Potential opportunities identified\",\n",
    "    \"risks\": \"Potential risks or concerns\",\n",
    "    \"recommended_actions\": \"Suggested actions for management/investors\",\n",
    "    \"insight_quality_score\": 0.0-1.0\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"ZERO-SHOT INSIGHT GENERATION PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_zero_shot_insight_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d67d5b0",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c957b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\", temperature=0.3):\n",
    "    \"\"\"Call Gemini API with retry logic - higher temperature for creativity\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=800,\n",
    "                ),\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.3):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=800,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_insight_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        # If parsing fails, return None\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ“ Model inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682d19c",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "### E23: Gemini 2.5 Pro (Zero-Shot Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93464ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, use a sample of the dataset\n",
    "test_df = df.head(50).copy()  # Using 50 samples for insight generation\n",
    "\n",
    "# E23: Gemini 2.5 Pro\n",
    "print(\"Running E23: Gemini 2.5 Pro (Zero-Shot Insight Generation)...\")\n",
    "e23_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E23 Progress\"):\n",
    "    prompt = create_zero_shot_insight_prompt(row[\"sentence\"])\n",
    "    response = call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_insight_response(response)\n",
    "        if parsed:\n",
    "            e23_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"key_trend\": parsed.get(\"key_trend\", \"N/A\"),\n",
    "                    \"business_impact\": parsed.get(\"business_impact\", \"N/A\"),\n",
    "                    \"stakeholder_impact\": parsed.get(\"stakeholder_impact\", \"N/A\"),\n",
    "                    \"opportunities\": parsed.get(\"opportunities\", \"N/A\"),\n",
    "                    \"risks\": parsed.get(\"risks\", \"N/A\"),\n",
    "                    \"recommended_actions\": parsed.get(\"recommended_actions\", \"N/A\"),\n",
    "                    \"insight_quality_score\": parsed.get(\"insight_quality_score\", 0.5),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e23_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"key_trend\": \"Parse error\",\n",
    "                    \"business_impact\": \"N/A\",\n",
    "                    \"stakeholder_impact\": \"N/A\",\n",
    "                    \"opportunities\": \"N/A\",\n",
    "                    \"risks\": \"N/A\",\n",
    "                    \"recommended_actions\": \"N/A\",\n",
    "                    \"insight_quality_score\": 0,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(1.0)  # Slower rate for longer responses\n",
    "\n",
    "e23_df = pd.DataFrame(e23_results)\n",
    "print(f\"\\nâœ“ E23 completed: {len(e23_df)} insights generated\")\n",
    "display(e23_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e886d25",
   "metadata": {},
   "source": [
    "## 5. Analyze Insight Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc34b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed insights for review\n",
    "def display_insight_details(df, num_samples=3):\n",
    "    \"\"\"Display formatted insights for qualitative review\"\"\"\n",
    "    samples = df.sample(min(num_samples, len(df)), random_state=42)\n",
    "\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"STATEMENT: {row['sentence']}\")\n",
    "        print(f\"TRUE SENTIMENT: {row['true_sentiment']}\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"\\nðŸ“Š KEY TREND:\\n{row['key_trend']}\")\n",
    "        print(f\"\\nðŸ’¼ BUSINESS IMPACT:\\n{row['business_impact']}\")\n",
    "        print(f\"\\nðŸ‘¥ STAKEHOLDER IMPACT:\\n{row['stakeholder_impact']}\")\n",
    "        print(f\"\\nâœ… OPPORTUNITIES:\\n{row['opportunities']}\")\n",
    "        print(f\"\\nâš ï¸  RISKS:\\n{row['risks']}\")\n",
    "        print(f\"\\nðŸŽ¯ RECOMMENDED ACTIONS:\\n{row['recommended_actions']}\")\n",
    "        print(f\"\\nâ­ QUALITY SCORE: {row['insight_quality_score']:.2f}\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "\n",
    "display_insight_details(e23_df, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31bdd3",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "e23_df.to_csv(f\"e23_gemini_pro_zero_shot_insights_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved: e23_gemini_pro_zero_shot_insights_{timestamp}.csv\")\n",
    "print(f\"\\nAverage Insight Quality Score: {e23_df['insight_quality_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ad426",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run E24 (Gemini Flash) and E25 (Llama) following the same pattern\n",
    "2. Compare insight quality across models\n",
    "3. Evaluate factual accuracy and actionability\n",
    "4. Analyze correlation between sentiment and insight tone\n",
    "5. Move to Few-Shot, Chain-of-Thought, and Tree-of-Thought strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}