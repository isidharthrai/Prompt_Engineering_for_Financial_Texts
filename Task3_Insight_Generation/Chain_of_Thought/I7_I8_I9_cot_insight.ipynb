{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9526b74a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip3 install pandas numpy matplotlib seaborn scikit-learn tqdm requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e486c85b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama API setup and imports\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    matthews_corrcoef,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(\"✓ Ollama will be used for LLM inference (local)\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SETUP COMPLETE - CHAIN-OF-THOUGHT INSIGHT GENERATION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Models configured:\")\n",
        "print(\"  • I7: Llama3.1:8b (Ollama - Local - Chain-of-Thought)\")\n",
        "print(\"  • I8: Qwen3:8b (Ollama - Local - Chain-of-Thought)\")\n",
        "print(\"  • I9: DeepSeek-R1:8b (Ollama - Local - Chain-of-Thought)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (14, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab7a8c8",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68548254",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
        "\n",
        "sentences = []\n",
        "insights = []\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if \"@\" in line:\n",
        "            parts = line.rsplit(\"@\", 1)\n",
        "            if len(parts) == 2:\n",
        "                sentences.append(parts[0])\n",
        "                insights.append(parts[1])\n",
        "\n",
        "df = pd.DataFrame({\"sentence\": sentences, \"true_insight\": insights})\n",
        "\n",
        "print(f\"Dataset loaded: {len(df)} sentences\")\n",
        "print(f\"\\nInsight distribution:\")\n",
        "print(df[\"true_insight\"].value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Sample sentences:\")\n",
        "print(\"=\" * 80)\n",
        "display(df.sample(5, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93ad40e",
      "metadata": {},
      "source": [
        "## 2. Chain-of-Thought Prompt Design\n",
        "\n",
        "**Reasoning Structure**:\n",
        "- Step 1: Identify key financial metrics/events\n",
        "- Step 2: Analyze positive indicators\n",
        "- Step 3: Analyze negative indicators\n",
        "- Step 4: Determine net impact from investor perspective\n",
        "- Step 5: Classify insight with confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd3d1de",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_cot_prompt(sentence):\n",
        "    \"\"\"\n",
        "    Creates a Chain-of-Thought prompt for financial insight classification.\n",
        "    Guides the model through stepwise reasoning.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are a financial insight analysis expert. Use step-by-step reasoning to classify this financial statement.\n",
        "\n",
        "Task: Classify the following financial statement as \"positive\", \"negative\", or \"neutral\" from an investor's perspective, using stepwise reasoning.\n",
        "\n",
        "Guidelines:\n",
        "- Positive: Financial improvements, growth, profits, revenue increases\n",
        "- Negative: Financial declines, losses, revenue drops\n",
        "- Neutral: Factual statements with no clear financial impact\n",
        "\n",
        "Financial Statement:\n",
        "\"{sentence}\"\n",
        "\n",
        "Think step-by-step about the financial signals. Then, provide a final concise summary in the 'rationale' field of the JSON.\n",
        "Return ONLY strictly JSON in this exact format:\n",
        "{{\n",
        "    \"insight\": \"positive/negative/neutral\",\n",
        "    \"confidence\": 0.0-1.0,\n",
        "    \"rationale\": \"Step-by-step reasoning in one or two sentences\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# Test prompt\n",
        "test_sentence = \"Net sales increased by 18.5% to EUR 167.8 million.\"\n",
        "print(\"=\" * 80)\n",
        "print(\"CHAIN-OF-THOUGHT PROMPT EXAMPLE (INSIGHT GENERATION)\")\n",
        "print(\"=\" * 80)\n",
        "print(create_cot_prompt(test_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6719e6",
      "metadata": {},
      "source": [
        "## 3. Model Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a4a646",
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_ollama(model_name, prompt, temperature=0.0):\n",
        "    \"\"\"Call Ollama API for LLM inference\"\"\"\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                OLLAMA_BASE_URL,\n",
        "                json={\n",
        "                    \"model\": model_name,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"stream\": False,\n",
        "                },\n",
        "                timeout=150,\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                return response.json().get(\"response\", \"\")\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2**attempt)\n",
        "                continue\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_llama(prompt, temperature=0.0):\n",
        "    \"\"\"Call Llama3.1:8b via Ollama (I7)\"\"\"\n",
        "    return call_ollama(\"llama3.1:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def call_qwen(prompt, temperature=0.0):\n",
        "    \"\"\"Call Qwen3:8b via Ollama (I8)\"\"\"\n",
        "    return call_ollama(\"qwen3:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def call_deepseek(prompt, temperature=0.0):\n",
        "    \"\"\"Call DeepSeek-R1:8b via Ollama (I9)\"\"\"\n",
        "    return call_ollama(\"deepseek-r1:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def parse_response(response_text):\n",
        "    \"\"\"Robustly parse JSON from potentially verbose CoT response\"\"\"\n",
        "    if not response_text:\n",
        "        return None\n",
        "    try:\n",
        "        if \"```json\" in response_text:\n",
        "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in response_text:\n",
        "            json_str = response_text.split(\"```\")[1].strip()\n",
        "        else:\n",
        "            match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
        "            if match:\n",
        "                json_str = match.group(1).strip()\n",
        "            else:\n",
        "                json_str = response_text.strip()\n",
        "        result = json.loads(json_str)\n",
        "        if \"sentiment\" in result and \"insight\" not in result:\n",
        "            result[\"insight\"] = result[\"sentiment\"]\n",
        "        return result\n",
        "    except Exception:\n",
        "        response_lower = response_text.lower()\n",
        "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
        "            return {\"insight\": \"positive\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        elif \"negative\" in response_lower:\n",
        "            return {\"insight\": \"negative\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        elif \"neutral\" in response_lower:\n",
        "            return {\"insight\": \"neutral\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"✓ Model inference functions defined\")\n",
        "print(\"  • call_llama() - Llama3.1:8b (I7)\")\n",
        "print(\"  • call_qwen() - Qwen3:8b (I8)\")\n",
        "print(\"  • call_deepseek() - DeepSeek-R1:8b (I9)\")\n",
        "print(\"  • parse_response() - JSON parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa3e124",
      "metadata": {},
      "source": [
        "## 4. Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9648a34b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test sample (remove .head(200) for full run)\n",
        "test_df = df.head(200).copy()\n",
        "\n",
        "\n",
        "def run_cot_experiment(test_df, model_func, model_name, exp_id):\n",
        "    \"\"\"Generic function to run CoT experiment\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Running {exp_id}: {model_name} (Chain-of-Thought)\")\n",
        "    print(\"=\" * 80)\n",
        "    results = []\n",
        "\n",
        "    for idx, row in tqdm(\n",
        "        test_df.iterrows(), total=len(test_df), desc=f\"{exp_id} Progress\"\n",
        "    ):\n",
        "        prompt = create_cot_prompt(row[\"sentence\"])\n",
        "        response = model_func(prompt)\n",
        "\n",
        "        if response:\n",
        "            parsed = parse_response(response)\n",
        "            if parsed:\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"sentence\": row[\"sentence\"],\n",
        "                        \"true_insight\": row[\"true_insight\"],\n",
        "                        \"predicted_insight\": parsed.get(\"insight\", \"unknown\"),\n",
        "                        \"confidence\": parsed.get(\"confidence\", 0),\n",
        "                        \"rationale\": parsed.get(\"rationale\", \"\"),\n",
        "                    }\n",
        "                )\n",
        "            else:\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"sentence\": row[\"sentence\"],\n",
        "                        \"true_insight\": row[\"true_insight\"],\n",
        "                        \"predicted_insight\": \"error\",\n",
        "                        \"confidence\": 0,\n",
        "                        \"rationale\": \"Parse error\",\n",
        "                    }\n",
        "                )\n",
        "        else:\n",
        "            results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": \"error\",\n",
        "                    \"confidence\": 0,\n",
        "                    \"rationale\": \"API call failed\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(f\"\\n✓ {exp_id} completed: {len(results_df)} predictions\")\n",
        "    print(f\"  Valid predictions: {len(results_df[results_df['predicted_insight'].isin(['positive', 'negative', 'neutral'])])}\")\n",
        "    print(f\"  Errors: {len(results_df[results_df['predicted_insight'] == 'error'])}\")\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Run all three experiments\n",
        "i7_df = run_cot_experiment(test_df, call_llama, \"Llama3.1:8b\", \"I7\")\n",
        "i8_df = run_cot_experiment(test_df, call_qwen, \"Qwen3:8b\", \"I8\")\n",
        "i9_df = run_cot_experiment(test_df, call_deepseek, \"DeepSeek-R1:8b\", \"I9\")\n",
        "\n",
        "display(i7_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1fd992",
      "metadata": {},
      "source": [
        "## 5. Calculate Metrics & Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80f3b4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(df, exp_name):\n",
        "    \"\"\"Calculate all evaluation metrics including MCC\"\"\"\n",
        "    if df.empty or \"predicted_insight\" not in df.columns:\n",
        "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
        "        return (\n",
        "            {\"Experiment\": exp_name, \"Total Samples\": 0, \"Valid Predictions\": 0, \"Accuracy\": 0, \"Macro-F1\": 0, \"Weighted-F1\": 0, \"Macro-Precision\": 0, \"Macro-Recall\": 0, \"MCC\": 0},\n",
        "            np.zeros((3, 3)),\n",
        "            pd.DataFrame(),\n",
        "        )\n",
        "    valid_df = df[df[\"predicted_insight\"].isin([\"positive\", \"negative\", \"neutral\"])].copy()\n",
        "    if valid_df.empty:\n",
        "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
        "        return (\n",
        "            {\"Experiment\": exp_name, \"Total Samples\": len(df), \"Valid Predictions\": 0, \"Accuracy\": 0, \"Macro-F1\": 0, \"Weighted-F1\": 0, \"Macro-Precision\": 0, \"Macro-Recall\": 0, \"MCC\": 0},\n",
        "            np.zeros((3, 3)),\n",
        "            pd.DataFrame(),\n",
        "        )\n",
        "    y_true = valid_df[\"true_insight\"]\n",
        "    y_pred = valid_df[\"predicted_insight\"]\n",
        "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
        "    metrics = {\n",
        "        \"Experiment\": exp_name,\n",
        "        \"Total Samples\": len(df),\n",
        "        \"Valid Predictions\": len(valid_df),\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"MCC\": mcc_score,\n",
        "    }\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    return metrics, cm, valid_df\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CALCULATING METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "i7_metrics, i7_cm, i7_valid = calculate_metrics(i7_df, \"I7: Llama3.1:8b (CoT)\")\n",
        "i8_metrics, i8_cm, i8_valid = calculate_metrics(i8_df, \"I8: Qwen3:8b (CoT)\")\n",
        "i9_metrics, i9_cm, i9_valid = calculate_metrics(i9_df, \"I9: DeepSeek-R1:8b (CoT)\")\n",
        "\n",
        "metrics_df = pd.DataFrame([i7_metrics, i8_metrics, i9_metrics])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CHAIN-OF-THOUGHT INSIGHT GENERATION PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "display(\n",
        "    metrics_df[[\"Experiment\", \"Valid Predictions\", \"Accuracy\", \"Macro-F1\", \"MCC\"]].round(4)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7b4348",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics computed above. Optional: Confusion matrix visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
        "for idx, (cm, title) in enumerate([\n",
        "    (i7_cm, \"I7: Llama3.1:8b\"),\n",
        "    (i8_cm, \"I8: Qwen3:8b\"),\n",
        "    (i9_cm, \"I9: DeepSeek-R1:8b\"),\n",
        "]):\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=labels, yticklabels=labels, ax=axes[idx])\n",
        "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
        "    axes[idx].set_ylabel(\"True Label\", fontsize=11)\n",
        "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11)\n",
        "plt.suptitle(\"Confusion Matrices - Chain-of-Thought Insight Generation\", fontsize=14, weight=\"bold\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cot_insight_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2da85e7",
      "metadata": {},
      "source": [
        "## 6. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b2b170",
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "i7_df.to_csv(f\"i7_llama3.1_8b_cot_insight_{timestamp}.csv\", index=False)\n",
        "i8_df.to_csv(f\"i8_qwen3_8b_cot_insight_{timestamp}.csv\", index=False)\n",
        "i9_df.to_csv(f\"i9_deepseek_r1_8b_cot_insight_{timestamp}.csv\", index=False)\n",
        "metrics_df.to_csv(f\"cot_insight_metrics_summary_{timestamp}.csv\", index=False)\n",
        "\n",
        "print(f\"\\n✓ Chain-of-Thought Insight Generation results saved\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
