{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fd0b1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip3 install pandas numpy matplotlib seaborn scikit-learn tqdm requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f3c070",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama API setup and imports\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(\"✓ Ollama will be used for LLM inference\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SETUP COMPLETE - FEW-SHOT INSIGHT GENERATION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Models configured:\")\n",
        "print(\"  • I4: Llama3.1:8b (Ollama - Local - Few-Shot)\")\n",
        "print(\"  • I5: Qwen3:8b (Ollama - Local - Few-Shot)\")\n",
        "print(\"  • I6: DeepSeek-R1:8b (Ollama - Local - Few-Shot)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (14, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76bad17b",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa34453",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the 100% agreement dataset (highest quality)\n",
        "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
        "\n",
        "sentences = []\n",
        "insights = []\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if \"@\" in line:\n",
        "            parts = line.rsplit(\"@\", 1)\n",
        "            if len(parts) == 2:\n",
        "                sentences.append(parts[0])\n",
        "                insights.append(parts[1])\n",
        "\n",
        "df = pd.DataFrame({\"sentence\": sentences, \"true_insight\": insights})\n",
        "\n",
        "print(f\"Dataset loaded: {len(df)} sentences\")\n",
        "print(f\"\\nInsight distribution:\")\n",
        "print(df[\"true_insight\"].value_counts())\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Sample sentences:\")\n",
        "print(\"=\" * 80)\n",
        "display(df.sample(5, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daeb34c9",
      "metadata": {},
      "source": [
        "## 2. Few-Shot Examples\n",
        "\n",
        "Carefully curated examples (2 positive, 2 negative, 1 neutral) representing typical financial insight patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89068875",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curated few-shot examples for INSIGHT GENERATION\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"sentence\": \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\",\n",
        "        \"insight\": \"positive\",\n",
        "        \"rationale\": \"Operating profit increased significantly, indicating improved financial performance.\",\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"Net sales increased by 18.5% to EUR 167.8 million compared to the previous year.\",\n",
        "        \"insight\": \"positive\",\n",
        "        \"rationale\": \"Strong revenue growth of 18.5% signals business expansion and market success.\",\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The company reported a net loss of EUR 2.5 million compared to a profit of EUR 1.2 million in the previous quarter.\",\n",
        "        \"insight\": \"negative\",\n",
        "        \"rationale\": \"Shift from profit to loss represents deteriorating financial health.\",\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"Sales decreased by 15% year-over-year due to weakening demand in key markets.\",\n",
        "        \"insight\": \"negative\",\n",
        "        \"rationale\": \"Significant sales decline indicates business challenges and market difficulties.\",\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The company announced the appointment of a new chief financial officer effective next month.\",\n",
        "        \"insight\": \"neutral\",\n",
        "        \"rationale\": \"Executive appointment is routine corporate news without clear financial impact.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"Few-Shot Examples for INSIGHT GENERATION:\")\n",
        "print(\"=\" * 80)\n",
        "for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
        "    print(f\"\\nExample {i} [{ex['insight'].upper()}]:\")\n",
        "    print(f\"Sentence: {ex['sentence']}\")\n",
        "    print(f\"Insight: {ex['rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6aa5433",
      "metadata": {},
      "source": [
        "## 3. Few-Shot Prompt Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790103e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_few_shot_prompt(sentence):\n",
        "    \"\"\"\n",
        "    Creates a few-shot prompt with 5 labeled examples for financial insight generation.\n",
        "    \"\"\"\n",
        "    examples_text = \"\"\n",
        "    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
        "        examples_text += f\"\"\"\\nExample {i}:\n",
        "Sentence: \"{ex[\"sentence\"]}\"\n",
        "Analysis:\n",
        "{{\n",
        "    \"insight\": \"{ex[\"insight\"]}\",\n",
        "    \"confidence\": 0.95,\n",
        "    \"rationale\": \"{ex[\"rationale\"]}\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are a financial insight analysis expert.\n",
        "\n",
        "Classify the following financial statement as \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
        "\n",
        "Guidelines:\n",
        "- Positive: Good news for stock price (revenue increase, profit growth, expansion)\n",
        "- Negative: Bad news for stock price (losses, declining sales, setbacks)\n",
        "- Neutral: No clear impact or mixed signals\n",
        "\n",
        "Here are 5 examples to learn from:\n",
        "{examples_text}\n",
        "\n",
        "Now classify this new statement:\n",
        "Sentence: \"{sentence}\"\n",
        "\n",
        "Provide your response in JSON format:\n",
        "{{\n",
        "    \"insight\": \"positive/negative/neutral\",\n",
        "    \"confidence\": 0.0-1.0,\n",
        "    \"rationale\": \"Brief explanation\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# Test prompt\n",
        "test_sentence = \"The company's quarterly revenue exceeded analyst expectations by 12%.\"\n",
        "print(\"=\" * 80)\n",
        "print(\"FEW-SHOT PROMPT EXAMPLE (INSIGHT GENERATION)\")\n",
        "print(\"=\" * 80)\n",
        "print(create_few_shot_prompt(test_sentence)[:1000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9233c39f",
      "metadata": {},
      "source": [
        "## 4. Model Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0375db93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama API configuration\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "\n",
        "def call_ollama(model_name, prompt, temperature=0.0):\n",
        "    \"\"\"Generic function to call Ollama API for any model\"\"\"\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                OLLAMA_BASE_URL,\n",
        "                json={\n",
        "                    \"model\": model_name,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"stream\": False,\n",
        "                },\n",
        "                timeout=120,\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                return response.json().get(\"response\", \"\")\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2**attempt)\n",
        "                    continue\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2**attempt)\n",
        "                continue\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def call_llama(prompt, temperature=0.0):\n",
        "    \"\"\"Call Llama3.1:8b via Ollama API (I4)\"\"\"\n",
        "    return call_ollama(\"llama3.1:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def call_qwen(prompt, temperature=0.0):\n",
        "    \"\"\"Call Qwen3:8b via Ollama API (I5)\"\"\"\n",
        "    return call_ollama(\"qwen3:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def call_deepseek(prompt, temperature=0.0):\n",
        "    \"\"\"Call DeepSeek-R1:8b via Ollama API (I6)\"\"\"\n",
        "    return call_ollama(\"deepseek-r1:8b\", prompt, temperature)\n",
        "\n",
        "\n",
        "def parse_response(response_text):\n",
        "    \"\"\"Robustly parse JSON response, handling conversational filler and markdown blocks.\"\"\"\n",
        "    if not response_text:\n",
        "        return None\n",
        "    try:\n",
        "        if \"```json\" in response_text:\n",
        "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in response_text:\n",
        "            json_str = response_text.split(\"```\")[1].strip()\n",
        "        else:\n",
        "            match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
        "            if match:\n",
        "                json_str = match.group(1).strip()\n",
        "            else:\n",
        "                json_str = response_text.strip()\n",
        "        result = json.loads(json_str)\n",
        "        if \"sentiment\" in result and \"insight\" not in result:\n",
        "            result[\"insight\"] = result[\"sentiment\"]\n",
        "        return result\n",
        "    except Exception:\n",
        "        response_lower = response_text.lower()\n",
        "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
        "            return {\"insight\": \"positive\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        elif \"negative\" in response_lower:\n",
        "            return {\"insight\": \"negative\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        elif \"neutral\" in response_lower:\n",
        "            return {\"insight\": \"neutral\", \"confidence\": 0.5, \"rationale\": \"Parsed from text\"}\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"✓ Model inference functions defined\")\n",
        "print(\"  • call_llama() - Llama3.1:8b (I4)\")\n",
        "print(\"  • call_qwen() - Qwen3:8b (I5)\")\n",
        "print(\"  • call_deepseek() - DeepSeek-R1:8b (I6)\")\n",
        "print(\"  • parse_response() - JSON parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9dd8cb",
      "metadata": {},
      "source": [
        "## 5. Run Experiments\n",
        "\n",
        "### I4: Llama3.1:8b (Few-Shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05f00fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test sample (remove .head(200) for full run)\n",
        "test_df = df.head(200).copy()\n",
        "\n",
        "# I4: Llama3.1:8b\n",
        "print(\"=\" * 80)\n",
        "print(\"Running I4: Llama3.1:8b (Few-Shot)\")\n",
        "print(\"=\" * 80)\n",
        "i4_results = []\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"I4 Progress\"):\n",
        "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
        "    response = call_llama(prompt)\n",
        "\n",
        "    if response:\n",
        "        parsed = parse_response(response)\n",
        "        if parsed:\n",
        "            i4_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": parsed.get(\"insight\", \"unknown\"),\n",
        "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
        "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            i4_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": \"error\",\n",
        "                    \"confidence\": 0,\n",
        "                    \"rationale\": \"Parse error\",\n",
        "                }\n",
        "            )\n",
        "    else:\n",
        "        i4_results.append(\n",
        "            {\n",
        "                \"sentence\": row[\"sentence\"],\n",
        "                \"true_insight\": row[\"true_insight\"],\n",
        "                \"predicted_insight\": \"error\",\n",
        "                \"confidence\": 0,\n",
        "                \"rationale\": \"API call failed\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "i4_df = pd.DataFrame(i4_results)\n",
        "print(f\"\\n✓ I4 completed: {len(i4_df)} predictions\")\n",
        "print(f\"  Valid predictions: {len(i4_df[i4_df['predicted_insight'].isin(['positive', 'negative', 'neutral'])])}\")\n",
        "print(f\"  Errors: {len(i4_df[i4_df['predicted_insight'] == 'error'])}\")\n",
        "display(i4_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0daa2de7",
      "metadata": {},
      "source": [
        "### I5: Qwen3:8b (Few-Shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6f63c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# I5: Qwen3:8b\n",
        "print(\"=\" * 80)\n",
        "print(\"Running I5: Qwen3:8b (Few-Shot)\")\n",
        "print(\"=\" * 80)\n",
        "i5_results = []\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"I5 Progress\"):\n",
        "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
        "    response = call_qwen(prompt)\n",
        "\n",
        "    if response:\n",
        "        parsed = parse_response(response)\n",
        "        if parsed:\n",
        "            i5_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": parsed.get(\"insight\", \"unknown\"),\n",
        "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
        "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            i5_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": \"error\",\n",
        "                    \"confidence\": 0,\n",
        "                    \"rationale\": \"Parse error\",\n",
        "                }\n",
        "            )\n",
        "    else:\n",
        "        i5_results.append(\n",
        "            {\n",
        "                \"sentence\": row[\"sentence\"],\n",
        "                \"true_insight\": row[\"true_insight\"],\n",
        "                \"predicted_insight\": \"error\",\n",
        "                \"confidence\": 0,\n",
        "                \"rationale\": \"API call failed\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "i5_df = pd.DataFrame(i5_results)\n",
        "print(f\"\\n✓ I5 completed: {len(i5_df)} predictions\")\n",
        "print(f\"  Valid predictions: {len(i5_df[i5_df['predicted_insight'].isin(['positive', 'negative', 'neutral'])])}\")\n",
        "print(f\"  Errors: {len(i5_df[i5_df['predicted_insight'] == 'error'])}\")\n",
        "display(i5_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e088fd90",
      "metadata": {},
      "source": [
        "### I6: DeepSeek-R1:8b (Few-Shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c35cb0b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# I6: DeepSeek-R1:8b\n",
        "print(\"=\" * 80)\n",
        "print(\"Running I6: DeepSeek-R1:8b (Few-Shot)\")\n",
        "print(\"=\" * 80)\n",
        "i6_results = []\n",
        "\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"I6 Progress\"):\n",
        "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
        "    response = call_deepseek(prompt)\n",
        "\n",
        "    if response:\n",
        "        parsed = parse_response(response)\n",
        "        if parsed:\n",
        "            i6_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": parsed.get(\"insight\", \"unknown\"),\n",
        "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
        "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            i6_results.append(\n",
        "                {\n",
        "                    \"sentence\": row[\"sentence\"],\n",
        "                    \"true_insight\": row[\"true_insight\"],\n",
        "                    \"predicted_insight\": \"error\",\n",
        "                    \"confidence\": 0,\n",
        "                    \"rationale\": \"Parse error\",\n",
        "                }\n",
        "            )\n",
        "    else:\n",
        "        i6_results.append(\n",
        "            {\n",
        "                \"sentence\": row[\"sentence\"],\n",
        "                \"true_insight\": row[\"true_insight\"],\n",
        "                \"predicted_insight\": \"error\",\n",
        "                \"confidence\": 0,\n",
        "                \"rationale\": \"API call failed\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "i6_df = pd.DataFrame(i6_results)\n",
        "print(f\"\\n✓ I6 completed: {len(i6_df)} predictions\")\n",
        "print(f\"  Valid predictions: {len(i6_df[i6_df['predicted_insight'].isin(['positive', 'negative', 'neutral'])])}\")\n",
        "print(f\"  Errors: {len(i6_df[i6_df['predicted_insight'] == 'error'])}\")\n",
        "display(i6_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fea0062",
      "metadata": {},
      "source": [
        "## 6. Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b27fe2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(df, exp_name):\n",
        "    \"\"\"Calculate all evaluation metrics including MCC\"\"\"\n",
        "    if df.empty or \"predicted_insight\" not in df.columns:\n",
        "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
        "        return (\n",
        "            {\n",
        "                \"Experiment\": exp_name,\n",
        "                \"Total Samples\": 0,\n",
        "                \"Valid Predictions\": 0,\n",
        "                \"Accuracy\": 0,\n",
        "                \"Macro-F1\": 0,\n",
        "                \"Weighted-F1\": 0,\n",
        "                \"Macro-Precision\": 0,\n",
        "                \"Macro-Recall\": 0,\n",
        "                \"MCC\": 0,\n",
        "                \"Positive_Precision\": 0,\n",
        "                \"Positive_Recall\": 0,\n",
        "                \"Positive_F1\": 0,\n",
        "                \"Negative_Precision\": 0,\n",
        "                \"Negative_Recall\": 0,\n",
        "                \"Negative_F1\": 0,\n",
        "                \"Neutral_Precision\": 0,\n",
        "                \"Neutral_Recall\": 0,\n",
        "                \"Neutral_F1\": 0,\n",
        "            },\n",
        "            np.zeros((3, 3)),\n",
        "            pd.DataFrame(),\n",
        "        )\n",
        "\n",
        "    valid_df = df[\n",
        "        df[\"predicted_insight\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
        "    ].copy()\n",
        "\n",
        "    if valid_df.empty:\n",
        "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
        "        return (\n",
        "            {\n",
        "                \"Experiment\": exp_name,\n",
        "                \"Total Samples\": len(df),\n",
        "                \"Valid Predictions\": 0,\n",
        "                \"Accuracy\": 0,\n",
        "                \"Macro-F1\": 0,\n",
        "                \"Weighted-F1\": 0,\n",
        "                \"Macro-Precision\": 0,\n",
        "                \"Macro-Recall\": 0,\n",
        "                \"MCC\": 0,\n",
        "                \"Positive_Precision\": 0,\n",
        "                \"Positive_Recall\": 0,\n",
        "                \"Positive_F1\": 0,\n",
        "                \"Negative_Precision\": 0,\n",
        "                \"Negative_Recall\": 0,\n",
        "                \"Negative_F1\": 0,\n",
        "                \"Neutral_Precision\": 0,\n",
        "                \"Neutral_Recall\": 0,\n",
        "                \"Neutral_F1\": 0,\n",
        "            },\n",
        "            np.zeros((3, 3)),\n",
        "            pd.DataFrame(),\n",
        "        )\n",
        "\n",
        "    y_true = valid_df[\"true_insight\"]\n",
        "    y_pred = valid_df[\"predicted_insight\"]\n",
        "\n",
        "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        \"Experiment\": exp_name,\n",
        "        \"Total Samples\": len(df),\n",
        "        \"Valid Predictions\": len(valid_df),\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"MCC\": mcc_score,\n",
        "    }\n",
        "\n",
        "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "    precision_per_class = precision_score(\n",
        "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "    )\n",
        "    recall_per_class = recall_score(\n",
        "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "    )\n",
        "    f1_per_class = f1_score(\n",
        "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
        "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
        "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    return metrics, cm, valid_df\n",
        "\n",
        "\n",
        "# Calculate metrics for all experiments\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CALCULATING METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "i4_metrics, i4_cm, i4_valid = calculate_metrics(i4_df, \"I4: Llama3.1:8b (Few-Shot)\")\n",
        "i5_metrics, i5_cm, i5_valid = calculate_metrics(i5_df, \"I5: Qwen3:8b (Few-Shot)\")\n",
        "i6_metrics, i6_cm, i6_valid = calculate_metrics(i6_df, \"I6: DeepSeek-R1:8b (Few-Shot)\")\n",
        "\n",
        "metrics_df = pd.DataFrame([i4_metrics, i5_metrics, i6_metrics])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEW-SHOT INSIGHT GENERATION PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "display(\n",
        "    metrics_df[\n",
        "        [\"Experiment\", \"Valid Predictions\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
        "    ].round(4)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETAILED METRICS\")\n",
        "print(\"=\" * 80)\n",
        "display(\n",
        "    metrics_df[[\"Experiment\", \"Macro-Precision\", \"Macro-Recall\", \"Weighted-F1\"]].round(4)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PER-CLASS F1 SCORES\")\n",
        "print(\"=\" * 80)\n",
        "display(\n",
        "    metrics_df[\n",
        "        [\"Experiment\", \"Positive_F1\", \"Negative_F1\", \"Neutral_F1\"]\n",
        "    ].round(4)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b27e02f3",
      "metadata": {},
      "source": [
        "## 7. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f850666f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics and confusion matrices (i4_cm, i5_cm, i6_cm) computed in cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74071aeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
        "\n",
        "for idx, (cm, title) in enumerate(\n",
        "    [\n",
        "        (i4_cm, \"I4: Llama3.1:8b\"),\n",
        "        (i5_cm, \"I5: Qwen3:8b\"),\n",
        "        (i6_cm, \"I6: DeepSeek-R1:8b\"),\n",
        "    ]\n",
        "):\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Greens\",\n",
        "        xticklabels=labels,\n",
        "        yticklabels=labels,\n",
        "        ax=axes[idx],\n",
        "    )\n",
        "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
        "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
        "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
        "\n",
        "plt.suptitle(\n",
        "    \"Confusion Matrices - Few-Shot Insight Generation\",\n",
        "    fontsize=14,\n",
        "    weight=\"bold\",\n",
        "    y=1.02,\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"few_shot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a39b34",
      "metadata": {},
      "source": [
        "## 8. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50d8f8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "i4_df.to_csv(f\"i4_llama3.1_8b_few_shot_insight_{timestamp}.csv\", index=False)\n",
        "i5_df.to_csv(f\"i5_qwen3_8b_few_shot_insight_{timestamp}.csv\", index=False)\n",
        "i6_df.to_csv(f\"i6_deepseek_r1_8b_few_shot_insight_{timestamp}.csv\", index=False)\n",
        "metrics_df.to_csv(f\"few_shot_insight_metrics_summary_{timestamp}.csv\", index=False)\n",
        "\n",
        "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8b2571",
      "metadata": {},
      "source": [
        "## 9. Key Insights\n",
        "\n",
        "### Few-Shot Learning Impact:\n",
        "\n",
        "1. **Learning from Examples**: Compare performance improvements vs zero-shot\n",
        "2. **Model Comparison**: Which model benefits most from examples?\n",
        "3. **Example Quality**: How do curated examples influence predictions?\n",
        "4. **Confidence Calibration**: Are models more confident with examples?"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
