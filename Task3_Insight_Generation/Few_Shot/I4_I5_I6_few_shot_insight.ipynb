{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "# Fix SSL/TLS certificate verification for gRPC (required for Google Gemini API on macOS)\n",
    "os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''\n",
    "os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# API setup\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bad17b",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa34453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    \"../../FinancialPhraseBank_Analysis/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb34c9",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Examples\n",
    "\n",
    "Carefully curated examples (2 positive, 2 negative, 1 neutral) representing typical financial sentiment patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated few-shot examples\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"sentence\": \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Operating profit increased significantly, indicating improved financial performance.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Net sales increased by 18.5% to EUR 167.8 million compared to the previous year.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Strong revenue growth of 18.5% signals business expansion and market success.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company reported a net loss of EUR 2.5 million compared to a profit of EUR 1.2 million in the previous quarter.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Shift from profit to loss represents deteriorating financial health.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Sales decreased by 15% year-over-year due to weakening demand in key markets.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Significant sales decline indicates business challenges and market difficulties.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company announced the appointment of a new chief financial officer effective next month.\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"rationale\": \"Executive appointment is routine corporate news without clear financial impact.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Few-Shot Examples:\")\n",
    "print(\"=\" * 80)\n",
    "for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "    print(f\"\\nExample {i} [{ex['sentiment'].upper()}]:\")\n",
    "    print(f\"Sentence: {ex['sentence']}\")\n",
    "    print(f\"Rationale: {ex['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa5433",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a few-shot prompt with 5 labeled examples.\n",
    "    \"\"\"\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "        examples_text += f\"\"\"\\nExample {i}:\n",
    "Sentence: \"{ex[\"sentence\"]}\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"sentiment\": \"{ex[\"sentiment\"]}\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"rationale\": \"{ex[\"rationale\"]}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert.\n",
    "\n",
    "Classify the sentiment of financial statements as \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Guidelines:\n",
    "- Positive: Good news for stock price (revenue increase, profit growth, expansion)\n",
    "- Negative: Bad news for stock price (losses, declining sales, setbacks)\n",
    "- Neutral: No clear impact or mixed signals\n",
    "\n",
    "Here are 5 examples to learn from:\n",
    "{examples_text}\n",
    "\n",
    "Now classify this new statement:\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Provide your response in JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"The company's quarterly revenue exceeded analyst expectations by 12%.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"FEW-SHOT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_few_shot_prompt(test_sentence)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233c39f",
   "metadata": {},
   "source": [
    "## 4. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\", temperature=0.0):\n",
    "    \"\"\"Call Gemini API with retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=500,\n",
    "                ),\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dd8cb",
   "metadata": {},
   "source": [
    "## 5. Run Experiments\n",
    "\n",
    "### E4: Gemini 2.5 Pro (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample (remove .head(100) for full run)\n",
    "test_df = df.head(100).copy()\n",
    "\n",
    "# E4: Gemini 2.5 Pro\n",
    "print(\"Running E4: Gemini 2.5 Pro (Few-Shot)...\")\n",
    "e4_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E4 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e4_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e4_df = pd.DataFrame(e4_results)\n",
    "print(f\"\\n✓ E4 completed: {len(e4_df)} predictions\")\n",
    "display(e4_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa2de7",
   "metadata": {},
   "source": [
    "### E5: Gemini 2.5 Flash (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5: Gemini 2.5 Flash\n",
    "print(\"Running E5: Gemini 2.5 Flash (Few-Shot)...\")\n",
    "e5_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E5 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e5_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e5_df = pd.DataFrame(e5_results)\n",
    "print(f\"\\n✓ E5 completed: {len(e5_df)} predictions\")\n",
    "display(e5_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088fd90",
   "metadata": {},
   "source": [
    "### E6: Llama-3.3-70B (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E6: Llama-3.3-70B\n",
    "print(\"Running E6: Llama-3.3-70B (Few-Shot)...\")\n",
    "e6_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E6 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            e6_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e6_df = pd.DataFrame(e6_results)\n",
    "print(f\"\\n✓ E6 completed: {len(e6_df)} predictions\")\n",
    "display(e6_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea0062",
   "metadata": {},
   "source": [
    "## 6. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "e4_metrics, e4_cm, e4_valid = calculate_metrics(e4_df, \"E4: Gemini Pro (Few-Shot)\")\n",
    "e5_metrics, e5_cm, e5_valid = calculate_metrics(e5_df, \"E5: Gemini Flash (Few-Shot)\")\n",
    "e6_metrics, e6_cm, e6_valid = calculate_metrics(e6_df, \"E6: Llama-3.3-70B (Few-Shot)\")\n",
    "\n",
    "metrics_df = pd.DataFrame([e4_metrics, e5_metrics, e6_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEW-SHOT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e02f3",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "e4_metrics, e4_cm, e4_valid = calculate_metrics(e4_df, \"E4: Gemini Pro\")\n",
    "e5_metrics, e5_cm, e5_valid = calculate_metrics(e5_df, \"E5: Gemini Flash\")\n",
    "e6_metrics, e6_cm, e6_valid = calculate_metrics(e6_df, \"E6: Llama-3.3-70B\")\n",
    "\n",
    "# Create comparison table\n",
    "metrics_df = pd.DataFrame([e4_metrics, e5_metrics, e6_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEW-SHOT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74071aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (e4_cm, \"E4: Gemini Pro\"),\n",
    "        (e5_cm, \"E5: Gemini Flash\"),\n",
    "        (e6_cm, \"E6: Llama-3.3-70B\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Greens\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Few-Shot Sentiment Analysis\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a39b34",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e4_df.to_csv(f\"e4_gemini_pro_few_shot_{timestamp}.csv\", index=False)\n",
    "e5_df.to_csv(f\"e5_gemini_flash_few_shot_{timestamp}.csv\", index=False)\n",
    "e6_df.to_csv(f\"e6_llama_few_shot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"few_shot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b2571",
   "metadata": {},
   "source": [
    "## 9. Key Insights\n",
    "\n",
    "### Few-Shot Learning Impact:\n",
    "\n",
    "1. **Learning from Examples**: Compare performance improvements vs zero-shot\n",
    "2. **Model Comparison**: Which model benefits most from examples?\n",
    "3. **Example Quality**: How do curated examples influence predictions?\n",
    "4. **Confidence Calibration**: Are models more confident with examples?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}