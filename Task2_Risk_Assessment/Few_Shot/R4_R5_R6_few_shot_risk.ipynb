{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# API setup\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Groq API (for Mixtral and Llama)\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "    print(\"‚úì Groq API configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: GROQ_API_KEY not found in environment variables\")\n",
    "    groq_client = None\n",
    "\n",
    "# Initialize FinBERT\n",
    "print(\"Loading FinBERT model (ProsusAI/finbert)...\")\n",
    "try:\n",
    "    finbert_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"ProsusAI/finbert\",\n",
    "        tokenizer=\"ProsusAI/finbert\",\n",
    "        device=-1,  # CPU\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"‚úì FinBERT model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  FinBERT loading failed: {e}\")\n",
    "    finbert_pipeline = None\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE - FEW-SHOT RISK ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Models configured:\")\n",
    "print(\"  ‚Ä¢ R4: Mixtral-8x7B-32768 (Groq API)\")\n",
    "print(\"  ‚Ä¢ R5: Llama-3.1-70B-Versatile (Groq API)\")\n",
    "print(\"  ‚Ä¢ R6: FinBERT (ProsusAI/finbert - Local)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bad17b",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa34453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "print(f\"\\nSample sentence:\")\n",
    "print(f\"  '{df.iloc[0]['sentence']}' -> {df.iloc[0]['true_sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb34c9",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Examples\n",
    "\n",
    "Carefully curated examples (2 positive, 2 negative, 1 neutral) representing typical financial sentiment patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated few-shot examples\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"sentence\": \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Operating profit increased significantly, indicating improved financial performance.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Net sales increased by 18.5% to EUR 167.8 million compared to the previous year.\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"rationale\": \"Strong revenue growth of 18.5% signals business expansion and market success.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company reported a net loss of EUR 2.5 million compared to a profit of EUR 1.2 million in the previous quarter.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Shift from profit to loss represents deteriorating financial health.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"Sales decreased by 15% year-over-year due to weakening demand in key markets.\",\n",
    "        \"sentiment\": \"negative\",\n",
    "        \"rationale\": \"Significant sales decline indicates business challenges and market difficulties.\",\n",
    "    },\n",
    "    {\n",
    "        \"sentence\": \"The company announced the appointment of a new chief financial officer effective next month.\",\n",
    "        \"sentiment\": \"neutral\",\n",
    "        \"rationale\": \"Executive appointment is routine corporate news without clear financial impact.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Few-Shot Examples:\")\n",
    "print(\"=\" * 80)\n",
    "for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "    print(f\"\\nExample {i} [{ex['sentiment'].upper()}]:\")\n",
    "    print(f\"Sentence: {ex['sentence']}\")\n",
    "    print(f\"Rationale: {ex['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa5433",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a few-shot prompt with 5 labeled examples.\n",
    "    \"\"\"\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(FEW_SHOT_EXAMPLES, 1):\n",
    "        examples_text += f\"\"\"\\nExample {i}:\n",
    "Sentence: \"{ex[\"sentence\"]}\"\n",
    "Analysis:\n",
    "{{\n",
    "    \"sentiment\": \"{ex[\"sentiment\"]}\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"rationale\": \"{ex[\"rationale\"]}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert.\n",
    "\n",
    "Classify the sentiment of financial statements as \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Guidelines:\n",
    "- Positive: Good news for stock price (revenue increase, profit growth, expansion)\n",
    "- Negative: Bad news for stock price (losses, declining sales, setbacks)\n",
    "- Neutral: No clear impact or mixed signals\n",
    "\n",
    "Here are 5 examples to learn from:\n",
    "{examples_text}\n",
    "\n",
    "Now classify this new statement:\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Provide your response in JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"The company's quarterly revenue exceeded analyst expectations by 12%.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"FEW-SHOT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_few_shot_prompt(test_sentence)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233c39f",
   "metadata": {},
   "source": [
    "## 4. Model Inference Functions\n",
    "\n",
    "Using Mixtral-8x7B, Llama-3.1-70B (via Groq), and FinBERT (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mixtral(prompt, temperature=0.0):\n",
    "    \"\"\"Call Mixtral-8x7B via Groq API\"\"\"\n",
    "    if not groq_client:\n",
    "        print(\"‚ö†Ô∏è  Groq client not initialized\")\n",
    "        return None\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"mixtral-8x7b-32768\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            print(f\"Error calling Mixtral: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    \"\"\"Call Llama-3.1-70B via Groq API\"\"\"\n",
    "    if not groq_client:\n",
    "        print(\"‚ö†Ô∏è  Groq client not initialized\")\n",
    "        return None\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            print(f\"Error calling Llama: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"Call FinBERT model for sentiment analysis\"\"\"\n",
    "    if not finbert_pipeline:\n",
    "        print(\"‚ö†Ô∏è  FinBERT pipeline not initialized\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence)[0]\n",
    "\n",
    "        # Map FinBERT labels to our format\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "\n",
    "        sentiment = label_map.get(result[\"label\"].lower(), \"neutral\")\n",
    "        confidence = result[\"score\"]\n",
    "\n",
    "        # Create JSON response matching other models\n",
    "        response = {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"confidence\": confidence,\n",
    "            \"rationale\": f\"FinBERT classification with {confidence:.2%} confidence\",\n",
    "        }\n",
    "\n",
    "        return json.dumps(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with FinBERT: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úì Model inference functions defined\")\n",
    "print(\"  ‚Ä¢ call_mixtral() - Mixtral-8x7B-32768\")\n",
    "print(\"  ‚Ä¢ call_llama() - Llama-3.1-70B-Versatile\")\n",
    "print(\"  ‚Ä¢ call_finbert() - FinBERT (ProsusAI/finbert)\")\n",
    "print(\"  ‚Ä¢ parse_response() - JSON parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9dd8cb",
   "metadata": {},
   "source": [
    "## 5. Run Experiments\n",
    "\n",
    "### R4: Mixtral-8x7B-32768 (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample (remove .head(100) for full run)\n",
    "test_df = df.head(100).copy()\n",
    "\n",
    "# R4: Mixtral-8x7B-32768\n",
    "print(\"=\" * 80)\n",
    "print(\"Running R4: Mixtral-8x7B-32768 (Few-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "r4_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R4 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_mixtral(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r4_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r4_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r4_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"API call failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "r4_df = pd.DataFrame(r4_results)\n",
    "print(f\"\\n‚úì R4 completed: {len(r4_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r4_df[r4_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r4_df[r4_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r4_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa2de7",
   "metadata": {},
   "source": [
    "### R5: Llama-3.1-70B-Versatile (Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R5: Llama-3.1-70B-Versatile\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Running R5: Llama-3.1-70B-Versatile (Few-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "r5_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R5 Progress\"):\n",
    "    prompt = create_few_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r5_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r5_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r5_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"API call failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "r5_df = pd.DataFrame(r5_results)\n",
    "print(f\"\\n‚úì R5 completed: {len(r5_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r5_df[r5_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r5_df[r5_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r5_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088fd90",
   "metadata": {},
   "source": [
    "### R6: FinBERT (ProsusAI/finbert - Few-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R6: FinBERT\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Running R6: FinBERT (ProsusAI/finbert - Few-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Note: FinBERT doesn't use few-shot examples - it's a fine-tuned model\")\n",
    "r6_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R6 Progress\"):\n",
    "    # FinBERT doesn't need the full prompt - just the sentence\n",
    "    response = call_finbert(row[\"sentence\"])\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r6_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r6_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r6_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"Model inference failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.1)  # Shorter delay for local model\n",
    "\n",
    "r6_df = pd.DataFrame(r6_results)\n",
    "print(f\"\\n‚úì R6 completed: {len(r6_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r6_df[r6_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r6_df[r6_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r6_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea0062",
   "metadata": {},
   "source": [
    "## 6. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics including MCC\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"MCC\": mcc_score,\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CALCULATING METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r4_metrics, r4_cm, r4_valid = calculate_metrics(r4_df, \"R4: Mixtral-8x7B (Few-Shot)\")\n",
    "r5_metrics, r5_cm, r5_valid = calculate_metrics(r5_df, \"R5: Llama-3.1-70B (Few-Shot)\")\n",
    "r6_metrics, r6_cm, r6_valid = calculate_metrics(r6_df, \"R6: FinBERT (Few-Shot)\")\n",
    "\n",
    "# Create comparison table\n",
    "metrics_df = pd.DataFrame([r4_metrics, r5_metrics, r6_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEW-SHOT RISK ASSESSMENT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Valid Predictions\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "    ].round(4)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED METRICS\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[[\"Experiment\", \"Macro-Precision\", \"Macro-Recall\", \"Weighted-F1\"]].round(\n",
    "        4\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df[[\"Experiment\", \"Positive_F1\", \"Negative_F1\", \"Neutral_F1\"]].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e02f3",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74071aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (r4_metrics, \"Mixtral-8x7B\"),\n",
    "        (r5_metrics, \"Llama-3.1-70B\"),\n",
    "        (r6_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Few-Shot Risk Assessment)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (r4_metrics, \"Mixtral-8x7B\"),\n",
    "        (r5_metrics, \"Llama-3.1-70B\"),\n",
    "        (r6_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Few-Shot)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_risk_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (r4_cm, \"R4: Mixtral-8x7B\"),\n",
    "        (r5_cm, \"R5: Llama-3.1-70B\"),\n",
    "        (r6_cm, \"R6: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Greens\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Few-Shot Risk Assessment\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"few_shot_risk_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrices saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a39b34",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b12f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r4_df.to_csv(f\"r4_mixtral_8x7b_few_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r4_mixtral_8x7b_few_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "r5_df.to_csv(f\"r5_llama_3_1_70b_few_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r5_llama_3_1_70b_few_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "r6_df.to_csv(f\"r6_finbert_few_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r6_finbert_few_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "metrics_df.to_csv(f\"few_shot_risk_metrics_summary_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì few_shot_risk_metrics_summary_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\n‚úì Visualizations saved:\")\n",
    "print(f\"  ‚Ä¢ few_shot_risk_performance_comparison.png\")\n",
    "print(f\"  ‚Ä¢ few_shot_risk_confusion_matrices.png\")\n",
    "\n",
    "print(f\"\\nüéâ All results saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c583263",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40062671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(df, model_name):\n",
    "    \"\"\"Analyze misclassification patterns\"\"\"\n",
    "    errors = df[df[\"true_sentiment\"] != df[\"predicted_sentiment\"]].copy()\n",
    "\n",
    "    print(f\"=== Error Analysis for {model_name} ===\\n\")\n",
    "    print(\n",
    "        f\"Total errors: {len(errors)}/{len(df)} ({len(errors) / len(df) * 100:.2f}%)\\n\"\n",
    "    )\n",
    "\n",
    "    # Misclassification patterns\n",
    "    print(\"Misclassification patterns:\")\n",
    "    confusion_pairs = (\n",
    "        errors.groupby([\"true_sentiment\", \"predicted_sentiment\"])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    for (true_label, pred_label), count in confusion_pairs.items():\n",
    "        print(\n",
    "            f\"  {true_label} ‚Üí {pred_label}: {count} ({count / len(errors) * 100:.1f}% of errors)\"\n",
    "        )\n",
    "\n",
    "    # High-confidence errors (confidence > 0.7)\n",
    "    if \"confidence\" in df.columns:\n",
    "        high_conf_errors = errors[errors[\"confidence\"] > 0.7]\n",
    "        print(f\"\\nHigh-confidence errors (conf > 0.7): {len(high_conf_errors)}\")\n",
    "        if len(high_conf_errors) > 0:\n",
    "            print(\"\\nSample high-confidence errors:\")\n",
    "            for idx in high_conf_errors.head(3).index:\n",
    "                row = df.loc[idx]\n",
    "                print(\n",
    "                    f\"  True: {row['true_sentiment']}, Pred: {row['predicted_sentiment']} (conf: {row['confidence']:.3f})\"\n",
    "                )\n",
    "                print(f\"  Text: {row['sentence'][:100]}...\")\n",
    "                print()\n",
    "\n",
    "    # Sample errors by type\n",
    "    print(\"\\nSample misclassifications:\")\n",
    "    for (true_label, pred_label), _ in confusion_pairs.head(3).items():\n",
    "        sample = errors[\n",
    "            (errors[\"true_sentiment\"] == true_label)\n",
    "            & (errors[\"predicted_sentiment\"] == pred_label)\n",
    "        ].iloc[0]\n",
    "        print(f\"  {true_label} ‚Üí {pred_label}:\")\n",
    "        print(f\"  {sample['sentence'][:120]}...\")\n",
    "        print()\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# Analyze errors for each model\n",
    "print(\"=== R4: Mixtral-8x7B-Instruct Few-Shot ===\")\n",
    "r4_errors = analyze_errors(r4_valid, \"R4: Mixtral-8x7B\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"=== R5: Llama-3.1-70B-Versatile Few-Shot ===\")\n",
    "r5_errors = analyze_errors(r5_valid, \"R5: Llama-3.1-70B\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"=== R6: FinBERT Few-Shot ===\")\n",
    "r6_errors = analyze_errors(r6_valid, \"R6: FinBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa01804",
   "metadata": {},
   "source": [
    "## 10. Confidence Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (valid_df, model_name) in enumerate(\n",
    "    [\n",
    "        (r4_valid, \"R4: Mixtral-8x7B\"),\n",
    "        (r5_valid, \"R5: Llama-3.1-70B\"),\n",
    "        (r6_valid, \"R6: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    if \"confidence\" in valid_df.columns:\n",
    "        correct = valid_df[\"true_sentiment\"] == valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "        axes[idx].hist(\n",
    "            valid_df[correct][\"confidence\"],\n",
    "            bins=20,\n",
    "            alpha=0.5,\n",
    "            label=\"Correct\",\n",
    "            color=\"green\",\n",
    "        )\n",
    "        axes[idx].hist(\n",
    "            valid_df[~correct][\"confidence\"],\n",
    "            bins=20,\n",
    "            alpha=0.5,\n",
    "            label=\"Incorrect\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "        axes[idx].set_xlabel(\"Confidence Score\")\n",
    "        axes[idx].set_ylabel(\"Frequency\")\n",
    "        axes[idx].set_title(f\"{model_name}\\nConfidence Distribution\")\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4601b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence_calibration(df, model_name):\n",
    "    \"\"\"Analyze confidence calibration\"\"\"\n",
    "\n",
    "    if \"confidence\" not in df.columns:\n",
    "        print(f\"No confidence scores for {model_name}\")\n",
    "        return\n",
    "\n",
    "    # Overall calibration\n",
    "    correct = df[\"true_sentiment\"] == df[\"predicted_sentiment\"]\n",
    "    avg_conf_correct = df[correct][\"confidence\"].mean()\n",
    "    avg_conf_incorrect = df[~correct][\"confidence\"].mean()\n",
    "    calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "    print(f\"=== Confidence Calibration: {model_name} ===\\n\")\n",
    "    print(f\"Average confidence when CORRECT: {avg_conf_correct:.3f}\")\n",
    "    print(f\"Average confidence when INCORRECT: {avg_conf_incorrect:.3f}\")\n",
    "    print(f\"Calibration gap: {calibration_gap:.3f}\")\n",
    "    print(\n",
    "        f\"  ‚Üí {'Well-calibrated' if calibration_gap > 0.15 else 'Poorly calibrated'}\\n\"\n",
    "    )\n",
    "\n",
    "    # Per-class confidence\n",
    "    print(\"Per-class average confidence:\")\n",
    "    for label in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        class_mask = df[\"predicted_sentiment\"] == label\n",
    "        if class_mask.sum() > 0:\n",
    "            avg_conf = df[class_mask][\"confidence\"].mean()\n",
    "            accuracy = (df[class_mask][\"true_sentiment\"] == label).mean()\n",
    "            print(f\"  {label}: {avg_conf:.3f} (accuracy: {accuracy:.3f})\")\n",
    "\n",
    "\n",
    "# Analyze confidence for each model\n",
    "analyze_confidence_calibration(r4_valid, \"R4: Mixtral-8x7B\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "analyze_confidence_calibration(r5_valid, \"R5: Llama-3.1-70B\")\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "analyze_confidence_calibration(r6_valid, \"R6: FinBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b91636",
   "metadata": {},
   "source": [
    "## 11. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-class metrics summary table\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def create_metrics_table(y_true, y_pred, model_name):\n",
    "    \"\"\"Create a summary table of per-class metrics\"\"\"\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"]\n",
    "    )\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": [\"positive\", \"negative\", \"neutral\"],\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1,\n",
    "            \"Support\": support,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_name} - Per-Class Metrics Summary:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "r4_class_metrics = create_metrics_table(\n",
    "    r4_valid[\"true_sentiment\"], r4_valid[\"predicted_sentiment\"], \"R4: Mixtral-8x7B\"\n",
    ")\n",
    "r5_class_metrics = create_metrics_table(\n",
    "    r5_valid[\"true_sentiment\"], r5_valid[\"predicted_sentiment\"], \"R5: Llama-3.1-70B\"\n",
    ")\n",
    "r6_class_metrics = create_metrics_table(\n",
    "    r6_valid[\"true_sentiment\"], r6_valid[\"predicted_sentiment\"], \"R6: FinBERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"R4: Mixtral-8x7B-Instruct Few-Shot Classification Report\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    classification_report(\n",
    "        r4_valid[\"true_sentiment\"],\n",
    "        r4_valid[\"predicted_sentiment\"],\n",
    "        target_names=[\"negative\", \"neutral\", \"positive\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R5: Llama-3.1-70B-Versatile Few-Shot Classification Report\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    classification_report(\n",
    "        r5_valid[\"true_sentiment\"],\n",
    "        r5_valid[\"predicted_sentiment\"],\n",
    "        target_names=[\"negative\", \"neutral\", \"positive\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R6: FinBERT Few-Shot Classification Report\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    classification_report(\n",
    "        r6_valid[\"true_sentiment\"],\n",
    "        r6_valid[\"predicted_sentiment\"],\n",
    "        target_names=[\"negative\", \"neutral\", \"positive\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcd121",
   "metadata": {},
   "source": [
    "## 12. Few-Shot vs Zero-Shot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88568ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zero-shot metrics summary for comparison\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Try to find zero-shot metrics summary file\n",
    "    zero_shot_dir = \"../Zero_Shot/\"\n",
    "    if os.path.exists(zero_shot_dir):\n",
    "        summary_files = [\n",
    "            f\n",
    "            for f in os.listdir(zero_shot_dir)\n",
    "            if \"metrics_summary\" in f and f.endswith(\".csv\")\n",
    "        ]\n",
    "        if summary_files:\n",
    "            zero_shot_summary_path = os.path.join(zero_shot_dir, summary_files[0])\n",
    "            print(f\"Loading zero-shot metrics from: {zero_shot_summary_path}\")\n",
    "\n",
    "            zero_shot_metrics_df = pd.read_csv(zero_shot_summary_path)\n",
    "            few_shot_metrics_df = metrics_df.copy()\n",
    "\n",
    "            # Compare metrics\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"Few-Shot vs Zero-Shot Performance Comparison\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            comparison_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"Model\": [\"Mixtral-8x7B\", \"Llama-3.1-70B\", \"FinBERT\"],\n",
    "                    \"Zero-Shot Accuracy\": zero_shot_metrics_df[\"Accuracy\"].values,\n",
    "                    \"Few-Shot Accuracy\": few_shot_metrics_df[\"Accuracy\"].values,\n",
    "                    \"Accuracy Improvement\": few_shot_metrics_df[\"Accuracy\"].values\n",
    "                    - zero_shot_metrics_df[\"Accuracy\"].values,\n",
    "                    \"Zero-Shot F1 (Macro)\": zero_shot_metrics_df[\"Macro-F1\"].values,\n",
    "                    \"Few-Shot F1 (Macro)\": few_shot_metrics_df[\"Macro-F1\"].values,\n",
    "                    \"F1 Improvement\": few_shot_metrics_df[\"Macro-F1\"].values\n",
    "                    - zero_shot_metrics_df[\"Macro-F1\"].values,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(comparison_df.to_string(index=False))\n",
    "\n",
    "            # Visualize improvement\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "            x = np.arange(len(comparison_df))\n",
    "            width = 0.35\n",
    "\n",
    "            # Accuracy comparison\n",
    "            axes[0].bar(\n",
    "                x - width / 2,\n",
    "                comparison_df[\"Zero-Shot Accuracy\"],\n",
    "                width,\n",
    "                label=\"Zero-Shot\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            axes[0].bar(\n",
    "                x + width / 2,\n",
    "                comparison_df[\"Few-Shot Accuracy\"],\n",
    "                width,\n",
    "                label=\"Few-Shot\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            axes[0].set_xlabel(\"Model\")\n",
    "            axes[0].set_ylabel(\"Accuracy\")\n",
    "            axes[0].set_title(\"Accuracy: Few-Shot vs Zero-Shot\")\n",
    "            axes[0].set_xticks(x)\n",
    "            axes[0].set_xticklabels(comparison_df[\"Model\"], rotation=15, ha=\"right\")\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "            # F1 comparison\n",
    "            axes[1].bar(\n",
    "                x - width / 2,\n",
    "                comparison_df[\"Zero-Shot F1 (Macro)\"],\n",
    "                width,\n",
    "                label=\"Zero-Shot\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            axes[1].bar(\n",
    "                x + width / 2,\n",
    "                comparison_df[\"Few-Shot F1 (Macro)\"],\n",
    "                width,\n",
    "                label=\"Few-Shot\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            axes[1].set_xlabel(\"Model\")\n",
    "            axes[1].set_ylabel(\"F1 Score (Macro)\")\n",
    "            axes[1].set_title(\"F1 Score: Few-Shot vs Zero-Shot\")\n",
    "            axes[1].set_xticks(x)\n",
    "            axes[1].set_xticklabels(comparison_df[\"Model\"], rotation=15, ha=\"right\")\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Key insights from comparison\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"Key Insights from Few-Shot Learning:\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            best_improvement = comparison_df.loc[\n",
    "                comparison_df[\"Accuracy Improvement\"].idxmax()\n",
    "            ]\n",
    "            print(\n",
    "                f\"1. Largest accuracy improvement: {best_improvement['Model']} (+{best_improvement['Accuracy Improvement']:.3f})\"\n",
    "            )\n",
    "\n",
    "            avg_improvement = comparison_df[\"Accuracy Improvement\"].mean()\n",
    "            print(f\"2. Average accuracy improvement: +{avg_improvement:.3f}\")\n",
    "\n",
    "            if (comparison_df[\"Accuracy Improvement\"] > 0).all():\n",
    "                print(\"3. All models benefited from few-shot examples\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"3. {(comparison_df['Accuracy Improvement'] > 0).sum()}/3 models improved with few-shot learning\"\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"4. F1 Score improvements range from {comparison_df['F1 Improvement'].min():.3f} to {comparison_df['F1 Improvement'].max():.3f}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(\"No zero-shot metrics summary file found - skipping comparison\")\n",
    "    else:\n",
    "        print(f\"Zero-shot directory not found: {zero_shot_dir} - skipping comparison\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load zero-shot metrics for comparison: {e}\")\n",
    "    print(\"Skipping few-shot vs zero-shot comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a0506",
   "metadata": {},
   "source": [
    "## 13. Expected Conclusions from Few-Shot Risk Assessment Experiment\n",
    "\n",
    "### Comprehensive Analysis of Few-Shot Learning for Financial Risk Assessment\n",
    "\n",
    "**1. Few-Shot Learning Impact**\n",
    "- Few-shot examples provide concrete demonstrations of risk assessment framing\n",
    "- Models learn to differentiate between investor sentiment and financial risk signals\n",
    "- 5 curated examples (2 positive, 2 negative, 1 neutral) establish clear classification patterns\n",
    "- Examples include both the label and reasoning, improving model understanding\n",
    "\n",
    "**2. Model Performance Comparison**\n",
    "- **R4 (Mixtral-8x7B)**: Likely shows moderate improvement over zero-shot, balances speed and accuracy\n",
    "- **R5 (Llama-3.1-70B)**: Expected to leverage examples effectively with strong pattern recognition\n",
    "- **R6 (FinBERT)**: Domain-specific pretraining + few-shot examples should yield highest accuracy\n",
    "- MCC scores indicate robustness across imbalanced classes\n",
    "\n",
    "**3. Example Quality Effect**\n",
    "- Carefully selected examples cover diverse financial scenarios (earnings, forecasts, operational changes)\n",
    "- Each example demonstrates risk assessment reasoning (why a statement indicates positive/negative/neutral risk)\n",
    "- Balance in example distribution (2-2-1) reflects realistic financial text class distribution\n",
    "- JSON format examples provide structured, parseable guidance to models\n",
    "\n",
    "**4. Risk vs Sentiment Framing**\n",
    "- Few-shot examples explicitly frame task as \"financial risk assessment\" not sentiment analysis\n",
    "- Examples teach models to evaluate: probability of loss, business uncertainty, operational threats\n",
    "- Positive risk = opportunity/growth signals; Negative risk = threats/losses; Neutral = informational\n",
    "- This framing differs from Task1 (Sentiment), requiring risk-specific reasoning\n",
    "\n",
    "**5. Negative Class Detection**\n",
    "- Few-shot examples likely improve negative (risk/threat) class detection\n",
    "- Examples demonstrate subtle risk indicators: \"challenging,\" \"may not,\" \"forecast cut\"\n",
    "- Models learn to distinguish between negative sentiment and actual business risk\n",
    "- Expected reduction in false positives for negative class compared to zero-shot\n",
    "\n",
    "**6. Neutral Class Handling**\n",
    "- Neutral examples show informational statements without risk implications\n",
    "- Models learn that factual updates ‚â† risk assessment\n",
    "- Neutral class precision expected to improve with clear examples\n",
    "- Reduces misclassification of informational content as positive/negative risk\n",
    "\n",
    "**7. Confidence Calibration**\n",
    "- Few-shot learning should improve confidence calibration (larger gap between correct/incorrect predictions)\n",
    "- Models express higher confidence when predictions align with example patterns\n",
    "- Expected calibration gap > 0.15 indicates well-calibrated predictions\n",
    "- Per-class confidence analysis reveals which risk categories models handle confidently\n",
    "\n",
    "**8. Error Analysis Insights**\n",
    "- Common misclassifications: Neutral ‚Üí Negative (overpredicting risk in informational content)\n",
    "- High-confidence errors indicate edge cases not covered by 5 examples\n",
    "- Confusion between positive risk (opportunity) and neutral (factual growth statements)\n",
    "- Few-shot reduces errors but doesn't eliminate ambiguous cases\n",
    "\n",
    "**9. Few-Shot vs Zero-Shot Improvement**\n",
    "- Expected accuracy improvement: +3-8% across models\n",
    "- Largest gains in F1-macro due to better minority class handling\n",
    "- FinBERT (R6) may show smaller improvement (already strong domain knowledge)\n",
    "- LLMs (R4, R5) expected to benefit more from explicit examples\n",
    "\n",
    "**10. Model-Specific Behaviors**\n",
    "- **Mixtral-8x7B**: Fast inference, moderate gains from examples, good balance\n",
    "- **Llama-3.1-70B**: Strong few-shot learning capability, may achieve highest improvement\n",
    "- **FinBERT**: Domain expertise + examples = robust performance, especially on financial jargon\n",
    "\n",
    "**11. Production Readiness**\n",
    "- Few-shot approach requires maintaining 5 high-quality curated examples\n",
    "- Examples must be updated if risk assessment framing or domain shifts\n",
    "- Trade-off: Better accuracy vs increased prompt length and API costs\n",
    "- Suitable for production when example maintenance is feasible\n",
    "\n",
    "**12. Recommendations for Deployment**\n",
    "- **Use Few-Shot when**: Need to establish specific risk assessment framing, have resources for example curation\n",
    "- **Consider Zero-Shot when**: Speed/cost critical, examples difficult to maintain, domain stable\n",
    "- **Next steps**: Chain-of-Thought (R7-R9) for explainability, Tree-of-Thought (R10-R12) for complex reasoning\n",
    "- Monitor performance drift; update examples if financial language/risk patterns evolve\n",
    "- Combine with confidence thresholds (e.g., manual review for conf < 0.7) for critical applications"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
