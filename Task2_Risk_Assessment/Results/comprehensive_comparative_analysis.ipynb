{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc18b24",
   "metadata": {},
   "source": [
    "# Comprehensive Comparative Analysis: Prompt Engineering for Financial Risk Assessment\n",
    "\n",
    "**Research Question**: How do different prompt engineering strategies and LLM models perform on financial risk assessment tasks?\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook provides deep statistical analysis across all 12 risk assessment experiments:\n",
    "- **4 Prompting Strategies**: Zero-Shot, Few-Shot, Chain-of-Thought (CoT), Tree-of-Thought (ToT)\n",
    "- **3 Models**: Gemini 2.0 Flash, Gemini 1.5 Flash, Llama-3.3-70B\n",
    "- **Dataset**: FinancialPhraseBank Sentences_AllAgree.txt (2,217 samples)\n",
    "\n",
    "## Analysis Framework\n",
    "\n",
    "1. **Strategy Effectiveness**: Does prompting complexity improve risk assessment accuracy?\n",
    "2. **Model Comparison**: Which model performs best across different strategies?\n",
    "3. **Cost-Benefit Analysis**: Trade-offs between performance and complexity\n",
    "4. **Statistical Significance**: Are performance differences statistically meaningful?\n",
    "5. **Production Recommendations**: Best approach for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00db01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn scipy statsmodels plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb312398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, f_oneway, friedmanchisquare\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "print(\"âœ“ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808f993",
   "metadata": {},
   "source": [
    "## 1. Load All Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63347e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment metadata\n",
    "experiments = {\n",
    "    \"R1\": {\n",
    "        \"model\": \"Gemini 2.0 Flash\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"pattern\": \"../Zero_Shot/r1_GPT_OSS_20B_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"R2\": {\n",
    "        \"model\": \"Gemini 1.5 Flash\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"pattern\": \"../Zero_Shot/r2_GPT_OSS_120B_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"R3\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Zero-Shot\",\n",
    "        \"pattern\": \"../Zero_Shot/r3_Llama_3.3_70B_zero_shot_*.csv\",\n",
    "    },\n",
    "    \"R4\": {\n",
    "        \"model\": \"Gemini 2.0 Flash\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"pattern\": \"../Few_Shot/r4_GPT_OSS_20B_few_shot_*.csv\",\n",
    "    },\n",
    "    \"R5\": {\n",
    "        \"model\": \"Gemini 1.5 Flash\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"pattern\": \"../Few_Shot/r5_GPT_OSS_120B_few_shot_*.csv\",\n",
    "    },\n",
    "    \"R6\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Few-Shot\",\n",
    "        \"pattern\": \"../Few_Shot/r6_Llama_3.3_70B_few_shot_*.csv\",\n",
    "    },\n",
    "    \"R7\": {\n",
    "        \"model\": \"Gemini 2.0 Flash\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"pattern\": \"../Chain_of_Thought/r7_GPT_OSS_20B_cot_*.csv\",\n",
    "    },\n",
    "    \"R8\": {\n",
    "        \"model\": \"Gemini 1.5 Flash\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"pattern\": \"../Chain_of_Thought/r8_GPT_OSS_120B_cot_*.csv\",\n",
    "    },\n",
    "    \"R9\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"pattern\": \"../Chain_of_Thought/r9_Llama-3.3-70B_cot_*.csv\",\n",
    "    },\n",
    "    \"R10\": {\n",
    "        \"model\": \"Gemini 2.0 Flash\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"pattern\": \"../Tree_of_Thought/r10_GPT_OSS_20B_tot_*.csv\",\n",
    "    },\n",
    "    \"R11\": {\n",
    "        \"model\": \"Gemini 1.5 Flash\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"pattern\": \"../Tree_of_Thought/r11_GPT_OSS_120B_flash_tot_*.csv\",\n",
    "    },\n",
    "    \"R12\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"pattern\": \"../Tree_of_Thought/r12_Llama_3.3_70B_tot_*.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Load results\n",
    "print(\"=\" * 100)\n",
    "print(\"ðŸ“ LOADING ALL EXPERIMENT DATA\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "all_data = {}\n",
    "for exp_id, meta in experiments.items():\n",
    "    files = sorted(glob(meta[\"pattern\"]))\n",
    "    if files:\n",
    "        latest_file = files[-1]\n",
    "        df = pd.read_csv(latest_file)\n",
    "        df[\"experiment\"] = exp_id\n",
    "        df[\"model\"] = meta[\"model\"]\n",
    "        df[\"strategy\"] = meta[\"strategy\"]\n",
    "        all_data[exp_id] = df\n",
    "        print(f\"âœ“ {exp_id}: {os.path.basename(latest_file)} ({len(df)} samples)\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  {exp_id}: No files found\")\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(all_data)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6c0d8",
   "metadata": {},
   "source": [
    "## 2. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    if valid_df.empty:\n",
    "        return None\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"total_samples\": len(df),\n",
    "        \"valid_predictions\": len(valid_df),\n",
    "        \"error_rate\": (len(df) - len(valid_df)) / len(df) if len(df) > 0 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "metrics_list = []\n",
    "for exp_id, df in all_data.items():\n",
    "    metrics = calculate_metrics(df)\n",
    "    if metrics:\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"experiment\": exp_id,\n",
    "                \"model\": experiments[exp_id][\"model\"],\n",
    "                \"strategy\": experiments[exp_id][\"strategy\"],\n",
    "            }\n",
    "        )\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ“Š PERFORMANCE METRICS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"experiment\", \"model\", \"strategy\", \"accuracy\", \"macro_f1\", \"mcc\", \"error_rate\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b1564",
   "metadata": {},
   "source": [
    "## 3. Research Question 1: Strategy Effectiveness\n",
    "\n",
    "**Does increasing prompt complexity improve risk assessment performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ”¬ STRATEGY EFFECTIVENESS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Group by strategy\n",
    "strategy_analysis = (\n",
    "    metrics_df.groupby(\"strategy\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"accuracy\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"macro_f1\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"mcc\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"error_rate\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance by Strategy:\")\n",
    "display(strategy_analysis)\n",
    "\n",
    "# Statistical test - ANOVA\n",
    "strategies = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "strategy_groups = {\n",
    "    s: metrics_df[metrics_df[\"strategy\"] == s][\"accuracy\"].values for s in strategies\n",
    "}\n",
    "\n",
    "f_stat, p_value = f_oneway(*[strategy_groups[s] for s in strategies])\n",
    "print(f\"\\nðŸ“ˆ ANOVA Test for Accuracy:\")\n",
    "print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(\n",
    "    f\"  Result: {'Statistically significant' if p_value < 0.05 else 'Not statistically significant'} (Î±=0.05)\"\n",
    ")\n",
    "\n",
    "# Post-hoc Tukey HSD test\n",
    "tukey_data = []\n",
    "for strategy in strategies:\n",
    "    for exp_id, row in metrics_df[metrics_df[\"strategy\"] == strategy].iterrows():\n",
    "        tukey_data.append({\"strategy\": strategy, \"accuracy\": row[\"accuracy\"]})\n",
    "\n",
    "tukey_df = pd.DataFrame(tukey_data)\n",
    "tukey_result = pairwise_tukeyhsd(tukey_df[\"accuracy\"], tukey_df[\"strategy\"], alpha=0.05)\n",
    "\n",
    "print(\"\\nðŸ“Š Tukey HSD Post-hoc Test:\")\n",
    "print(tukey_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439841a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize strategy progression\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = [\"accuracy\", \"macro_f1\", \"mcc\", \"error_rate\"]\n",
    "titles = [\"Accuracy\", \"Macro-F1\", \"Matthews Correlation Coefficient\", \"Error Rate\"]\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#9b59b6\", \"#e74c3c\"]\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    strategy_means = []\n",
    "    strategy_stds = []\n",
    "\n",
    "    for strategy in strategies:\n",
    "        values = metrics_df[metrics_df[\"strategy\"] == strategy][metric].values\n",
    "        strategy_means.append(np.mean(values))\n",
    "        strategy_stds.append(np.std(values))\n",
    "\n",
    "    x_pos = np.arange(len(strategies))\n",
    "    bars = ax.bar(\n",
    "        x_pos,\n",
    "        strategy_means,\n",
    "        yerr=strategy_stds,\n",
    "        capsize=10,\n",
    "        color=colors[idx],\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, mean, std) in enumerate(zip(bars, strategy_means, strategy_stds)):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + std + 0.01,\n",
    "            f\"{mean:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Prompting Strategy\", fontsize=12, weight=\"bold\")\n",
    "    ax.set_ylabel(title, fontsize=12, weight=\"bold\")\n",
    "    ax.set_title(f\"{title} Progression Across Strategies\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(strategies, rotation=45, ha=\"right\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"strategy_effectiveness_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Strategy effectiveness visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ab903",
   "metadata": {},
   "source": [
    "## 4. Research Question 2: Model Comparison\n",
    "\n",
    "**Which model performs best across different prompting strategies?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ¤– MODEL COMPARISON ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Group by model\n",
    "model_analysis = (\n",
    "    metrics_df.groupby(\"model\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"accuracy\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"macro_f1\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"mcc\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "            \"error_rate\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance by Model:\")\n",
    "display(model_analysis)\n",
    "\n",
    "# Two-way ANOVA: Strategy Ã— Model interaction\n",
    "anova_data = metrics_df[[\"strategy\", \"model\", \"accuracy\"]].copy()\n",
    "model = ols(\n",
    "    \"accuracy ~ C(strategy) + C(model) + C(strategy):C(model)\", data=anova_data\n",
    ").fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Two-Way ANOVA (Strategy Ã— Model):\")\n",
    "display(anova_table)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "if anova_table.loc[\"C(strategy)\", \"PR(>F)\"] < 0.05:\n",
    "    print(\"  âœ“ Strategy has a statistically significant effect on accuracy\")\n",
    "else:\n",
    "    print(\"  âœ— Strategy does not have a significant effect on accuracy\")\n",
    "\n",
    "if anova_table.loc[\"C(model)\", \"PR(>F)\"] < 0.05:\n",
    "    print(\"  âœ“ Model has a statistically significant effect on accuracy\")\n",
    "else:\n",
    "    print(\"  âœ— Model does not have a significant effect on accuracy\")\n",
    "\n",
    "if anova_table.loc[\"C(strategy):C(model)\", \"PR(>F)\"] < 0.05:\n",
    "    print(\"  âœ“ Significant interaction between strategy and model\")\n",
    "else:\n",
    "    print(\"  âœ— No significant interaction between strategy and model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b49f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "models = [\"Gemini 2.0 Flash\", \"Gemini 1.5 Flash\", \"Llama-3.3-70B\"]\n",
    "metrics_to_compare = [\"accuracy\", \"macro_f1\", \"mcc\"]\n",
    "metric_titles = [\"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_compare, metric_titles)):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Create pivot table\n",
    "    pivot_data = metrics_df.pivot_table(\n",
    "        values=metric, index=\"strategy\", columns=\"model\", aggfunc=\"mean\"\n",
    "    )\n",
    "    pivot_data = pivot_data.reindex(strategies)[models]\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot_data,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"RdYlGn\",\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": title},\n",
    "        vmin=pivot_data.min().min(),\n",
    "        vmax=pivot_data.max().max(),\n",
    "    )\n",
    "    ax.set_title(f\"{title}: Strategy Ã— Model\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    ax.set_ylabel(\"Strategy\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Model comparison heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d05bd",
   "metadata": {},
   "source": [
    "## 5. Research Question 3: Cost-Benefit Analysis\n",
    "\n",
    "**What are the trade-offs between performance gains and implementation complexity?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37892c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ’° COST-BENEFIT ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Define complexity scores (1-10 scale)\n",
    "complexity_scores = {\n",
    "    \"Zero-Shot\": 1,\n",
    "    \"Few-Shot\": 3,\n",
    "    \"Chain-of-Thought\": 6,\n",
    "    \"Tree-of-Thought\": 9,\n",
    "}\n",
    "\n",
    "# Calculate efficiency: Performance / Complexity\n",
    "metrics_df[\"complexity\"] = metrics_df[\"strategy\"].map(complexity_scores)\n",
    "metrics_df[\"efficiency_accuracy\"] = metrics_df[\"accuracy\"] / metrics_df[\"complexity\"]\n",
    "metrics_df[\"efficiency_f1\"] = metrics_df[\"macro_f1\"] / metrics_df[\"complexity\"]\n",
    "\n",
    "print(\"\\nEfficiency Rankings (Performance per Complexity Unit):\")\n",
    "print(\"\\nBy Accuracy Efficiency:\")\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"experiment\", \"strategy\", \"accuracy\", \"complexity\", \"efficiency_accuracy\"]\n",
    "    ]\n",
    "    .sort_values(\"efficiency_accuracy\", ascending=False)\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(\"\\nBy F1 Efficiency:\")\n",
    "display(\n",
    "    metrics_df[[\"experiment\", \"strategy\", \"macro_f1\", \"complexity\", \"efficiency_f1\"]]\n",
    "    .sort_values(\"efficiency_f1\", ascending=False)\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "# Performance gain analysis\n",
    "baseline_accuracy = metrics_df[metrics_df[\"strategy\"] == \"Zero-Shot\"][\"accuracy\"].mean()\n",
    "baseline_f1 = metrics_df[metrics_df[\"strategy\"] == \"Zero-Shot\"][\"macro_f1\"].mean()\n",
    "\n",
    "strategy_gains = []\n",
    "for strategy in strategies:\n",
    "    strategy_data = metrics_df[metrics_df[\"strategy\"] == strategy]\n",
    "    accuracy_gain = (\n",
    "        (strategy_data[\"accuracy\"].mean() - baseline_accuracy) / baseline_accuracy\n",
    "    ) * 100\n",
    "    f1_gain = ((strategy_data[\"macro_f1\"].mean() - baseline_f1) / baseline_f1) * 100\n",
    "    complexity = complexity_scores[strategy]\n",
    "\n",
    "    strategy_gains.append(\n",
    "        {\n",
    "            \"Strategy\": strategy,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Accuracy_Gain_%\": accuracy_gain,\n",
    "            \"F1_Gain_%\": f1_gain,\n",
    "            \"Efficiency_Score\": (accuracy_gain + f1_gain) / (2 * complexity),\n",
    "        }\n",
    "    )\n",
    "\n",
    "gains_df = pd.DataFrame(strategy_gains)\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Gains vs. Baseline (Zero-Shot):\")\n",
    "display(gains_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c541c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-benefit visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Performance vs. Complexity\n",
    "ax1 = axes[0]\n",
    "for model in models:\n",
    "    model_data = metrics_df[metrics_df[\"model\"] == model]\n",
    "    ax1.scatter(\n",
    "        model_data[\"complexity\"],\n",
    "        model_data[\"accuracy\"],\n",
    "        s=200,\n",
    "        alpha=0.7,\n",
    "        label=model,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Implementation Complexity (1-10)\", fontsize=12, weight=\"bold\")\n",
    "ax1.set_ylabel(\"Accuracy\", fontsize=12, weight=\"bold\")\n",
    "ax1.set_title(\"Performance vs. Complexity Trade-off\", fontsize=14, weight=\"bold\")\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Efficiency comparison\n",
    "ax2 = axes[1]\n",
    "strategy_efficiency = (\n",
    "    metrics_df.groupby(\"strategy\")[\"efficiency_accuracy\"].mean().reindex(strategies)\n",
    ")\n",
    "bars = ax2.bar(\n",
    "    range(len(strategies)),\n",
    "    strategy_efficiency,\n",
    "    color=[\"#3498db\", \"#2ecc71\", \"#f39c12\", \"#e74c3c\"],\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, strategy_efficiency)):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.005,\n",
    "        f\"{val:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel(\"Prompting Strategy\", fontsize=12, weight=\"bold\")\n",
    "ax2.set_ylabel(\"Efficiency Score (Accuracy / Complexity)\", fontsize=12, weight=\"bold\")\n",
    "ax2.set_title(\"Strategy Efficiency Comparison\", fontsize=14, weight=\"bold\")\n",
    "ax2.set_xticks(range(len(strategies)))\n",
    "ax2.set_xticklabels(strategies, rotation=45, ha=\"right\")\n",
    "ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cost_benefit_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Cost-benefit analysis visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52f700",
   "metadata": {},
   "source": [
    "## 6. Research Question 4: Statistical Significance\n",
    "\n",
    "**Are performance differences statistically meaningful or due to chance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ“Š STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Pairwise comparisons between strategies\n",
    "print(\"\\n1. Pairwise t-tests between consecutive strategies:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparisons = [\n",
    "    (\"Zero-Shot\", \"Few-Shot\"),\n",
    "    (\"Few-Shot\", \"Chain-of-Thought\"),\n",
    "    (\"Chain-of-Thought\", \"Tree-of-Thought\"),\n",
    "]\n",
    "\n",
    "for strategy1, strategy2 in comparisons:\n",
    "    acc1 = metrics_df[metrics_df[\"strategy\"] == strategy1][\"accuracy\"].values\n",
    "    acc2 = metrics_df[metrics_df[\"strategy\"] == strategy2][\"accuracy\"].values\n",
    "\n",
    "    t_stat, p_val = ttest_ind(acc1, acc2)\n",
    "\n",
    "    print(f\"\\n{strategy1} vs. {strategy2}:\")\n",
    "    print(f\"  Mean Accuracy: {acc1.mean():.4f} vs. {acc2.mean():.4f}\")\n",
    "    print(f\"  Difference: {(acc2.mean() - acc1.mean()):.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.4f}\")\n",
    "    print(f\"  Significant: {'Yes âœ“' if p_val < 0.05 else 'No âœ—'} (Î±=0.05)\")\n",
    "\n",
    "# Model comparisons\n",
    "print(\"\\n\\n2. Pairwise t-tests between models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_pairs = [\n",
    "    (\"Gemini 2.0 Flash\", \"Gemini 1.5 Flash\"),\n",
    "    (\"Gemini 1.5 Flash\", \"Llama-3.3-70B\"),\n",
    "    (\"Gemini 2.0 Flash\", \"Llama-3.3-70B\"),\n",
    "]\n",
    "\n",
    "for model1, model2 in model_pairs:\n",
    "    acc1 = metrics_df[metrics_df[\"model\"] == model1][\"accuracy\"].values\n",
    "    acc2 = metrics_df[metrics_df[\"model\"] == model2][\"accuracy\"].values\n",
    "\n",
    "    t_stat, p_val = ttest_ind(acc1, acc2)\n",
    "\n",
    "    print(f\"\\n{model1} vs. {model2}:\")\n",
    "    print(f\"  Mean Accuracy: {acc1.mean():.4f} vs. {acc2.mean():.4f}\")\n",
    "    print(f\"  Difference: {(acc2.mean() - acc1.mean()):.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.4f}\")\n",
    "    print(f\"  Significant: {'Yes âœ“' if p_val < 0.05 else 'No âœ—'} (Î±=0.05)\")\n",
    "\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "\n",
    "print(\"\\n\\n3. Effect sizes (Cohen's d):\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    \"Interpretation: |d| < 0.2 (small), 0.2-0.5 (medium), 0.5-0.8 (large), > 0.8 (very large)\"\n",
    ")\n",
    "\n",
    "for strategy1, strategy2 in comparisons:\n",
    "    acc1 = metrics_df[metrics_df[\"strategy\"] == strategy1][\"accuracy\"].values\n",
    "    acc2 = metrics_df[metrics_df[\"strategy\"] == strategy2][\"accuracy\"].values\n",
    "    d = cohens_d(acc2, acc1)\n",
    "\n",
    "    magnitude = (\n",
    "        \"small\"\n",
    "        if abs(d) < 0.2\n",
    "        else \"medium\"\n",
    "        if abs(d) < 0.5\n",
    "        else \"large\"\n",
    "        if abs(d) < 0.8\n",
    "        else \"very large\"\n",
    "    )\n",
    "    print(f\"\\n{strategy1} â†’ {strategy2}: d = {d:.4f} ({magnitude})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d63f41",
   "metadata": {},
   "source": [
    "## 7. Research Question 5: Production Deployment Recommendations\n",
    "\n",
    "**Which approach should be deployed in production systems?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5646f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸŽ¯ PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "\n",
    "# Multi-criteria scoring\n",
    "def calculate_production_score(row):\n",
    "    \"\"\"\n",
    "    Calculate production readiness score based on:\n",
    "    - Performance (40%): accuracy + macro_f1 + mcc\n",
    "    - Reliability (30%): 1 - error_rate\n",
    "    - Efficiency (30%): performance / complexity\n",
    "    \"\"\"\n",
    "    performance = (row[\"accuracy\"] + row[\"macro_f1\"] + row[\"mcc\"]) / 3\n",
    "    reliability = 1 - row[\"error_rate\"]\n",
    "    efficiency = row[\"efficiency_accuracy\"]\n",
    "\n",
    "    return (performance * 0.4) + (reliability * 0.3) + (efficiency * 0.3)\n",
    "\n",
    "\n",
    "metrics_df[\"production_score\"] = metrics_df.apply(calculate_production_score, axis=1)\n",
    "\n",
    "print(\"\\nðŸ“Š Production Readiness Rankings:\")\n",
    "rankings = metrics_df[\n",
    "    [\n",
    "        \"experiment\",\n",
    "        \"model\",\n",
    "        \"strategy\",\n",
    "        \"accuracy\",\n",
    "        \"macro_f1\",\n",
    "        \"mcc\",\n",
    "        \"error_rate\",\n",
    "        \"efficiency_accuracy\",\n",
    "        \"production_score\",\n",
    "    ]\n",
    "].sort_values(\"production_score\", ascending=False)\n",
    "display(rankings.round(4))\n",
    "\n",
    "# Top recommendations\n",
    "print(\"\\n\\nðŸ† TOP 3 RECOMMENDATIONS FOR PRODUCTION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (idx, row) in enumerate(rankings.head(3).iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['experiment']}: {row['model']} with {row['strategy']}\")\n",
    "    print(f\"   â€¢ Accuracy: {row['accuracy']:.4f}\")\n",
    "    print(f\"   â€¢ Macro-F1: {row['macro_f1']:.4f}\")\n",
    "    print(f\"   â€¢ MCC: {row['mcc']:.4f}\")\n",
    "    print(f\"   â€¢ Error Rate: {row['error_rate']:.4f}\")\n",
    "    print(f\"   â€¢ Efficiency: {row['efficiency_accuracy']:.4f}\")\n",
    "    print(f\"   â€¢ Production Score: {row['production_score']:.4f}\")\n",
    "\n",
    "# Decision matrix\n",
    "print(\"\\n\\nðŸ’¡ DEPLOYMENT DECISION MATRIX:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸš€ High Performance Required:\")\n",
    "best_perf = metrics_df.nlargest(1, \"accuracy\").iloc[0]\n",
    "print(\n",
    "    f\"   Recommended: {best_perf['experiment']} ({best_perf['model']}, {best_perf['strategy']})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Accuracy: {best_perf['accuracy']:.4f} | F1: {best_perf['macro_f1']:.4f} | MCC: {best_perf['mcc']:.4f}\"\n",
    ")\n",
    "\n",
    "print(\"\\nâš¡ Cost-Efficiency Priority:\")\n",
    "best_eff = metrics_df.nlargest(1, \"efficiency_accuracy\").iloc[0]\n",
    "print(\n",
    "    f\"   Recommended: {best_eff['experiment']} ({best_eff['model']}, {best_eff['strategy']})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Efficiency: {best_eff['efficiency_accuracy']:.4f} | Accuracy: {best_eff['accuracy']:.4f}\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Balanced Approach:\")\n",
    "best_balanced = metrics_df.nlargest(1, \"production_score\").iloc[0]\n",
    "print(\n",
    "    f\"   Recommended: {best_balanced['experiment']} ({best_balanced['model']}, {best_balanced['strategy']})\"\n",
    ")\n",
    "print(f\"   Production Score: {best_balanced['production_score']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”’ Reliability Priority (Lowest Error Rate):\")\n",
    "best_reliable = metrics_df.nsmallest(1, \"error_rate\").iloc[0]\n",
    "print(\n",
    "    f\"   Recommended: {best_reliable['experiment']} ({best_reliable['model']}, {best_reliable['strategy']})\"\n",
    ")\n",
    "print(\n",
    "    f\"   Error Rate: {best_reliable['error_rate']:.4f} | Accuracy: {best_reliable['accuracy']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production recommendation visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Production scores\n",
    "ax1 = axes[0, 0]\n",
    "sorted_df = metrics_df.sort_values(\"production_score\", ascending=False)\n",
    "colors_map = {\n",
    "    \"Zero-Shot\": \"#3498db\",\n",
    "    \"Few-Shot\": \"#2ecc71\",\n",
    "    \"Chain-of-Thought\": \"#f39c12\",\n",
    "    \"Tree-of-Thought\": \"#e74c3c\",\n",
    "}\n",
    "colors_list = [colors_map[s] for s in sorted_df[\"strategy\"]]\n",
    "\n",
    "bars = ax1.barh(\n",
    "    range(len(sorted_df)),\n",
    "    sorted_df[\"production_score\"],\n",
    "    color=colors_list,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "ax1.set_yticks(range(len(sorted_df)))\n",
    "ax1.set_yticklabels(sorted_df[\"experiment\"])\n",
    "ax1.set_xlabel(\"Production Readiness Score\", fontsize=12, weight=\"bold\")\n",
    "ax1.set_title(\"Production Readiness Rankings\", fontsize=14, weight=\"bold\")\n",
    "ax1.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Plot 2: Performance vs. Efficiency scatter\n",
    "ax2 = axes[0, 1]\n",
    "for strategy in strategies:\n",
    "    strategy_data = metrics_df[metrics_df[\"strategy\"] == strategy]\n",
    "    ax2.scatter(\n",
    "        strategy_data[\"efficiency_accuracy\"],\n",
    "        strategy_data[\"accuracy\"],\n",
    "        s=200,\n",
    "        alpha=0.7,\n",
    "        label=strategy,\n",
    "        color=colors_map[strategy],\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel(\"Efficiency (Accuracy/Complexity)\", fontsize=12, weight=\"bold\")\n",
    "ax2.set_ylabel(\"Accuracy\", fontsize=12, weight=\"bold\")\n",
    "ax2.set_title(\"Performance vs. Efficiency\", fontsize=14, weight=\"bold\")\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Multi-criteria radar chart for top 3\n",
    "ax3 = axes[1, 0]\n",
    "ax3.axis(\"off\")\n",
    "\n",
    "top3 = rankings.head(3)\n",
    "categories = [\"Accuracy\", \"Macro-F1\", \"MCC\", \"Reliability\", \"Efficiency\"]\n",
    "num_vars = len(categories)\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "ax_radar = plt.subplot(2, 2, 3, projection=\"polar\")\n",
    "\n",
    "for idx, (_, row) in enumerate(top3.iterrows()):\n",
    "    values = [\n",
    "        row[\"accuracy\"],\n",
    "        row[\"macro_f1\"],\n",
    "        row[\"mcc\"],\n",
    "        1 - row[\"error_rate\"],\n",
    "        row[\"efficiency_accuracy\"],\n",
    "    ]\n",
    "    values += values[:1]\n",
    "\n",
    "    ax_radar.plot(\n",
    "        angles,\n",
    "        values,\n",
    "        \"o-\",\n",
    "        linewidth=2,\n",
    "        label=f\"{row['experiment']}: {row['strategy']}\",\n",
    "        markersize=8,\n",
    "    )\n",
    "    ax_radar.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax_radar.set_xticks(angles[:-1])\n",
    "ax_radar.set_xticklabels(categories, fontsize=10)\n",
    "ax_radar.set_ylim(0, 1)\n",
    "ax_radar.set_title(\n",
    "    \"Top 3 Configurations: Multi-Criteria Comparison\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax_radar.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "ax_radar.grid(True)\n",
    "\n",
    "# Plot 4: Strategy-wise box plots\n",
    "ax4 = axes[1, 1]\n",
    "strategy_data = [\n",
    "    metrics_df[metrics_df[\"strategy\"] == s][\"accuracy\"].values for s in strategies\n",
    "]\n",
    "bp = ax4.boxplot(\n",
    "    strategy_data,\n",
    "    labels=strategies,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"#3498db\", alpha=0.7),\n",
    "    medianprops=dict(color=\"red\", linewidth=2),\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    ")\n",
    "\n",
    "ax4.set_xlabel(\"Prompting Strategy\", fontsize=12, weight=\"bold\")\n",
    "ax4.set_ylabel(\"Accuracy\", fontsize=12, weight=\"bold\")\n",
    "ax4.set_title(\"Accuracy Distribution by Strategy\", fontsize=14, weight=\"bold\")\n",
    "ax4.set_xticklabels(strategies, rotation=45, ha=\"right\")\n",
    "ax4.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"production_recommendations.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Production recommendations visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b14663",
   "metadata": {},
   "source": [
    "## 8. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸ“‹ KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Calculate key statistics\n",
    "best_overall = metrics_df.nlargest(1, \"accuracy\").iloc[0]\n",
    "most_efficient = metrics_df.nlargest(1, \"efficiency_accuracy\").iloc[0]\n",
    "best_production = metrics_df.nlargest(1, \"production_score\").iloc[0]\n",
    "\n",
    "strategy_improvement = (\n",
    "    (\n",
    "        metrics_df[metrics_df[\"strategy\"] == \"Tree-of-Thought\"][\"accuracy\"].mean()\n",
    "        - metrics_df[metrics_df[\"strategy\"] == \"Zero-Shot\"][\"accuracy\"].mean()\n",
    "    )\n",
    "    / metrics_df[metrics_df[\"strategy\"] == \"Zero-Shot\"][\"accuracy\"].mean()\n",
    ") * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "1. BEST OVERALL PERFORMER:\n",
    "   â€¢ Configuration: {best_overall[\"experiment\"]} ({best_overall[\"model\"]}, {best_overall[\"strategy\"]})\n",
    "   â€¢ Accuracy: {best_overall[\"accuracy\"]:.4f}\n",
    "   â€¢ Macro-F1: {best_overall[\"macro_f1\"]:.4f}\n",
    "   â€¢ MCC: {best_overall[\"mcc\"]:.4f}\n",
    "\n",
    "2. MOST EFFICIENT:\n",
    "   â€¢ Configuration: {most_efficient[\"experiment\"]} ({most_efficient[\"model\"]}, {most_efficient[\"strategy\"]})\n",
    "   â€¢ Efficiency Score: {most_efficient[\"efficiency_accuracy\"]:.4f}\n",
    "   â€¢ Accuracy: {most_efficient[\"accuracy\"]:.4f}\n",
    "\n",
    "3. PRODUCTION RECOMMENDED:\n",
    "   â€¢ Configuration: {best_production[\"experiment\"]} ({best_production[\"model\"]}, {best_production[\"strategy\"]})\n",
    "   â€¢ Production Score: {best_production[\"production_score\"]:.4f}\n",
    "   â€¢ Balanced across performance, reliability, and efficiency\n",
    "\n",
    "4. STRATEGY PROGRESSION:\n",
    "   â€¢ Zero-Shot â†’ Tree-of-Thought improvement: {strategy_improvement:.2f}%\n",
    "   â€¢ Statistical significance: {\"Confirmed\" if p_value < 0.05 else \"Not confirmed\"}\n",
    "\n",
    "5. MODEL PERFORMANCE:\n",
    "   â€¢ Best model overall: {metrics_df.groupby(\"model\")[\"accuracy\"].mean().idxmax()}\n",
    "   â€¢ Average accuracy: {metrics_df.groupby(\"model\")[\"accuracy\"].mean().max():.4f}\n",
    "\n",
    "6. IMPLEMENTATION RECOMMENDATIONS:\n",
    "   â€¢ For production deployment: {best_production[\"strategy\"]} with {best_production[\"model\"]}\n",
    "   â€¢ For research/experimentation: {best_overall[\"strategy\"]} with {best_overall[\"model\"]}\n",
    "   â€¢ For cost-sensitive applications: {most_efficient[\"strategy\"]} with {most_efficient[\"model\"]}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"âœ“ COMPREHENSIVE COMPARATIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55d6b8",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive analysis results\n",
    "output_file = \"comprehensive_comparative_analysis_results.csv\"\n",
    "metrics_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Comprehensive analysis results saved to: {output_file}\")\n",
    "print(f\"  Total experiments analyzed: {len(metrics_df)}\")\n",
    "print(f\"  Metrics computed: {len(metrics_df.columns)}\")\n",
    "print(f\"  Visualizations generated: 4 figures\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
