{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6aadf0b",
   "metadata": {},
   "source": [
    "# Comprehensive Results Comparison: All 12 Risk Assessment Experiments\n",
    "\n",
    "**Experiment Matrix**: 4 Approaches √ó 3 Models = 12 Experiments\n",
    "\n",
    "| Approach | Mistral-8x7B-32768 | Llama-3.1-70B-Versatile | Llama-3.3-70B |\n",
    "|----------|-------------------|------------------------|---------------|\n",
    "| **Zero-Shot** | R1 | R2 | R3 |\n",
    "| **Few-Shot** | R4 | R5 | R6 |\n",
    "| **Chain-of-Thought** | R7 | R8 | R9 |\n",
    "| **Tree-of-Thought** | R10 | R11 | R12 |\n",
    "\n",
    "**Dataset**: FinancialPhraseBank Sentences_AllAgree.txt (2,217 samples)\n",
    "\n",
    "This notebook provides a comprehensive comparison across all risk assessment experiments with automatic data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee94126",
   "metadata": {},
   "source": [
    "## 2. Calculate Comprehensive Metrics\n",
    "\n",
    "Calculate all evaluation metrics including MCC for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment metadata\n",
    "experiments = {\n",
    "    \"R1\": {\"model\": \"Mistral-8x7B\", \"strategy\": \"Zero-Shot\", \"dir\": \"Zero_Shot\"},\n",
    "    \"R2\": {\"model\": \"Llama-3.1-70B\", \"strategy\": \"Zero-Shot\", \"dir\": \"Zero_Shot\"},\n",
    "    \"R3\": {\"model\": \"Llama-3.3-70B\", \"strategy\": \"Zero-Shot\", \"dir\": \"Zero_Shot\"},\n",
    "    \"R4\": {\"model\": \"Mistral-8x7B\", \"strategy\": \"Few-Shot\", \"dir\": \"Few_Shot\"},\n",
    "    \"R5\": {\"model\": \"Llama-3.1-70B\", \"strategy\": \"Few-Shot\", \"dir\": \"Few_Shot\"},\n",
    "    \"R6\": {\"model\": \"Llama-3.3-70B\", \"strategy\": \"Few-Shot\", \"dir\": \"Few_Shot\"},\n",
    "    \"R7\": {\n",
    "        \"model\": \"Mistral-8x7B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"dir\": \"Chain_of_Thought\",\n",
    "    },\n",
    "    \"R8\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"dir\": \"Chain_of_Thought\",\n",
    "    },\n",
    "    \"R9\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Chain-of-Thought\",\n",
    "        \"dir\": \"Chain_of_Thought\",\n",
    "    },\n",
    "    \"R10\": {\n",
    "        \"model\": \"Mistral-8x7B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"dir\": \"Tree_of_Thought\",\n",
    "    },\n",
    "    \"R11\": {\n",
    "        \"model\": \"Llama-3.1-70B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"dir\": \"Tree_of_Thought\",\n",
    "    },\n",
    "    \"R12\": {\n",
    "        \"model\": \"Llama-3.3-70B\",\n",
    "        \"strategy\": \"Tree-of-Thought\",\n",
    "        \"dir\": \"Tree_of_Thought\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# File patterns for each experiment\n",
    "file_patterns = {\n",
    "    \"R1\": \"../Zero_Shot/r1_GPT_OSS_20B_zero_shot_*.csv\",\n",
    "    \"R2\": \"../Zero_Shot/r2_GPT_OSS_120B_zero_shot_*.csv\",\n",
    "    \"R3\": \"../Zero_Shot/r3_Llama_3.3_70B_zero_shot_*.csv\",\n",
    "    \"R4\": \"../Few_Shot/r4_GPT_OSS_20B_few_shot_*.csv\",\n",
    "    \"R5\": \"../Few_Shot/r5_GPT_OSS_120B_few_shot_*.csv\",\n",
    "    \"R6\": \"../Few_Shot/r6_Llama_3.3_70B_few_shot_*.csv\",\n",
    "    \"R7\": \"../Chain_of_Thought/r7_GPT_OSS_20B_cot_*.csv\",\n",
    "    \"R8\": \"../Chain_of_Thought/r8_GPT_OSS_120B_cot_*.csv\",\n",
    "    \"R9\": \"../Chain_of_Thought/r9_Llama-3.3-70B_cot_*.csv\",\n",
    "    \"R10\": \"../Tree_of_Thought/r10_GPT_OSS_20B_tot_*.csv\",\n",
    "    \"R11\": \"../Tree_of_Thought/r11_GPT_OSS_120B_flash_tot_*.csv\",\n",
    "    \"R12\": \"../Tree_of_Thought/r12_Llama_3.3_70B_tot_*.csv\",\n",
    "}\n",
    "\n",
    "# Load results\n",
    "print(\"=\" * 100)\n",
    "print(\"üìÅ LOADING EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_data = {}\n",
    "for exp_id, pattern in file_patterns.items():\n",
    "    files = sorted(glob(pattern))\n",
    "    if files:\n",
    "        latest_file = files[-1]\n",
    "        results_data[exp_id] = pd.read_csv(latest_file)\n",
    "        print(\n",
    "            f\"‚úì {exp_id}: {os.path.basename(latest_file)} ({len(results_data[exp_id])} samples)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {exp_id}: No files found for pattern {pattern}\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(results_data)} experiments successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a17393",
   "metadata": {},
   "source": [
    "## 1. Load All Results\n",
    "\n",
    "Load metrics summaries from all prompting strategy experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd69cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_id, model, strategy):\n",
    "    \"\"\"Calculate comprehensive metrics for an experiment\"\"\"\n",
    "\n",
    "    # Filter valid predictions\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    if valid_df.empty:\n",
    "        print(f\"‚ö†Ô∏è  {exp_id}: No valid predictions found\")\n",
    "        return None\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    macro_precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Parsing errors\n",
    "    parsing_errors = len(df) - len(valid_df)\n",
    "    error_rate = parsing_errors / len(df) if len(df) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Experiment\": exp_id,\n",
    "        \"Model\": model,\n",
    "        \"Strategy\": strategy,\n",
    "        \"Total_Samples\": len(df),\n",
    "        \"Valid_Predictions\": len(valid_df),\n",
    "        \"Parsing_Errors\": parsing_errors,\n",
    "        \"Error_Rate\": error_rate,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Macro-F1\": macro_f1,\n",
    "        \"Weighted-F1\": weighted_f1,\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"MCC\": mcc,\n",
    "        \"Positive-F1\": f1_per_class[0],\n",
    "        \"Positive-Precision\": precision_per_class[0],\n",
    "        \"Positive-Recall\": recall_per_class[0],\n",
    "        \"Negative-F1\": f1_per_class[1],\n",
    "        \"Negative-Precision\": precision_per_class[1],\n",
    "        \"Negative-Recall\": recall_per_class[1],\n",
    "        \"Neutral-F1\": f1_per_class[2],\n",
    "        \"Neutral-Precision\": precision_per_class[2],\n",
    "        \"Neutral-Recall\": recall_per_class[2],\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate metrics for all loaded experiments\n",
    "all_metrics = []\n",
    "for exp_id, df in results_data.items():\n",
    "    exp_info = experiments[exp_id]\n",
    "    metrics = calculate_metrics(df, exp_id, exp_info[\"model\"], exp_info[\"strategy\"])\n",
    "    if metrics:\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä CALCULATING METRICS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nProcessing {len(results_data)} experiments...\\n\")\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Display summary\n",
    "print(\"‚úì Metrics calculated for all experiments\")\n",
    "print(f\"\\nTotal experiments: {len(metrics_df)}\")\n",
    "print(f\"Strategies: {metrics_df['Strategy'].nunique()}\")\n",
    "print(f\"Models: {metrics_df['Model'].nunique()}\")\n",
    "\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive metrics\n",
    "output_file = \"risk_assessment_comprehensive_metrics.csv\"\n",
    "metrics_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Comprehensive metrics saved to: {output_file}\")\n",
    "print(f\"  Total experiments: {len(metrics_df)}\")\n",
    "print(f\"  Columns: {len(metrics_df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"‚úì COMPREHENSIVE RESULTS COMPARISON COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea904592",
   "metadata": {},
   "source": [
    "## 7. Export Summary\n",
    "\n",
    "Save comprehensive results to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üèÜ BEST PERFORMERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n1. Highest Accuracy:\")\n",
    "best_acc = metrics_df.nlargest(3, \"Accuracy\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "]\n",
    "display(best_acc.round(4))\n",
    "\n",
    "print(\"\\n2. Highest Macro-F1:\")\n",
    "best_f1 = metrics_df.nlargest(3, \"Macro-F1\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "]\n",
    "display(best_f1.round(4))\n",
    "\n",
    "print(\"\\n3. Highest MCC:\")\n",
    "best_mcc = metrics_df.nlargest(3, \"MCC\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "]\n",
    "display(best_mcc.round(4))\n",
    "\n",
    "print(\"\\n4. Lowest Error Rate:\")\n",
    "best_error = metrics_df.nsmallest(3, \"Error_Rate\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Error_Rate\", \"Valid_Predictions\"]\n",
    "]\n",
    "display(best_error.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c410cf3",
   "metadata": {},
   "source": [
    "## 6. Best Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(\n",
    "    x - width,\n",
    "    metrics_df[\"Positive-F1\"],\n",
    "    width,\n",
    "    label=\"Positive\",\n",
    "    alpha=0.8,\n",
    "    color=\"#2ecc71\",\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x, metrics_df[\"Negative-F1\"], width, label=\"Negative\", alpha=0.8, color=\"#e74c3c\"\n",
    ")\n",
    "bars3 = ax.bar(\n",
    "    x + width,\n",
    "    metrics_df[\"Neutral-F1\"],\n",
    "    width,\n",
    "    label=\"Neutral\",\n",
    "    alpha=0.8,\n",
    "    color=\"#95a5a6\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Experiment\", fontsize=12, weight=\"bold\")\n",
    "ax.set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Per-Class F1 Scores Across All Risk Assessment Experiments\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df[\"Experiment\"], rotation=0)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"risk_assessment_per_class_f1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Per-class F1 chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4fa5e",
   "metadata": {},
   "source": [
    "## 5. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d94b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    model_data = metrics_df.groupby(\"Model\")[metric].mean().reindex(models)\n",
    "    bars = ax.bar(\n",
    "        range(len(models)),\n",
    "        model_data,\n",
    "        color=colors[idx],\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, model_data)):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Model\", fontsize=12, weight=\"bold\")\n",
    "    ax.set_ylabel(metric, fontsize=12, weight=\"bold\")\n",
    "    ax.set_title(f\"Average {metric} by Model\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim([0, 1 if metric != \"MCC\" else max(model_data) * 1.1])\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"risk_assessment_model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Model comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    strategy_data = metrics_df.groupby(\"Strategy\")[metric].mean().reindex(strategies)\n",
    "    bars = ax.bar(\n",
    "        range(len(strategies)),\n",
    "        strategy_data,\n",
    "        color=colors[idx],\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, strategy_data)):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Prompting Strategy\", fontsize=12, weight=\"bold\")\n",
    "    ax.set_ylabel(metric, fontsize=12, weight=\"bold\")\n",
    "    ax.set_title(f\"Average {metric} by Strategy\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xticks(range(len(strategies)))\n",
    "    ax.set_xticklabels(strategies, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim([0, 1 if metric != \"MCC\" else max(strategy_data) * 1.1])\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"risk_assessment_strategy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Strategy comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Prepare data for heatmaps\n",
    "strategies = [\"Zero-Shot\", \"Few-Shot\", \"Chain-of-Thought\", \"Tree-of-Thought\"]\n",
    "models = [\"Gemini 2.0 Flash\", \"Gemini 1.5 Flash\", \"Llama-3.3-70B\"]\n",
    "\n",
    "for idx, metric in enumerate([\"Accuracy\", \"Macro-F1\", \"MCC\", \"Error_Rate\"]):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    # Create pivot table\n",
    "    pivot_data = metrics_df.pivot_table(\n",
    "        values=metric, index=\"Strategy\", columns=\"Model\", aggfunc=\"mean\"\n",
    "    )\n",
    "    pivot_data = pivot_data.reindex(strategies)[models]\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot_data,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"RdYlGn\" if metric != \"Error_Rate\" else \"RdYlGn_r\",\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": metric},\n",
    "        vmin=0,\n",
    "        vmax=1 if metric != \"MCC\" else None,\n",
    "    )\n",
    "    ax.set_title(f\"{metric} by Strategy and Model\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    ax.set_ylabel(\"Strategy\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"risk_assessment_performance_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance heatmap saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0c974",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc12f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\n",
    "            \"Experiment\",\n",
    "            \"Model\",\n",
    "            \"Strategy\",\n",
    "            \"Valid_Predictions\",\n",
    "            \"Accuracy\",\n",
    "            \"Macro-F1\",\n",
    "            \"Weighted-F1\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"MCC\",\n",
    "        ]\n",
    "    ].round(4)\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä SUMMARY STATISTICS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nBy Strategy:\")\n",
    "strategy_summary = metrics_df.groupby(\"Strategy\")[[\"Accuracy\", \"Macro-F1\", \"MCC\"]].agg(\n",
    "    [\"mean\", \"std\", \"min\", \"max\"]\n",
    ")\n",
    "display(strategy_summary.round(4))\n",
    "\n",
    "print(\"\\nBy Model:\")\n",
    "model_summary = metrics_df.groupby(\"Model\")[[\"Accuracy\", \"Macro-F1\", \"MCC\"]].agg(\n",
    "    [\"mean\", \"std\", \"min\", \"max\"]\n",
    ")\n",
    "display(model_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb3036",
   "metadata": {},
   "source": [
    "## 3. Overall Performance Comparison"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
