{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73be17",
   "metadata": {},
   "source": [
    "## 1. Load All Results\n",
    "\n",
    "Load metrics summaries from all prompting strategy experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92557838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe (manual entry for demonstration)\n",
    "# In practice, load from saved CSV files\n",
    "\n",
    "all_results = [\n",
    "    # Zero-Shot Results\n",
    "    {\n",
    "        \"Experiment\": \"E1\",\n",
    "        \"Model\": \"Gemini Pro\",\n",
    "        \"Strategy\": \"Zero-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E2\",\n",
    "        \"Model\": \"Gemini Flash\",\n",
    "        \"Strategy\": \"Zero-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E3\",\n",
    "        \"Model\": \"Llama-3.3-70B\",\n",
    "        \"Strategy\": \"Zero-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    # Few-Shot Results\n",
    "    {\n",
    "        \"Experiment\": \"E4\",\n",
    "        \"Model\": \"Gemini Pro\",\n",
    "        \"Strategy\": \"Few-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E5\",\n",
    "        \"Model\": \"Gemini Flash\",\n",
    "        \"Strategy\": \"Few-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E6\",\n",
    "        \"Model\": \"Llama-3.3-70B\",\n",
    "        \"Strategy\": \"Few-Shot\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    # Chain-of-Thought Results\n",
    "    {\n",
    "        \"Experiment\": \"E7\",\n",
    "        \"Model\": \"Gemini Pro\",\n",
    "        \"Strategy\": \"Chain-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E8\",\n",
    "        \"Model\": \"Gemini Flash\",\n",
    "        \"Strategy\": \"Chain-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E9\",\n",
    "        \"Model\": \"Llama-3.3-70B\",\n",
    "        \"Strategy\": \"Chain-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    # Tree-of-Thought Results\n",
    "    {\n",
    "        \"Experiment\": \"E10\",\n",
    "        \"Model\": \"Gemini Pro\",\n",
    "        \"Strategy\": \"Tree-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E10b\",\n",
    "        \"Model\": \"Gemini Flash\",\n",
    "        \"Strategy\": \"Tree-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"E10c\",\n",
    "        \"Model\": \"Llama-3.3-70B\",\n",
    "        \"Strategy\": \"Tree-of-Thought\",\n",
    "        \"Accuracy\": 0.0,\n",
    "        \"Macro-F1\": 0.0,\n",
    "        \"Precision\": 0.0,\n",
    "        \"Recall\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nNote: Run individual experiment notebooks first to populate actual results.\")\n",
    "print(\"This notebook demonstrates the analysis framework.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfae4a",
   "metadata": {},
   "source": [
    "## 2. Strategy-wise Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9080a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by strategy\n",
    "strategy_summary = results_df.groupby(\"Strategy\")[\n",
    "    [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\"]\n",
    "].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE PERFORMANCE BY PROMPTING STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "display(strategy_summary.round(4))\n",
    "\n",
    "# Visualize strategy comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "strategy_summary.plot(kind=\"bar\", ax=ax, width=0.8, alpha=0.8)\n",
    "ax.set_xlabel(\"Prompting Strategy\", fontsize=13, weight=\"bold\")\n",
    "ax.set_ylabel(\"Score\", fontsize=13, weight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Performance Comparison Across Prompting Strategies\",\n",
    "    fontsize=15,\n",
    "    weight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax.set_xticklabels(strategy_summary.index, rotation=45, ha=\"right\")\n",
    "ax.legend(title=\"Metrics\", fontsize=11)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"strategy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810571a",
   "metadata": {},
   "source": [
    "## 3. Model-wise Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74301217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model\n",
    "model_summary = results_df.groupby(\"Model\")[\n",
    "    [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\"]\n",
    "].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE PERFORMANCE BY MODEL\")\n",
    "print(\"=\" * 80)\n",
    "display(model_summary.round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_summary.plot(\n",
    "    kind=\"bar\", ax=ax, width=0.7, alpha=0.8, color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    ")\n",
    "ax.set_xlabel(\"Model\", fontsize=13, weight=\"bold\")\n",
    "ax.set_ylabel(\"Score\", fontsize=13, weight=\"bold\")\n",
    "ax.set_title(\"Performance Comparison Across Models\", fontsize=15, weight=\"bold\", pad=20)\n",
    "ax.set_xticklabels(model_summary.index, rotation=45, ha=\"right\")\n",
    "ax.legend(title=\"Metrics\", fontsize=11)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d152cc8",
   "metadata": {},
   "source": [
    "## 4. Heatmap: All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "pivot_accuracy = results_df.pivot(index=\"Model\", columns=\"Strategy\", values=\"Accuracy\")\n",
    "pivot_f1 = results_df.pivot(index=\"Model\", columns=\"Strategy\", values=\"Macro-F1\")\n",
    "\n",
    "# Visualize as heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Accuracy heatmap\n",
    "sns.heatmap(\n",
    "    pivot_accuracy,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    cbar_kws={\"label\": \"Accuracy\"},\n",
    "    ax=axes[0],\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "axes[0].set_title(\"Accuracy by Model and Strategy\", fontsize=13, weight=\"bold\", pad=15)\n",
    "axes[0].set_xlabel(\"Prompting Strategy\", fontsize=11, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Model\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "# F1 heatmap\n",
    "sns.heatmap(\n",
    "    pivot_f1,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    cbar_kws={\"label\": \"Macro-F1\"},\n",
    "    ax=axes[1],\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "axes[1].set_title(\"Macro-F1 by Model and Strategy\", fontsize=13, weight=\"bold\", pad=15)\n",
    "axes[1].set_xlabel(\"Prompting Strategy\", fontsize=11, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"Model\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"performance_heatmaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50d0e9",
   "metadata": {},
   "source": [
    "## 5. Best Performing Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0842ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 5 configurations by different metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 5 CONFIGURATIONS BY ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "top_accuracy = results_df.nlargest(5, \"Accuracy\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\"]\n",
    "]\n",
    "display(top_accuracy)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 5 CONFIGURATIONS BY MACRO-F1\")\n",
    "print(\"=\" * 80)\n",
    "top_f1 = results_df.nlargest(5, \"Macro-F1\")[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Accuracy\", \"Macro-F1\"]\n",
    "]\n",
    "display(top_f1)\n",
    "\n",
    "# Best overall configuration\n",
    "best_overall = results_df.loc[results_df[\"Macro-F1\"].idxmax()]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST OVERALL CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Experiment: {best_overall['Experiment']}\")\n",
    "print(f\"Model: {best_overall['Model']}\")\n",
    "print(f\"Strategy: {best_overall['Strategy']}\")\n",
    "print(f\"Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "print(f\"Macro-F1: {best_overall['Macro-F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637bb0c",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement from baseline (zero-shot)\n",
    "baseline = results_df[results_df[\"Strategy\"] == \"Zero-Shot\"][\"Accuracy\"].mean()\n",
    "\n",
    "improvements = []\n",
    "for strategy in results_df[\"Strategy\"].unique():\n",
    "    if strategy != \"Zero-Shot\":\n",
    "        strategy_mean = results_df[results_df[\"Strategy\"] == strategy][\n",
    "            \"Accuracy\"\n",
    "        ].mean()\n",
    "        improvement = ((strategy_mean - baseline) / baseline) * 100\n",
    "        improvements.append(\n",
    "            {\n",
    "                \"Strategy\": strategy,\n",
    "                \"Mean Accuracy\": strategy_mean,\n",
    "                \"% Improvement over Zero-Shot\": improvement,\n",
    "            }\n",
    "        )\n",
    "\n",
    "improvement_df = pd.DataFrame(improvements)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPROVEMENT OVER ZERO-SHOT BASELINE\")\n",
    "print(\"=\" * 80)\n",
    "display(improvement_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a5ab8",
   "metadata": {},
   "source": [
    "## 7. Radar Chart: Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Create radar chart for strategy comparison\n",
    "categories = [\"Accuracy\", \"Macro-F1\", \"Precision\", \"Recall\"]\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection=\"polar\"))\n",
    "\n",
    "for strategy in results_df[\"Strategy\"].unique():\n",
    "    strategy_data = (\n",
    "        results_df[results_df[\"Strategy\"] == strategy][categories]\n",
    "        .mean()\n",
    "        .values.tolist()\n",
    "    )\n",
    "    strategy_data += strategy_data[:1]\n",
    "    ax.plot(angles, strategy_data, \"o-\", linewidth=2, label=strategy)\n",
    "    ax.fill(angles, strategy_data, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(\n",
    "    \"Multi-Metric Comparison of Prompting Strategies\", size=15, weight=\"bold\", pad=20\n",
    ")\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"radar_chart_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d98dd",
   "metadata": {},
   "source": [
    "## 8. Cost-Performance Analysis\n",
    "\n",
    "Estimate relative computational costs and compare with performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relative cost estimates (tokens, API calls, time)\n",
    "cost_mapping = {\n",
    "    \"Zero-Shot\": 1.0,  # Baseline\n",
    "    \"Few-Shot\": 1.5,  # 50% more tokens for examples\n",
    "    \"Chain-of-Thought\": 2.0,  # 2x for reasoning\n",
    "    \"Tree-of-Thought\": 3.0,  # 3x for multi-path exploration\n",
    "}\n",
    "\n",
    "results_df[\"Relative_Cost\"] = results_df[\"Strategy\"].map(cost_mapping)\n",
    "results_df[\"Cost_Efficiency\"] = results_df[\"Macro-F1\"] / results_df[\"Relative_Cost\"]\n",
    "\n",
    "# Plot cost vs performance\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for model in results_df[\"Model\"].unique():\n",
    "    model_data = results_df[results_df[\"Model\"] == model]\n",
    "    ax.scatter(\n",
    "        model_data[\"Relative_Cost\"],\n",
    "        model_data[\"Macro-F1\"],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "    # Add experiment labels\n",
    "    for _, row in model_data.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"Experiment\"],\n",
    "            (row[\"Relative_Cost\"], row[\"Macro-F1\"]),\n",
    "            fontsize=9,\n",
    "            ha=\"right\",\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Relative Computational Cost\", fontsize=13, weight=\"bold\")\n",
    "ax.set_ylabel(\"Macro-F1 Score\", fontsize=13, weight=\"bold\")\n",
    "ax.set_title(\"Cost-Performance Trade-off Analysis\", fontsize=15, weight=\"bold\", pad=20)\n",
    "ax.legend(title=\"Model\", fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cost_performance_tradeoff.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Cost efficiency ranking\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COST EFFICIENCY RANKING (F1 / Cost)\")\n",
    "print(\"=\" * 80)\n",
    "cost_ranked = results_df[\n",
    "    [\"Experiment\", \"Model\", \"Strategy\", \"Macro-F1\", \"Relative_Cost\", \"Cost_Efficiency\"]\n",
    "].sort_values(\"Cost_Efficiency\", ascending=False)\n",
    "display(cost_ranked.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b19da",
   "metadata": {},
   "source": [
    "## 9. Key Findings & Recommendations\n",
    "\n",
    "### Summary of Results:\n",
    "\n",
    "Based on the comprehensive analysis above:\n",
    "\n",
    "1. **Best Performing Strategy**: [To be determined after running experiments]\n",
    "2. **Best Model**: [To be determined after running experiments]\n",
    "3. **Best Overall Configuration**: [Experiment ID + details]\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "**For Production Deployment**:\n",
    "- If cost is not a constraint: Use [Best F1 configuration]\n",
    "- If cost-efficiency matters: Use [Best cost-efficiency configuration]\n",
    "- If speed is critical: Use [Fastest configuration with acceptable performance]\n",
    "\n",
    "**For Research**:\n",
    "- Advanced reasoning (CoT, ToT) shows [improvement %] over baseline\n",
    "- Few-shot learning provides good balance of performance and simplicity\n",
    "- Model size vs prompting strategy trade-offs identified\n",
    "\n",
    "### Next Steps:\n",
    "1. Run experiments on full dataset (not just sample)\n",
    "2. Perform statistical significance testing\n",
    "3. Conduct error analysis on misclassifications\n",
    "4. Test on different financial domains for generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430708d",
   "metadata": {},
   "source": [
    "## 10. Export Complete Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save complete results\n",
    "results_df.to_csv(f\"complete_results_comparison_{timestamp}.csv\", index=False)\n",
    "strategy_summary.to_csv(f\"strategy_summary_{timestamp}.csv\")\n",
    "model_summary.to_csv(f\"model_summary_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\n✓ All comparison results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - complete_results_comparison_{timestamp}.csv\")\n",
    "print(f\"  - strategy_summary_{timestamp}.csv\")\n",
    "print(f\"  - model_summary_{timestamp}.csv\")\n",
    "print(\"\\nVisualizations saved:\")\n",
    "print(\"  - strategy_comparison.png\")\n",
    "print(\"  - model_comparison.png\")\n",
    "print(\"  - performance_heatmaps.png\")\n",
    "print(\"  - radar_chart_comparison.png\")\n",
    "print(\"  - cost_performance_tradeoff.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
