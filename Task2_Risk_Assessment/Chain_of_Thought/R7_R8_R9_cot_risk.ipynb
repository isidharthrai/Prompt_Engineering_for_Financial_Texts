{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')\n",
    "\n",
    "\n",
    "# Fix SSL/TLS certificate verification for gRPC (required for Google Gemini API on macOS)\n",
    "os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''\n",
    "os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7a8c8",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68548254",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    \"../../FinancialPhraseBank_Analysis/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ad40e",
   "metadata": {},
   "source": [
    "## 2. Chain-of-Thought Prompt Design\n",
    "\n",
    "**Reasoning Structure**:\n",
    "- Step 1: Identify key financial metrics/events\n",
    "- Step 2: Analyze positive indicators\n",
    "- Step 3: Analyze negative indicators  \n",
    "- Step 4: Determine net impact on investor sentiment\n",
    "- Step 5: Classify sentiment with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Chain-of-Thought prompt that guides stepwise reasoning.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze the following financial statement step-by-step.\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "Think through this systematically:\n",
    "\n",
    "Step 1: Identify the key financial metrics, events, or indicators mentioned in the statement.\n",
    "Step 2: List any positive signals (growth, profit increases, expansions, etc.).\n",
    "Step 3: List any negative signals (losses, declines, challenges, etc.).\n",
    "Step 4: Evaluate the net impact on stock price from an investor's perspective.\n",
    "Step 5: Based on your analysis, classify the sentiment.\n",
    "\n",
    "Classification guidelines:\n",
    "- Positive: Clear good news for stock price\n",
    "- Negative: Clear bad news for stock price\n",
    "- Neutral: No clear impact or mixed signals\n",
    "\n",
    "IMPORTANT: Provide your final answer in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief summary of your step-by-step reasoning\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"Net sales increased by 18.5% to EUR 167.8 million.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_cot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6719e6",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\", temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=1000,  # More tokens for reasoning\n",
    "                ),\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON from potentially verbose CoT response\"\"\"\n",
    "    try:\n",
    "        # Look for JSON in the response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        elif \"{\" in response_text:\n",
    "            # Extract JSON object\n",
    "            start = response_text.find(\"{\")\n",
    "            end = response_text.rfind(\"}\") + 1\n",
    "            json_str = response_text[start:end]\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3e124",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample\n",
    "test_df = df.head(100).copy()\n",
    "\n",
    "\n",
    "def run_cot_experiment(test_df, model_func, model_name, exp_id):\n",
    "    \"\"\"Generic function to run CoT experiment\"\"\"\n",
    "    print(f\"Running {exp_id}: {model_name} (Chain-of-Thought)...\")\n",
    "    results = []\n",
    "\n",
    "    for idx, row in tqdm(\n",
    "        test_df.iterrows(), total=len(test_df), desc=f\"{exp_id} Progress\"\n",
    "    ):\n",
    "        prompt = create_cot_prompt(row[\"sentence\"])\n",
    "        response = model_func(prompt)\n",
    "\n",
    "        if response:\n",
    "            parsed = parse_response(response)\n",
    "            if parsed:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"sentence\": row[\"sentence\"],\n",
    "                        \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                        \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                        \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                        \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                        \"full_response\": response[:500],  # Store reasoning for analysis\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ {exp_id} completed: {len(results_df)} predictions\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run all three experiments\n",
    "e7_df = run_cot_experiment(\n",
    "    test_df, lambda p: call_gemini(p, \"gemini-2.0-flash-exp\"), \"Gemini Pro\", \"E7\"\n",
    ")\n",
    "e8_df = run_cot_experiment(\n",
    "    test_df, lambda p: call_gemini(p, \"gemini-2.0-flash-exp\"), \"Gemini Flash\", \"E8\"\n",
    ")\n",
    "e9_df = run_cot_experiment(test_df, call_llama, \"Llama-3.3-70B\", \"E9\")\n",
    "\n",
    "display(e7_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fd992",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "e7_metrics, e7_cm, e7_valid = calculate_metrics(e7_df, \"E7: Gemini Pro (CoT)\")\n",
    "e8_metrics, e8_cm, e8_valid = calculate_metrics(e8_df, \"E8: Gemini Flash (CoT)\")\n",
    "e9_metrics, e9_cm, e9_valid = calculate_metrics(e9_df, \"E9: Llama-3.3-70B (CoT)\")\n",
    "\n",
    "metrics_df = pd.DataFrame([e7_metrics, e8_metrics, e9_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "e7_metrics, e7_cm, e7_valid = calculate_metrics(e7_df, \"E7: Gemini Pro CoT\")\n",
    "e8_metrics, e8_cm, e8_valid = calculate_metrics(e8_df, \"E8: Gemini Flash CoT\")\n",
    "e9_metrics, e9_cm, e9_valid = calculate_metrics(e9_df, \"E9: Llama CoT\")\n",
    "\n",
    "# Comparison table\n",
    "metrics_df = pd.DataFrame([e7_metrics, e8_metrics, e9_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAIN-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "    ].round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da85e7",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e7_df.to_csv(f\"e7_gemini_pro_cot_{timestamp}.csv\", index=False)\n",
    "e8_df.to_csv(f\"e8_gemini_flash_cot_{timestamp}.csv\", index=False)\n",
    "e9_df.to_csv(f\"e9_llama_cot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"cot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Chain-of-Thought results saved\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}