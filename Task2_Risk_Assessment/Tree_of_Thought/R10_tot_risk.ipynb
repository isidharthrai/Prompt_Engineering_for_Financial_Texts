{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b799b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')\n",
    "\n",
    "\n",
    "# Fix SSL/TLS certificate verification for gRPC (required for Google Gemini API on macOS)\n",
    "os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''\n",
    "os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bfa95",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    \"../../FinancialPhraseBank_Analysis/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50693acd",
   "metadata": {},
   "source": [
    "## 2. Tree-of-Thought Prompt Design\n",
    "\n",
    "**Multi-Path Reasoning**:\n",
    "- Path 1: Consider \"positive\" hypothesis\n",
    "- Path 2: Consider \"negative\" hypothesis  \n",
    "- Path 3: Consider \"neutral\" hypothesis\n",
    "- Evaluation: Score each path's evidence strength\n",
    "- Selection: Choose the most supported hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a Tree-of-Thought prompt with multi-path exploration.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert. Analyze this statement using a tree-of-thought approach.\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "TASK: Explore three possible sentiment classifications and select the best one.\n",
    "\n",
    "---\n",
    "PATH 1: Hypothesis = POSITIVE\n",
    "Consider if this statement represents positive news for investors.\n",
    "- What evidence supports this being positive?\n",
    "- What evidence contradicts this being positive?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 2: Hypothesis = NEGATIVE\n",
    "Consider if this statement represents negative news for investors.\n",
    "- What evidence supports this being negative?\n",
    "- What evidence contradicts this being negative?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "PATH 3: Hypothesis = NEUTRAL\n",
    "Consider if this statement has no clear market impact.\n",
    "- What evidence supports this being neutral?\n",
    "- What evidence contradicts this being neutral?\n",
    "- Confidence score (0-1) for this hypothesis:\n",
    "\n",
    "---\n",
    "FINAL DECISION:\n",
    "Based on evaluating all three paths, select the hypothesis with the strongest evidence.\n",
    "\n",
    "Provide your final answer in this exact JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Explanation of why this hypothesis was selected over the others\",\n",
    "    \"path_scores\": {{\n",
    "        \"positive\": 0.0-1.0,\n",
    "        \"negative\": 0.0-1.0,\n",
    "        \"neutral\": 0.0-1.0\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = (\n",
    "    \"The company reported mixed results with revenue up 10% but margins declining.\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_tot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81294531",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\", temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=1500,  # More tokens for multi-path reasoning\n",
    "                ),\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=1500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON with path scores from ToT response\"\"\"\n",
    "    try:\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"{\" in response_text:\n",
    "            start = response_text.find(\"{\")\n",
    "            end = response_text.rfind(\"}\") + 1\n",
    "            json_str = response_text[start:end]\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed\",\n",
    "                \"path_scores\": {},\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eac435",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb95a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sample\n",
    "test_df = df.head(100).copy()\n",
    "\n",
    "\n",
    "def run_tot_experiment(test_df, model_func, model_name, exp_id):\n",
    "    print(f\"Running {exp_id}: {model_name} (Tree-of-Thought)...\")\n",
    "    results = []\n",
    "\n",
    "    for idx, row in tqdm(\n",
    "        test_df.iterrows(), total=len(test_df), desc=f\"{exp_id} Progress\"\n",
    "    ):\n",
    "        prompt = create_tot_prompt(row[\"sentence\"])\n",
    "        response = model_func(prompt)\n",
    "\n",
    "        if response:\n",
    "            parsed = parse_response(response)\n",
    "            if parsed:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"sentence\": row[\"sentence\"],\n",
    "                        \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                        \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                        \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                        \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                        \"path_scores\": str(parsed.get(\"path_scores\", {})),\n",
    "                        \"full_response\": response[:700],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        time.sleep(0.6)  # ToT requires more processing\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ {exp_id} completed: {len(results_df)} predictions\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run Tree-of-Thought experiments\n",
    "e10_df = run_tot_experiment(\n",
    "    test_df, lambda p: call_gemini(p, \"gemini-2.0-flash-exp\"), \"Gemini Pro\", \"E10\"\n",
    ")\n",
    "e10b_df = run_tot_experiment(\n",
    "    test_df, lambda p: call_gemini(p, \"gemini-2.0-flash-exp\"), \"Gemini Flash\", \"E10b\"\n",
    ")\n",
    "e10c_df = run_tot_experiment(test_df, call_llama, \"Llama-3.3-70B\", \"E10c\")\n",
    "\n",
    "display(e10_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27302649",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9519858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "e10_metrics, e10_cm, e10_valid = calculate_metrics(e10_df, \"E10: Gemini Pro (ToT)\")\n",
    "e10b_metrics, e10b_cm, e10b_valid = calculate_metrics(\n",
    "    e10b_df, \"E10b: Gemini Flash (ToT)\"\n",
    ")\n",
    "e10c_metrics, e10c_cm, e10c_valid = calculate_metrics(\n",
    "    e10c_df, \"E10c: Llama-3.3-70B (ToT)\"\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame([e10_metrics, e10b_metrics, e10c_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"⚠️ Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "e10_metrics, e10_cm, e10_valid = calculate_metrics(e10_df, \"E10: Gemini ToT\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TREE-OF-THOUGHT PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExperiment: E10\")\n",
    "print(f\"Accuracy: {e10_metrics['Accuracy']:.4f}\")\n",
    "print(f\"Macro-F1: {e10_metrics['Macro-F1']:.4f}\")\n",
    "print(f\"Macro-Precision: {e10_metrics['Macro-Precision']:.4f}\")\n",
    "print(f\"Macro-Recall: {e10_metrics['Macro-Recall']:.4f}\")\n",
    "print(f\"\\nPer-class F1 Scores:\")\n",
    "print(f\"  Positive: {e10_metrics['Positive_F1']:.4f}\")\n",
    "print(f\"  Negative: {e10_metrics['Negative_F1']:.4f}\")\n",
    "print(f\"  Neutral: {e10_metrics['Neutral_F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30d456",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd49c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e10_df.to_csv(f\"e10_gemini_pro_tot_{timestamp}.csv\", index=False)\n",
    "e10b_df.to_csv(f\"e10b_gemini_flash_tot_{timestamp}.csv\", index=False)\n",
    "e10c_df.to_csv(f\"e10c_llama_tot_{timestamp}.csv\", index=False)\n",
    "metrics_df.to_csv(f\"tot_metrics_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Tree-of-Thought results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec11747",
   "metadata": {},
   "source": [
    "## 7. Key Insights\n",
    "\n",
    "### Tree-of-Thought Analysis:\n",
    "\n",
    "1. **Multi-Path Reasoning**: How does exploring all three sentiment hypotheses affect accuracy?\n",
    "2. **Decision Quality**: Are ToT predictions more justified and explainable?\n",
    "3. **Computational Cost**: Does the added complexity justify performance gains?\n",
    "4. **Path Score Analysis**: Which sentiment hypotheses receive highest scores for which types of statements?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}