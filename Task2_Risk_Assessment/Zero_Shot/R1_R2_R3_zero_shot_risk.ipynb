{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# API setup\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Groq API (for Mixtral and Llama)\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "    print(\"‚úì Groq API configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: GROQ_API_KEY not found in environment variables\")\n",
    "    groq_client = None\n",
    "\n",
    "# Initialize FinBERT\n",
    "print(\"Loading FinBERT model (ProsusAI/finbert)...\")\n",
    "try:\n",
    "    finbert_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"ProsusAI/finbert\",\n",
    "        tokenizer=\"ProsusAI/finbert\",\n",
    "        device=-1,  # CPU\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"‚úì FinBERT model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  FinBERT loading failed: {e}\")\n",
    "    finbert_pipeline = None\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Models configured:\")\n",
    "print(\"  ‚Ä¢ R1: Mixtral-8x7B-32768 (Groq API)\")\n",
    "print(\"  ‚Ä¢ R2: Llama-3.1-70B-Versatile (Groq API)\")\n",
    "print(\"  ‚Ä¢ R3: FinBERT (ProsusAI/finbert - Local)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c68069",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset (highest quality)\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"true_sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"true_sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320df8cb",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Simple, direct instruction with no examples. Enforces strict JSON output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b12ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a zero-shot prompt for sentiment classification.\n",
    "    No examples provided - model relies on pretrained knowledge.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial sentiment analysis expert.\n",
    "\n",
    "Classify the sentiment of the following financial statement as either \"positive\", \"negative\", or \"neutral\" from an investor's perspective.\n",
    "\n",
    "Consider:\n",
    "- Positive: Good news for stock price (revenue increase, profit growth, etc.)\n",
    "- Negative: Bad news for stock price (losses, declining sales, etc.)\n",
    "- Neutral: No clear impact on stock price or mixed signals\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation in one sentence\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"ZERO-SHOT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_zero_shot_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b7e2e",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mixtral(prompt, temperature=0.0):\n",
    "    \"\"\"Call Mixtral-8x7B via Groq API\"\"\"\n",
    "    if not groq_client:\n",
    "        print(\"‚ö†Ô∏è  Groq client not initialized\")\n",
    "        return None\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"mixtral-8x7b-32768\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)  # Exponential backoff\n",
    "                continue\n",
    "            print(f\"Error calling Mixtral: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    \"\"\"Call Llama-3.1-70B via Groq API\"\"\"\n",
    "    if not groq_client:\n",
    "        print(\"‚ö†Ô∏è  Groq client not initialized\")\n",
    "        return None\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            print(f\"Error calling Llama: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_finbert(sentence):\n",
    "    \"\"\"Call FinBERT model for sentiment analysis\"\"\"\n",
    "    if not finbert_pipeline:\n",
    "        print(\"‚ö†Ô∏è  FinBERT pipeline not initialized\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result = finbert_pipeline(sentence)[0]\n",
    "\n",
    "        # Map FinBERT labels to our format\n",
    "        label_map = {\n",
    "            \"positive\": \"positive\",\n",
    "            \"negative\": \"negative\",\n",
    "            \"neutral\": \"neutral\",\n",
    "        }\n",
    "\n",
    "        sentiment = label_map.get(result[\"label\"].lower(), \"neutral\")\n",
    "        confidence = result[\"score\"]\n",
    "\n",
    "        # Create JSON response matching other models\n",
    "        response = {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"confidence\": confidence,\n",
    "            \"rationale\": f\"FinBERT classification with {confidence:.2%} confidence\",\n",
    "        }\n",
    "\n",
    "        return json.dumps(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with FinBERT: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        # Fallback: try to extract sentiment with regex\n",
    "        response_lower = response_text.lower()\n",
    "        if \"positive\" in response_lower and \"negative\" not in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"positive\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"negative\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"negative\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        elif \"neutral\" in response_lower:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úì Model inference functions defined\")\n",
    "print(\"  ‚Ä¢ call_mixtral() - Mixtral-8x7B-32768\")\n",
    "print(\"  ‚Ä¢ call_llama() - Llama-3.1-70B-Versatile\")\n",
    "print(\"  ‚Ä¢ call_finbert() - FinBERT (ProsusAI/finbert)\")\n",
    "print(\"  ‚Ä¢ parse_response() - JSON parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac076fa6",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "### R1: Mixtral-8x7B-32768 (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, use a sample of the dataset (remove .head(100) for full run)\n",
    "test_df = df.head(100).copy()  # Remove .head(100) for full dataset\n",
    "\n",
    "# R1: Mixtral-8x7B-32768\n",
    "print(\"=\" * 80)\n",
    "print(\"Running R1: Mixtral-8x7B-32768 (Zero-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "r1_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R1 Progress\"):\n",
    "    prompt = create_zero_shot_prompt(row[\"sentence\"])\n",
    "    response = call_mixtral(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r1_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r1_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r1_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"API call failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "r1_df = pd.DataFrame(r1_results)\n",
    "print(f\"\\n‚úì R1 completed: {len(r1_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r1_df[r1_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r1_df[r1_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r1_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da33bd",
   "metadata": {},
   "source": [
    "### R2: Llama-3.1-70B-Versatile (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2: Llama-3.1-70B-Versatile\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Running R2: Llama-3.1-70B-Versatile (Zero-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "r2_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R2 Progress\"):\n",
    "    prompt = create_zero_shot_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r2_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r2_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r2_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"API call failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "r2_df = pd.DataFrame(r2_results)\n",
    "print(f\"\\n‚úì R2 completed: {len(r2_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r2_df[r2_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r2_df[r2_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r2_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49f817",
   "metadata": {},
   "source": [
    "### R3: FinBERT (ProsusAI/finbert - Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R3: FinBERT\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Running R3: FinBERT (ProsusAI/finbert - Zero-Shot)\")\n",
    "print(\"=\" * 80)\n",
    "r3_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"R3 Progress\"):\n",
    "    # FinBERT doesn't need the full prompt - just the sentence\n",
    "    response = call_finbert(row[\"sentence\"])\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_response(response)\n",
    "        if parsed:\n",
    "            r3_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": parsed.get(\"sentiment\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            r3_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                    \"predicted_sentiment\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        r3_results.append(\n",
    "            {\n",
    "                \"sentence\": row[\"sentence\"],\n",
    "                \"true_sentiment\": row[\"true_sentiment\"],\n",
    "                \"predicted_sentiment\": \"error\",\n",
    "                \"confidence\": 0,\n",
    "                \"rationale\": \"Model inference failed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    time.sleep(0.1)  # Shorter delay for local model\n",
    "\n",
    "r3_df = pd.DataFrame(r3_results)\n",
    "print(f\"\\n‚úì R3 completed: {len(r3_df)} predictions\")\n",
    "print(\n",
    "    f\"  Valid predictions: {len(r3_df[r3_df['predicted_sentiment'].isin(['positive', 'negative', 'neutral'])])}\"\n",
    ")\n",
    "print(f\"  Errors: {len(r3_df[r3_df['predicted_sentiment'] == 'error'])}\")\n",
    "display(r3_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0e570",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "\n",
    "def calculate_metrics(df, exp_name):\n",
    "    \"\"\"Calculate all evaluation metrics including MCC\"\"\"\n",
    "    # Check if dataframe is empty or missing required columns\n",
    "    if df.empty or \"predicted_sentiment\" not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Warning: {exp_name} has no valid predictions!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": 0,\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    # Filter out errors\n",
    "    valid_df = df[\n",
    "        df[\"predicted_sentiment\"].isin([\"positive\", \"negative\", \"neutral\"])\n",
    "    ].copy()\n",
    "\n",
    "    # Check if we have valid predictions\n",
    "    if valid_df.empty:\n",
    "        print(f\"‚ö†Ô∏è Warning: {exp_name} has no valid predictions after filtering!\")\n",
    "        return (\n",
    "            {\n",
    "                \"Experiment\": exp_name,\n",
    "                \"Total Samples\": len(df),\n",
    "                \"Valid Predictions\": 0,\n",
    "                \"Accuracy\": 0,\n",
    "                \"Macro-F1\": 0,\n",
    "                \"Weighted-F1\": 0,\n",
    "                \"Macro-Precision\": 0,\n",
    "                \"Macro-Recall\": 0,\n",
    "                \"MCC\": 0,\n",
    "                \"Positive_Precision\": 0,\n",
    "                \"Positive_Recall\": 0,\n",
    "                \"Positive_F1\": 0,\n",
    "                \"Negative_Precision\": 0,\n",
    "                \"Negative_Recall\": 0,\n",
    "                \"Negative_F1\": 0,\n",
    "                \"Neutral_Precision\": 0,\n",
    "                \"Neutral_Recall\": 0,\n",
    "                \"Neutral_F1\": 0,\n",
    "            },\n",
    "            np.zeros((3, 3)),\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    y_true = valid_df[\"true_sentiment\"]\n",
    "    y_pred = valid_df[\"predicted_sentiment\"]\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Total Samples\": len(df),\n",
    "        \"Valid Predictions\": len(valid_df),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Macro-F1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Weighted-F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Macro-Precision\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"Macro-Recall\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"MCC\": mcc_score,\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    precision_per_class = precision_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    recall_per_class = recall_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    f1_per_class = f1_score(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"{label.capitalize()}_Precision\"] = precision_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_Recall\"] = recall_per_class[i]\n",
    "        metrics[f\"{label.capitalize()}_F1\"] = f1_per_class[i]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return metrics, cm, valid_df\n",
    "\n",
    "\n",
    "# Calculate metrics for all experiments\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CALCULATING METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r1_metrics, r1_cm, r1_valid = calculate_metrics(r1_df, \"R1: Mixtral-8x7B (Zero-Shot)\")\n",
    "r2_metrics, r2_cm, r2_valid = calculate_metrics(r2_df, \"R2: Llama-3.1-70B (Zero-Shot)\")\n",
    "r3_metrics, r3_cm, r3_valid = calculate_metrics(r3_df, \"R3: FinBERT (Zero-Shot)\")\n",
    "\n",
    "# Create comparison table\n",
    "metrics_df = pd.DataFrame([r1_metrics, r2_metrics, r3_metrics])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZERO-SHOT RISK ASSESSMENT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[\n",
    "        [\"Experiment\", \"Valid Predictions\", \"Accuracy\", \"Macro-F1\", \"MCC\"]\n",
    "    ].round(4)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED METRICS\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    metrics_df[[\"Experiment\", \"Macro-Precision\", \"Macro-Recall\", \"Weighted-F1\"]].round(\n",
    "        4\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df[[\"Experiment\", \"Positive_F1\", \"Negative_F1\", \"Neutral_F1\"]].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc0d5b",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = [\"Accuracy\", \"Macro-F1\", \"Macro-Precision\", \"Macro-Recall\"]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (r1_metrics, \"Mixtral-8x7B\"),\n",
    "        (r2_metrics, \"Llama-3.1-70B\"),\n",
    "        (r3_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel(\"Metrics\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_ylabel(\"Score\", fontsize=12, weight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Overall Performance Comparison (Zero-Shot Risk Assessment)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Per-class F1 scores\n",
    "classes = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "x2 = np.arange(len(classes))\n",
    "\n",
    "for i, (metrics, label) in enumerate(\n",
    "    [\n",
    "        (r1_metrics, \"Mixtral-8x7B\"),\n",
    "        (r2_metrics, \"Llama-3.1-70B\"),\n",
    "        (r3_metrics, \"FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    values = [metrics[f\"{c}_F1\"] for c in classes]\n",
    "    axes[1].bar(x2 + i * width, values, width, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Sentiment Class\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_ylabel(\"F1 Score\", fontsize=12, weight=\"bold\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores (Zero-Shot)\", fontsize=14, weight=\"bold\")\n",
    "axes[1].set_xticks(x2 + width)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_risk_performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Performance comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "for idx, (cm, title) in enumerate(\n",
    "    [\n",
    "        (r1_cm, \"R1: Mixtral-8x7B\"),\n",
    "        (r2_cm, \"R2: Llama-3.1-70B\"),\n",
    "        (r3_cm, \"R3: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Count\"},\n",
    "    )\n",
    "    axes[idx].set_title(title, fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_ylabel(\"True Label\", fontsize=11, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Predicted Label\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confusion Matrices - Zero-Shot Risk Assessment\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_risk_confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrices saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6827921",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aede83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "r1_df.to_csv(f\"r1_mixtral_8x7b_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r1_mixtral_8x7b_zero_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "r2_df.to_csv(f\"r2_llama_3_1_70b_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r2_llama_3_1_70b_zero_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "r3_df.to_csv(f\"r3_finbert_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì r3_finbert_zero_shot_risk_{timestamp}.csv\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_df.to_csv(f\"zero_shot_risk_metrics_summary_{timestamp}.csv\", index=False)\n",
    "print(f\"‚úì zero_shot_risk_metrics_summary_{timestamp}.csv\")\n",
    "\n",
    "print(f\"\\n‚úì Visualizations saved:\")\n",
    "print(f\"  ‚Ä¢ zero_shot_risk_performance_comparison.png\")\n",
    "print(f\"  ‚Ä¢ zero_shot_risk_confusion_matrices.png\")\n",
    "print(f\"  ‚Ä¢ zero_shot_risk_confidence_analysis.png\")\n",
    "\n",
    "print(f\"\\nüéâ All results saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf23103",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e260f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis: Most Common Misclassifications\n",
    "print(\"=\" * 80)\n",
    "print(\"ERROR ANALYSIS: TOP MISCLASSIFIED PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for df_result, exp_name in [\n",
    "    (r1_valid, \"R1: Mixtral-8x7B\"),\n",
    "    (r2_valid, \"R2: Llama-3.1-70B\"),\n",
    "    (r3_valid, \"R3: FinBERT\"),\n",
    "]:\n",
    "    if not df_result.empty:\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        errors = df_result[\n",
    "            df_result[\"true_sentiment\"] != df_result[\"predicted_sentiment\"]\n",
    "        ]\n",
    "\n",
    "        # Count confusion pairs\n",
    "        if not errors.empty:\n",
    "            confusion_pairs = errors.groupby(\n",
    "                [\"true_sentiment\", \"predicted_sentiment\"]\n",
    "            ).size()\n",
    "            print(f\"Total Errors: {len(errors)}\")\n",
    "            print(\"\\nMost Common Misclassifications:\")\n",
    "            for (true_label, pred_label), count in (\n",
    "                confusion_pairs.sort_values(ascending=False).head(5).items()\n",
    "            ):\n",
    "                print(f\"  {true_label} ‚Üí {pred_label}: {count} errors\")\n",
    "\n",
    "            # High-confidence errors\n",
    "            if \"confidence\" in df_result.columns:\n",
    "                high_conf_errors = errors[errors[\"confidence\"] > 0.7]\n",
    "                if not high_conf_errors.empty:\n",
    "                    print(\n",
    "                        f\"\\nHigh-Confidence Errors (confidence > 0.7): {len(high_conf_errors)}\"\n",
    "                    )\n",
    "                    print(\"Sample high-confidence misclassifications:\")\n",
    "                    for idx, row in high_conf_errors.head(2).iterrows():\n",
    "                        print(f\"\\n  Sentence: {row['sentence'][:100]}...\")\n",
    "                        print(\n",
    "                            f\"  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Confidence: {row['confidence']:.3f}\"\n",
    "                        )\n",
    "\n",
    "            # Show sample errors\n",
    "            print(f\"\\nSample Misclassified Sentences:\")\n",
    "            for idx, row in errors.head(3).iterrows():\n",
    "                print(f\"\\n  Sentence: {row['sentence'][:100]}...\")\n",
    "                print(\n",
    "                    f\"  True: {row['true_sentiment']} | Predicted: {row['predicted_sentiment']} | Confidence: {row.get('confidence', 0):.3f}\"\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  ‚úì No errors - perfect classification!\")\n",
    "    else:\n",
    "        print(f\"\\n{exp_name}: No valid predictions to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b3f2e",
   "metadata": {},
   "source": [
    "## 9. Confidence Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIDENCE CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, name) in enumerate(\n",
    "    [\n",
    "        (r1_valid, \"R1: Mixtral-8x7B\"),\n",
    "        (r2_valid, \"R2: Llama-3.1-70B\"),\n",
    "        (r3_valid, \"R3: FinBERT\"),\n",
    "    ]\n",
    "):\n",
    "    if not df.empty and \"confidence\" in df.columns:\n",
    "        # Confidence by correctness\n",
    "        df_copy = df.copy()\n",
    "        df_copy[\"correct\"] = df_copy[\"true_sentiment\"] == df_copy[\"predicted_sentiment\"]\n",
    "\n",
    "        correct_conf = df_copy[df_copy[\"correct\"]][\"confidence\"]\n",
    "        incorrect_conf = df_copy[~df_copy[\"correct\"]][\"confidence\"]\n",
    "\n",
    "        axes[idx].hist(\n",
    "            correct_conf,\n",
    "            bins=20,\n",
    "            alpha=0.6,\n",
    "            label=f\"Correct (n={len(correct_conf)})\",\n",
    "            color=\"green\",\n",
    "        )\n",
    "        axes[idx].hist(\n",
    "            incorrect_conf,\n",
    "            bins=20,\n",
    "            alpha=0.6,\n",
    "            label=f\"Incorrect (n={len(incorrect_conf)})\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "        axes[idx].set_xlabel(\"Confidence Score\", fontsize=11, weight=\"bold\")\n",
    "        axes[idx].set_ylabel(\"Frequency\", fontsize=11, weight=\"bold\")\n",
    "        axes[idx].set_title(name, fontsize=12, weight=\"bold\")\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Print calibration statistics\n",
    "        avg_conf_correct = correct_conf.mean() if len(correct_conf) > 0 else 0\n",
    "        avg_conf_incorrect = incorrect_conf.mean() if len(incorrect_conf) > 0 else 0\n",
    "        calibration_gap = avg_conf_correct - avg_conf_incorrect\n",
    "\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Correct predictions - Mean confidence: {avg_conf_correct:.3f}\")\n",
    "        print(f\"  Incorrect predictions - Mean confidence: {avg_conf_incorrect:.3f}\")\n",
    "        print(f\"  Calibration gap: {calibration_gap:.3f}\")\n",
    "\n",
    "        # Per-class confidence\n",
    "        print(f\"  Per-class average confidence:\")\n",
    "        for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "            class_df = df_copy[df_copy[\"predicted_sentiment\"] == sentiment]\n",
    "            if not class_df.empty:\n",
    "                print(\n",
    "                    f\"    {sentiment.capitalize()}: {class_df['confidence'].mean():.3f}\"\n",
    "                )\n",
    "    else:\n",
    "        axes[idx].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No data available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            color=\"red\",\n",
    "        )\n",
    "        axes[idx].set_title(name, fontsize=12, weight=\"bold\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Confidence Score Distribution - Zero-Shot Risk Assessment\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_risk_confidence_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Confidence analysis visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13929033",
   "metadata": {},
   "source": [
    "## 10. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79fe108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for exp_name, valid_df in [\n",
    "    (\"R1: Mixtral-8x7B (Zero-Shot)\", r1_valid),\n",
    "    (\"R2: Llama-3.1-70B (Zero-Shot)\", r2_valid),\n",
    "    (\"R3: FinBERT (Zero-Shot)\", r3_valid),\n",
    "]:\n",
    "    if not valid_df.empty:\n",
    "        print(f\"\\n{exp_name}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\n",
    "            classification_report(\n",
    "                valid_df[\"true_sentiment\"],\n",
    "                valid_df[\"predicted_sentiment\"],\n",
    "                labels=[\"positive\", \"negative\", \"neutral\"],\n",
    "                target_names=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "                zero_division=0,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\n{exp_name}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"‚ö†Ô∏è  No valid predictions to report\")\n",
    "\n",
    "# Per-class metrics summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "class_metrics_summary = metrics_df[\n",
    "    [\n",
    "        \"Experiment\",\n",
    "        \"Positive_Precision\",\n",
    "        \"Positive_Recall\",\n",
    "        \"Positive_F1\",\n",
    "        \"Negative_Precision\",\n",
    "        \"Negative_Recall\",\n",
    "        \"Negative_F1\",\n",
    "        \"Neutral_Precision\",\n",
    "        \"Neutral_Recall\",\n",
    "        \"Neutral_F1\",\n",
    "    ]\n",
    "]\n",
    "display(class_metrics_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d50f4",
   "metadata": {},
   "source": [
    "## 11. Expected Conclusions from Zero-Shot Risk Assessment\n",
    "\n",
    "### Zero-Shot Performance Analysis\n",
    "\n",
    "**1. Model Performance Ranking**\n",
    "- **Best Overall**: Identify which model achieves highest Macro-F1 and MCC scores\n",
    "- **Expected Leader**: FinBERT (R3) likely outperforms due to financial domain specialization\n",
    "- **LLM Comparison**: Llama-3.1-70B (R2) expected to beat Mixtral-8x7B (R1) due to larger parameter count\n",
    "- **Accuracy vs F1**: Check if rankings differ between accuracy and F1 (important for imbalanced classes)\n",
    "\n",
    "**2. Zero-Shot Baseline Establishment**\n",
    "- **Purpose**: This experiment establishes baseline performance without any examples\n",
    "- **Benchmark**: All future experiments (Few-Shot, CoT, ToT) should exceed this baseline\n",
    "- **Minimum Threshold**: Zero-Shot Macro-F1 should be > 0.60 for production viability\n",
    "- **Model Comparison**: Establishes which model architecture (LLM vs domain-specific) works best for financial sentiment\n",
    "\n",
    "**3. Class-Specific Performance (Critical for Risk Assessment)**\n",
    "\n",
    "   **Negative Class Detection** (Highest Priority):\n",
    "   - **Business Impact**: Missing negative sentiment = missing financial risks\n",
    "   - **Expected Challenge**: Negative class typically has lowest recall across all models\n",
    "   - **Risk Assessment Goal**: Negative_Recall > 0.70 is acceptable, > 0.80 is excellent\n",
    "   - **False Negatives**: Count how many true negative cases were missed (critical errors)\n",
    "\n",
    "   **Positive Class Detection**:\n",
    "   - **Expected Performance**: Highest F1 score (clearest signals: profit, growth, revenue increase)\n",
    "   - **Common Errors**: May confuse neutral statements with positive (overly optimistic bias)\n",
    "\n",
    "   **Neutral Class Detection**:\n",
    "   - **Expected Challenge**: Hardest to classify (ambiguous signals, mixed news)\n",
    "   - **Error Pattern**: Often misclassified as positive or negative\n",
    "   - **Acceptable F1**: Neutral_F1 > 0.50 is acceptable given inherent ambiguity\n",
    "\n",
    "**4. Matthews Correlation Coefficient (MCC) Analysis**\n",
    "- **Why MCC**: Better than accuracy for imbalanced classes, ranges from -1 to +1\n",
    "- **Interpretation**: \n",
    "  - MCC > 0.5 = Good performance\n",
    "  - MCC > 0.7 = Excellent performance\n",
    "  - MCC < 0.3 = Model barely better than random guessing\n",
    "- **Expected Ranking**: MCC should rank models similarly to Macro-F1\n",
    "- **Class Balance Check**: If MCC << Accuracy, model is biased toward majority class\n",
    "\n",
    "**5. Confidence Calibration Quality**\n",
    "- **Well-Calibrated Model**: Avg confidence for correct predictions >> avg confidence for incorrect predictions\n",
    "- **Calibration Gap**: Target gap > 0.10 (well-calibrated), > 0.20 (excellent)\n",
    "- **Overconfidence Risk**: If incorrect predictions have high confidence, model is dangerously overconfident\n",
    "- **Expected Behavior**:\n",
    "  - FinBERT: Best calibration (fine-tuned on financial data with proper probability outputs)\n",
    "  - LLMs: May be overconfident (trained to sound confident even when uncertain)\n",
    "\n",
    "**6. Error Pattern Analysis**\n",
    "\n",
    "   **Most Common Misclassifications** (Expected):\n",
    "   - **neutral ‚Üí negative**: Model interprets caution/uncertainty as bad news\n",
    "   - **neutral ‚Üí positive**: Model misses subtle negative signals in mixed statements\n",
    "   - **positive ‚Üí neutral**: Model underestimates positive impact\n",
    "   - **negative ‚Üí neutral**: Most dangerous - missing risk signals\n",
    "\n",
    "   **High-Confidence Errors**:\n",
    "   - When model is confident but wrong, indicates systematic misunderstanding\n",
    "   - Review these cases to understand model's fundamental limitations\n",
    "   - May require prompt engineering or few-shot examples to fix\n",
    "\n",
    "**7. Model-Specific Behaviors**\n",
    "\n",
    "   **R1: Mixtral-8x7B** (Groq API):\n",
    "   - **Architecture**: Mixture-of-Experts (8 specialists)\n",
    "   - **Expected Strength**: Fast inference, cost-effective, good general reasoning\n",
    "   - **Expected Weakness**: Limited financial domain knowledge, may miss subtle signals\n",
    "   - **Prediction**: Moderate performance (Macro-F1: 0.60-0.70)\n",
    "\n",
    "   **R2: Llama-3.1-70B-Versatile** (Groq API):\n",
    "   - **Architecture**: Dense 70B parameter model\n",
    "   - **Expected Strength**: Best general language understanding, complex reasoning\n",
    "   - **Expected Weakness**: No financial specialization, may interpret statements too literally\n",
    "   - **Prediction**: Best among LLMs (Macro-F1: 0.65-0.75)\n",
    "\n",
    "   **R3: FinBERT** (ProsusAI/finbert - Local):\n",
    "   - **Architecture**: BERT-base fine-tuned on 4.9M financial sentences\n",
    "   - **Expected Strength**: Deep financial domain knowledge, well-calibrated probabilities\n",
    "   - **Expected Weakness**: Cannot reason beyond training patterns, no context beyond 512 tokens\n",
    "   - **Prediction**: Best overall (Macro-F1: 0.75-0.85)\n",
    "\n",
    "**8. Zero-Shot vs Future Approaches**\n",
    "\n",
    "   **Expected Improvements**:\n",
    "   - **Few-Shot (R4-R6)**: +5-10% F1 improvement from providing curated examples\n",
    "   - **Chain-of-Thought (R7-R9)**: +3-7% F1 improvement from structured reasoning\n",
    "   - **Tree-of-Thought (R10-R12)**: +2-5% F1 improvement from multi-path hypothesis exploration\n",
    "\n",
    "   **When Zero-Shot is Sufficient**:\n",
    "   - If Macro-F1 > 0.75 and Negative_Recall > 0.75\n",
    "   - No computational budget for complex prompting\n",
    "   - Real-time inference requirements (latency-sensitive)\n",
    "\n",
    "**9. Production Deployment Considerations**\n",
    "\n",
    "   **Best for Accuracy** (regardless of cost):\n",
    "   - Model with highest Macro-F1 and MCC\n",
    "   - Priority: Minimize all misclassifications\n",
    "   - Use Case: High-stakes financial decisions, regulatory compliance\n",
    "\n",
    "   **Best for Risk Detection** (highest negative recall):\n",
    "   - Model with highest Negative_Recall score\n",
    "   - Priority: Never miss financial risks\n",
    "   - Use Case: Early warning systems, portfolio risk monitoring\n",
    "\n",
    "   **Best for Cost-Efficiency**:\n",
    "   - Acceptable F1 (> 0.65) at lowest inference cost\n",
    "   - Priority: Balance accuracy and operational cost\n",
    "   - Use Case: High-volume sentiment monitoring, preliminary screening\n",
    "\n",
    "   **Recommended Approach**:\n",
    "   - **Primary**: FinBERT (R3) for accuracy and speed\n",
    "   - **Backup**: Llama-3.1-70B (R2) for cases requiring reasoning\n",
    "   - **Cost-Optimized**: Mixtral-8x7B (R1) for high-volume batch processing\n",
    "\n",
    "**10. Limitations and Constraints**\n",
    "\n",
    "   **Zero-Shot Limitations**:\n",
    "   - No examples = model relies entirely on pretrained knowledge\n",
    "   - Cannot guide model toward specific interpretation patterns\n",
    "   - May misinterpret domain-specific terminology\n",
    "   - No control over output format consistency\n",
    "\n",
    "   **Data Limitations**:\n",
    "   - Sentences_AllAgree.txt = 100% annotator agreement (2,217 samples)\n",
    "   - High-quality but limited size\n",
    "   - May not represent full diversity of financial statements\n",
    "   - Balanced distribution (positive/negative/neutral) may not reflect real-world skew\n",
    "\n",
    "   **Task Framing**:\n",
    "   - \"Risk Assessment\" uses same sentiment labels as Task1\n",
    "   - Risk = negative sentiment, opportunity = positive sentiment\n",
    "   - Real financial risk assessment more complex (volatility, uncertainty, exposure)\n",
    "\n",
    "**11. Next Steps and Improvements**\n",
    "\n",
    "   **If Zero-Shot Performance is Good (Macro-F1 > 0.70)**:\n",
    "   - Skip to production deployment testing\n",
    "   - May not need Few-Shot or CoT approaches\n",
    "   - Focus on error analysis and edge case handling\n",
    "\n",
    "   **If Zero-Shot Performance is Weak (Macro-F1 < 0.65)**:\n",
    "   - Proceed to Few-Shot experiments (R4-R6) with curated examples\n",
    "   - Design examples specifically targeting weak classes (likely negative and neutral)\n",
    "   - Consider Chain-of-Thought for complex reasoning cases\n",
    "\n",
    "   **Prompt Engineering Opportunities**:\n",
    "   - Add specific financial terminology definitions\n",
    "   - Provide clearer distinction between neutral and negative\n",
    "   - Include examples of edge cases in system prompt\n",
    "   - Request confidence scores and reasoning (improve calibration)\n",
    "\n",
    "**12. Key Validation Questions**\n",
    "- ‚úì/‚úó Does any model achieve Macro-F1 > 0.70?\n",
    "- ‚úì/‚úó Does FinBERT outperform LLMs in zero-shot setting?\n",
    "- ‚úì/‚úó Is Negative_Recall > 0.65 for best model?\n",
    "- ‚úì/‚úó Is calibration gap > 0.10 for at least one model?\n",
    "- ‚úì/‚úó Are most errors neutral‚Üîpositive/negative (expected pattern)?\n",
    "- ‚úì/‚úó Does MCC ranking match Macro-F1 ranking?\n",
    "- ‚úì/‚úó Is zero-shot baseline sufficient for production deployment?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
