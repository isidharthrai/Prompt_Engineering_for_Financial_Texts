{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn google-generativeai groq python-dotenv tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress deprecation warnings\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "# Fix SSL/TLS certificate verification for gRPC (required for Google Gemini API on macOS)\n",
    "os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''\n",
    "os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import ssl\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Fix SSL certificate verification issue on macOS\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# API setup\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure APIs\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "if GROQ_API_KEY:\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(\"✓ SSL certificate verification disabled for macOS compatibility\")\n",
    "print(f\"✓ Google API configured: {bool(GOOGLE_API_KEY)}\")\n",
    "print(f\"✓ Groq API configured: {bool(GROQ_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f475085",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100% agreement dataset\n",
    "data_path = \"../../DatasetAnalysis_FinancialPhraseBank/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"@\" in line:\n",
    "            parts = line.rsplit(\"@\", 1)\n",
    "            if len(parts) == 2:\n",
    "                sentences.append(parts[0])\n",
    "                sentiments.append(parts[1])\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"sentiment\": sentiments})\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} sentences\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"sentiment\"].value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Sample sentences for risk assessment:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9ad89",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Risk Assessment Prompt Design\n",
    "\n",
    "**Prompt Strategy**: Assess financial risk level based on statement content without examples.\n",
    "\n",
    "**Risk Mapping**:\n",
    "- **Low**: Positive news, growth, strong performance\n",
    "- **Medium**: Neutral or mixed signals, stable conditions\n",
    "- **High**: Negative news, declining performance\n",
    "- **Critical**: Severe financial distress, losses, bankruptcy risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_risk_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Creates a zero-shot prompt for financial risk assessment.\n",
    "    No examples provided - model relies on pretrained knowledge.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial risk assessment expert.\n",
    "\n",
    "Analyze the following financial statement and assess the risk level as one of: \"low\", \"medium\", \"high\", or \"critical\".\n",
    "\n",
    "Risk Level Guidelines:\n",
    "- **Low Risk**: Strong financial performance, revenue growth, profitability increases, positive market position\n",
    "- **Medium Risk**: Stable performance with no significant changes, mixed signals, or neutral developments\n",
    "- **High Risk**: Declining performance, revenue drops, reduced profitability, concerning market conditions\n",
    "- **Critical Risk**: Severe financial distress, major losses, bankruptcy risk, existential threats to the business\n",
    "\n",
    "Financial Statement:\n",
    "\"{sentence}\"\n",
    "\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "    \"risk_level\": \"low/medium/high/critical\",\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"rationale\": \"Brief explanation in one sentence\",\n",
    "    \"key_indicators\": \"Main factors influencing the risk assessment\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_sentence = \"The company reported significant losses and announced layoffs affecting 30% of the workforce.\"\n",
    "print(\"=\" * 80)\n",
    "print(\"ZERO-SHOT RISK ASSESSMENT PROMPT EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(create_zero_shot_risk_prompt(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb7767",
   "metadata": {},
   "source": [
    "## 3. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\", temperature=0.0):\n",
    "    \"\"\"Call Gemini API with retry logic\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=temperature,\n",
    "                    max_output_tokens=600,\n",
    "                ),\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_llama(prompt, temperature=0.0):\n",
    "    \"\"\"Call Llama via Groq API\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=600,\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2**attempt)\n",
    "                continue\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_risk_response(response_text):\n",
    "    \"\"\"Parse JSON response from model\"\"\"\n",
    "    try:\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            json_str = response_text.split(\"```\")[1].strip()\n",
    "        else:\n",
    "            json_str = response_text.strip()\n",
    "\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except:\n",
    "        # Fallback: try to extract risk level with regex\n",
    "        response_lower = response_text.lower()\n",
    "        if \"critical\" in response_lower:\n",
    "            return {\n",
    "                \"risk_level\": \"critical\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "                \"key_indicators\": \"N/A\",\n",
    "            }\n",
    "        elif \"high\" in response_lower and \"risk\" in response_lower:\n",
    "            return {\n",
    "                \"risk_level\": \"high\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "                \"key_indicators\": \"N/A\",\n",
    "            }\n",
    "        elif \"medium\" in response_lower:\n",
    "            return {\n",
    "                \"risk_level\": \"medium\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "                \"key_indicators\": \"N/A\",\n",
    "            }\n",
    "        elif \"low\" in response_lower:\n",
    "            return {\n",
    "                \"risk_level\": \"low\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"rationale\": \"Parsed from text\",\n",
    "                \"key_indicators\": \"N/A\",\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✓ Model inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adf30f",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "### E11: Gemini 2.5 Pro (Zero-Shot Risk Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, use a sample of the dataset (remove .head(100) for full run)\n",
    "test_df = df.head(100).copy()  # Remove .head(100) for full dataset\n",
    "\n",
    "# E11: Gemini 2.5 Pro\n",
    "print(\"Running E11: Gemini 2.5 Pro (Zero-Shot Risk Assessment)...\")\n",
    "e11_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E11 Progress\"):\n",
    "    prompt = create_zero_shot_risk_prompt(row[\"sentence\"])\n",
    "    response = call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_risk_response(response)\n",
    "        if parsed:\n",
    "            e11_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": parsed.get(\"risk_level\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                    \"key_indicators\": parsed.get(\"key_indicators\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e11_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                    \"key_indicators\": \"N/A\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "e11_df = pd.DataFrame(e11_results)\n",
    "print(f\"\\n✓ E11 completed: {len(e11_df)} risk assessments\")\n",
    "display(e11_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e3286",
   "metadata": {},
   "source": [
    "### E12: Gemini 2.5 Flash (Zero-Shot Risk Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E12: Gemini 2.5 Flash\n",
    "print(\"Running E12: Gemini 2.5 Flash (Zero-Shot Risk Assessment)...\")\n",
    "e12_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E12 Progress\"):\n",
    "    prompt = create_zero_shot_risk_prompt(row[\"sentence\"])\n",
    "    response = call_gemini(prompt, model_name=\"gemini-2.0-flash-exp\")\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_risk_response(response)\n",
    "        if parsed:\n",
    "            e12_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": parsed.get(\"risk_level\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                    \"key_indicators\": parsed.get(\"key_indicators\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e12_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                    \"key_indicators\": \"N/A\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e12_df = pd.DataFrame(e12_results)\n",
    "print(f\"\\n✓ E12 completed: {len(e12_df)} risk assessments\")\n",
    "display(e12_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b33b3",
   "metadata": {},
   "source": [
    "### E13: Llama-3.3-70B-Versatile (Zero-Shot Risk Assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463980be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E13: Llama-3.3-70B\n",
    "print(\"Running E13: Llama-3.3-70B (Zero-Shot Risk Assessment)...\")\n",
    "e13_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"E13 Progress\"):\n",
    "    prompt = create_zero_shot_risk_prompt(row[\"sentence\"])\n",
    "    response = call_llama(prompt)\n",
    "\n",
    "    if response:\n",
    "        parsed = parse_risk_response(response)\n",
    "        if parsed:\n",
    "            e13_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": parsed.get(\"risk_level\", \"unknown\"),\n",
    "                    \"confidence\": parsed.get(\"confidence\", 0),\n",
    "                    \"rationale\": parsed.get(\"rationale\", \"\"),\n",
    "                    \"key_indicators\": parsed.get(\"key_indicators\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            e13_results.append(\n",
    "                {\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"true_sentiment\": row[\"sentiment\"],\n",
    "                    \"predicted_risk\": \"error\",\n",
    "                    \"confidence\": 0,\n",
    "                    \"rationale\": \"Parse error\",\n",
    "                    \"key_indicators\": \"N/A\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "e13_df = pd.DataFrame(e13_results)\n",
    "print(f\"\\n✓ E13 completed: {len(e13_df)} risk assessments\")\n",
    "display(e13_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5388cfb",
   "metadata": {},
   "source": [
    "## 5. Analyze Risk Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cba16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk distribution analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df_result, title) in enumerate(\n",
    "    [\n",
    "        (e11_df, \"E11: Gemini Pro\"),\n",
    "        (e12_df, \"E12: Gemini Flash\"),\n",
    "        (e13_df, \"E13: Llama-3.3-70B\"),\n",
    "    ]\n",
    "):\n",
    "    valid_df = df_result[\n",
    "        df_result[\"predicted_risk\"].isin([\"low\", \"medium\", \"high\", \"critical\"])\n",
    "    ]\n",
    "    risk_counts = valid_df[\"predicted_risk\"].value_counts()\n",
    "\n",
    "    axes[idx].bar(\n",
    "        risk_counts.index,\n",
    "        risk_counts.values,\n",
    "        color=[\"green\", \"yellow\", \"orange\", \"red\"],\n",
    "    )\n",
    "    axes[idx].set_title(f\"{title}\\nRisk Distribution\", fontsize=12, weight=\"bold\")\n",
    "    axes[idx].set_xlabel(\"Risk Level\", fontsize=11)\n",
    "    axes[idx].set_ylabel(\"Count\", fontsize=11)\n",
    "    axes[idx].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_risk_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "for df_result, exp_name in [\n",
    "    (e11_df, \"E11: Gemini Pro\"),\n",
    "    (e12_df, \"E12: Gemini Flash\"),\n",
    "    (e13_df, \"E13: Llama-3.3-70B\"),\n",
    "]:\n",
    "    print(f\"\\n{exp_name} - Risk Level Distribution:\")\n",
    "    print(df_result[\"predicted_risk\"].value_counts())\n",
    "    print(f\"Average Confidence: {df_result['confidence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e62f4",
   "metadata": {},
   "source": [
    "## 6. Sentiment-Risk Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ec35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how sentiment correlates with assessed risk\n",
    "def analyze_sentiment_risk_mapping(df_result, exp_name):\n",
    "    \"\"\"\n",
    "    Expected mapping:\n",
    "    - positive sentiment -> low/medium risk\n",
    "    - neutral sentiment -> medium risk\n",
    "    - negative sentiment -> high/critical risk\n",
    "    \"\"\"\n",
    "    valid_df = df_result[\n",
    "        df_result[\"predicted_risk\"].isin([\"low\", \"medium\", \"high\", \"critical\"])\n",
    "    ].copy()\n",
    "\n",
    "    mapping = (\n",
    "        pd.crosstab(\n",
    "            valid_df[\"true_sentiment\"], valid_df[\"predicted_risk\"], normalize=\"index\"\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{exp_name}: Sentiment → Risk Mapping (%)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(mapping.round(2))\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "e11_mapping = analyze_sentiment_risk_mapping(e11_df, \"E11: Gemini Pro\")\n",
    "e12_mapping = analyze_sentiment_risk_mapping(e12_df, \"E12: Gemini Flash\")\n",
    "e13_mapping = analyze_sentiment_risk_mapping(e13_df, \"E13: Llama-3.3-70B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbb423",
   "metadata": {},
   "source": [
    "## 7. Visualize Sentiment-Risk Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for sentiment-risk correlation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, (mapping, title) in enumerate(\n",
    "    [\n",
    "        (e11_mapping, \"E11: Gemini Pro\"),\n",
    "        (e12_mapping, \"E12: Gemini Flash\"),\n",
    "        (e13_mapping, \"E13: Llama-3.3-70B\"),\n",
    "    ]\n",
    "):\n",
    "    sns.heatmap(\n",
    "        mapping,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"RdYlGn_r\",\n",
    "        ax=axes[idx],\n",
    "        cbar_kws={\"label\": \"Percentage (%)\"},\n",
    "        vmin=0,\n",
    "        vmax=100,\n",
    "    )\n",
    "    axes[idx].set_title(\n",
    "        f\"{title}\\nSentiment → Risk Mapping\", fontsize=12, weight=\"bold\"\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Predicted Risk Level\", fontsize=11)\n",
    "    axes[idx].set_ylabel(\"True Sentiment\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Sentiment to Risk Level Correlation Analysis\", fontsize=14, weight=\"bold\", y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"zero_shot_sentiment_risk_heatmaps.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421e732",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "e11_df.to_csv(f\"e11_gemini_pro_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "e12_df.to_csv(f\"e12_gemini_flash_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "e13_df.to_csv(f\"e13_llama_zero_shot_risk_{timestamp}.csv\", index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_data = []\n",
    "for df_result, exp_name in [\n",
    "    (e11_df, \"E11: Gemini Pro\"),\n",
    "    (e12_df, \"E12: Gemini Flash\"),\n",
    "    (e13_df, \"E13: Llama-3.3-70B\"),\n",
    "]:\n",
    "    valid_df = df_result[\n",
    "        df_result[\"predicted_risk\"].isin([\"low\", \"medium\", \"high\", \"critical\"])\n",
    "    ]\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Experiment\": exp_name,\n",
    "            \"Total_Samples\": len(df_result),\n",
    "            \"Valid_Predictions\": len(valid_df),\n",
    "            \"Avg_Confidence\": valid_df[\"confidence\"].mean(),\n",
    "            \"Low_Risk_Count\": (valid_df[\"predicted_risk\"] == \"low\").sum(),\n",
    "            \"Medium_Risk_Count\": (valid_df[\"predicted_risk\"] == \"medium\").sum(),\n",
    "            \"High_Risk_Count\": (valid_df[\"predicted_risk\"] == \"high\").sum(),\n",
    "            \"Critical_Risk_Count\": (valid_df[\"predicted_risk\"] == \"critical\").sum(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(f\"zero_shot_risk_summary_{timestamp}.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved with timestamp: {timestamp}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - e11_gemini_pro_zero_shot_risk_{timestamp}.csv\")\n",
    "print(f\"  - e12_gemini_flash_zero_shot_risk_{timestamp}.csv\")\n",
    "print(f\"  - e13_llama_zero_shot_risk_{timestamp}.csv\")\n",
    "print(f\"  - zero_shot_risk_summary_{timestamp}.csv\")\n",
    "print(f\"  - zero_shot_risk_distribution.png\")\n",
    "print(f\"  - zero_shot_sentiment_risk_heatmaps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b202d3d",
   "metadata": {},
   "source": [
    "## 9. Key Findings\n",
    "\n",
    "### Analysis Questions:\n",
    "\n",
    "1. **Risk Assessment Consistency**\n",
    "   - Do models agree on risk levels?\n",
    "   - Which model is most conservative/aggressive in risk assessment?\n",
    "\n",
    "2. **Sentiment-Risk Correlation**\n",
    "   - Does negative sentiment correctly map to high/critical risk?\n",
    "   - Does positive sentiment correctly map to low/medium risk?\n",
    "\n",
    "3. **Model Confidence**\n",
    "   - Which model shows highest confidence in risk assessments?\n",
    "   - Correlation between confidence and sentiment clarity?\n",
    "\n",
    "4. **Edge Cases**\n",
    "   - Which statements cause disagreement between models?\n",
    "   - Neutral statements - do they map to medium risk as expected?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}